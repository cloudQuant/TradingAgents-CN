from typing import Optional, Dict, Any
from datetime import datetime, timedelta
from fastapi import APIRouter, Depends, Query, BackgroundTasks, HTTPException, status
from pydantic import BaseModel
import hashlib
import logging
import uuid
from fastapi.responses import JSONResponse

from app.routers.auth_db import get_current_user
from tradingagents.dataflows.interface import (
    get_cn_bond_data_unified,
    get_cn_bond_info_unified,
    get_cn_bond_yield_curve_unified,
)
from tradingagents.dataflows.providers.china.bonds import AKShareBondProvider
from tradingagents.utils.instrument_validator import normalize_bond_code
from app.core.database import get_mongo_db
from app.services.bond_data_service import BondDataService

router = APIRouter(prefix="/api/bonds", tags=["bonds"])
logger = logging.getLogger("webapi")  # ä½¿ç”¨ä¸å…¶ä»–è·¯ç”±ä¸€è‡´çš„æ—¥å¿—å™¨

# ç®€å•çš„å†…å­˜ç¼“å­˜ï¼Œç”¨äºå‡å°‘æ•°æ®åº“æŸ¥è¯¢
_bond_list_cache = {}
_cache_ttl_seconds = 300  # 5åˆ†é’Ÿç¼“å­˜

def _get_cache_key(q: Optional[str], category: Optional[str], exchange: Optional[str], 
                   only_not_matured: bool, page: int, page_size: int, 
                   sort_by: Optional[str], sort_dir: str) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    key_str = f"{q}_{category}_{exchange}_{only_not_matured}_{page}_{page_size}_{sort_by}_{sort_dir}"
    return hashlib.md5(key_str.encode()).hexdigest()

def _is_cache_valid(cache_entry: dict) -> bool:
    """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
    if not cache_entry:
        return False
    cache_time = cache_entry.get("timestamp")
    if not cache_time:
        return False
    age = (datetime.now() - cache_time).total_seconds()
    return age < _cache_ttl_seconds


@router.get("/list")
async def list_bonds(
    q: Optional[str] = Query(None, description="å…³é”®è¯è¿‡æ»¤ï¼ŒæŒ‰ä»£ç æˆ–åç§°åŒ…å«åŒ¹é…"),
    limit: int = Query(100, ge=1, le=1000, description="æœ€å¤§è¿”å›é™åˆ¶ï¼ˆå…¼å®¹å‚æ•°ï¼Œåˆ†é¡µä¼˜å…ˆï¼‰"),
    category: Optional[str] = Query(None, description="å€ºåˆ¸ç±»åˆ«ï¼šconvertible|exchangeable|interest|credit|other"),
    exchange: Optional[str] = Query(None, description="äº¤æ˜“æ‰€ï¼šSH|SZ"),
    only_not_matured: bool = Query(False, description="ä»…æ˜¾ç¤ºæœªåˆ°æœŸï¼ˆä»…å¯¹åˆ©ç‡å€ºç”Ÿæ•ˆï¼‰"),
    page: int = Query(1, ge=1, description="é¡µç ï¼Œä»1å¼€å§‹"),
    page_size: int = Query(20, ge=1, le=200, description="æ¯é¡µæ•°é‡ï¼Œé»˜è®¤20"),
    sort_by: Optional[str] = Query(None, description="æ’åºå­—æ®µï¼šcode|name|maturity_date|list_date|coupon_rate"),
    sort_dir: str = Query("asc", description="æ’åºæ–¹å‘ï¼šasc/desc"),
    current_user: dict = Depends(get_current_user),
):
    try:
        logger.info(f"ğŸ” [å€ºåˆ¸åˆ—è¡¨] æ”¶åˆ°è¯·æ±‚: category={category}, page={page}, page_size={page_size}, q={q}, exchange={exchange}")
        db = get_mongo_db()
        if db is None:
            logger.error("âŒ [å€ºåˆ¸åˆ—è¡¨] MongoDBæ•°æ®åº“è¿æ¥ä¸ºNone")
            raise HTTPException(status_code=500, detail="æ•°æ®åº“è¿æ¥å¤±è´¥")
        
        # è§„èŒƒå‚æ•°ï¼šæ¯é¡µæœ€å¤š20æ¡ï¼›æ’åºæ–¹å‘ä»…å…è®¸ asc/desc
        try:
            page_size = max(1, min(int(page_size), 20))
        except Exception as pe:
            logger.warning(f"âš ï¸ [å€ºåˆ¸åˆ—è¡¨] å‚æ•°è§£æå¤±è´¥: {pe}")
            page_size = 20
        sdir = str(sort_dir or "asc").lower()
        if sdir not in ("asc", "desc"):
            sdir = "asc"
        sort_dir = sdir
        # å¦‚æœcategoryä¸ºç©ºæˆ–Noneï¼Œä¸è®¾ç½®é»˜è®¤å€¼ï¼ŒæŸ¥è¯¢æ‰€æœ‰ç±»åˆ«
        # æ³¨æ„ï¼šè¿™é‡Œä¸å†å¼ºåˆ¶è®¾ç½®é»˜è®¤å€¼ï¼Œè®©å‰ç«¯æ§åˆ¶é»˜è®¤æ˜¾ç¤º
        if category and category.strip() == "":
            category = None
        # æ£€æŸ¥ç¼“å­˜
        cache_key = _get_cache_key(q, category, exchange, only_not_matured, page, page_size, sort_by, sort_dir)
        cached_result = _bond_list_cache.get(cache_key)
        
        if _is_cache_valid(cached_result):
            logger.info(f"ğŸ“¦ [å€ºåˆ¸åˆ—è¡¨] ä»ç¼“å­˜è·å–æ•°æ® (category={category}, page={page})")
            return cached_result["data"]
        
        logger.info(f"ğŸ”§ [å€ºåˆ¸åˆ—è¡¨] åˆå§‹åŒ–BondDataService")
        svc = BondDataService(db)
        logger.info(f"ğŸ”§ [å€ºåˆ¸åˆ—è¡¨] ç¡®ä¿ç´¢å¼•å­˜åœ¨")
        try:
            await svc.ensure_indexes()
            logger.info(f"âœ… [å€ºåˆ¸åˆ—è¡¨] ç´¢å¼•æ£€æŸ¥å®Œæˆ")
        except Exception as idx_err:
            logger.error(f"âŒ [å€ºåˆ¸åˆ—è¡¨] ç´¢å¼•æ£€æŸ¥å¤±è´¥: {idx_err}", exc_info=True)
            # ç´¢å¼•å¤±è´¥ä¸åº”è¯¥é˜»æ­¢æŸ¥è¯¢ï¼Œç»§ç»­æ‰§è¡Œ

        # ä¼˜å…ˆä»æ•°æ®åº“æŸ¥è¯¢
        logger.info(f"ğŸ” [å€ºåˆ¸åˆ—è¡¨] å¼€å§‹ä»æ•°æ®åº“æŸ¥è¯¢æ•°æ® (category={category}, page={page}, page_size={page_size}, q={q}, exchange={exchange})")
        try:
            result = await svc.query_basic_list(q=q, category=category, exchange=exchange, only_not_matured=only_not_matured, page=page, page_size=page_size, sort_by=sort_by, sort_dir=sort_dir)
        except TypeError as te:
            # å…¼å®¹è€ç‰ˆæœ¬æœªæ”¯æŒæ’åºå‚æ•°çš„æ–¹æ³•ç­¾å
            logger.warning(f"âš ï¸ [å€ºåˆ¸åˆ—è¡¨] æ–¹æ³•ç­¾åä¸åŒ¹é…ï¼Œå°è¯•å…¼å®¹è°ƒç”¨: {te}")
            try:
                result = await svc.query_basic_list(q=q, category=category, exchange=exchange, only_not_matured=only_not_matured, page=page, page_size=page_size)  # type: ignore
            except TypeError:
                # å…¼å®¹æ›´è€ç‰ˆæœ¬æœªæ”¯æŒexchangeå‚æ•°çš„æ–¹æ³•ç­¾å
                result = await svc.query_basic_list(q=q, category=category, only_not_matured=only_not_matured, page=page, page_size=page_size)  # type: ignore
        except Exception as e:
            logger.error(f"âŒ [å€ºåˆ¸åˆ—è¡¨] æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
            result = {"total": 0, "items": []}

        total = int(result.get("total") or 0)
        items = list(result.get("items") or [])
        logger.info(f"ğŸ“Š [å€ºåˆ¸åˆ—è¡¨] æ•°æ®åº“æŸ¥è¯¢ç»“æœ: total={total}, items={len(items)}")

        # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®ï¼Œæ‰ä» AKShare è·å–å¹¶ä¿å­˜
        if total == 0:
            logger.warning(f"âš ï¸ [å€ºåˆ¸åˆ—è¡¨] æ•°æ®åº“ä¸ºç©º (total=0)ï¼Œå°†ä» AKShare è·å–æ•°æ®å¹¶ä¿å­˜åˆ°æ•°æ®åº“ (category={category})")
            try:
                provider = AKShareBondProvider()
                fetched = await provider.get_symbol_list()
                if fetched:
                    logger.info(f"ğŸ“¡ [å€ºåˆ¸åˆ—è¡¨] ä» AKShare è·å–åˆ° {len(fetched)} æ¡å€ºåˆ¸æ•°æ®ï¼Œæ­£åœ¨ä¿å­˜åˆ°æ•°æ®åº“...")
                    saved_count = await svc.save_basic_list(fetched)
                    logger.info(f"ğŸ’¾ [å€ºåˆ¸åˆ—è¡¨] å·²ä¿å­˜ {saved_count} æ¡å€ºåˆ¸æ•°æ®åˆ°æ•°æ®åº“")
                    # é‡æ–°æŸ¥è¯¢æ•°æ®åº“
                    try:
                        result = await svc.query_basic_list(q=q, category=category, exchange=exchange, only_not_matured=only_not_matured, page=page, page_size=page_size, sort_by=sort_by, sort_dir=sort_dir)
                    except TypeError:
                        # å…¼å®¹è€ç‰ˆæœ¬æœªæ”¯æŒæ’åºå‚æ•°çš„æ–¹æ³•ç­¾å
                        try:
                            result = await svc.query_basic_list(q=q, category=category, exchange=exchange, only_not_matured=only_not_matured, page=page, page_size=page_size)  # type: ignore
                        except TypeError:
                            # å…¼å®¹æ›´è€ç‰ˆæœ¬æœªæ”¯æŒexchangeå‚æ•°çš„æ–¹æ³•ç­¾å
                            result = await svc.query_basic_list(q=q, category=category, only_not_matured=only_not_matured, page=page, page_size=page_size)  # type: ignore
                    total = int(result.get("total") or 0)
                    items = list(result.get("items") or [])
                    logger.info(f"âœ… [å€ºåˆ¸åˆ—è¡¨] ä¿å­˜åé‡æ–°æŸ¥è¯¢æ•°æ®åº“: total={total}, items={len(items)}")
                else:
                    logger.warning(f"âš ï¸ [å€ºåˆ¸åˆ—è¡¨] ä» AKShare è·å–æ•°æ®ä¸ºç©º")
            except Exception as e:
                logger.error(f"âŒ [å€ºåˆ¸åˆ—è¡¨] ä» AKShare è·å–æ•°æ®å¤±è´¥: {e}", exc_info=True)
        else:
            logger.info(f"âœ… [å€ºåˆ¸åˆ—è¡¨] ä»æ•°æ®åº“è·å– {total} æ¡å€ºåˆ¸æ•°æ® (category={category}, page={page}, items={len(items)})")

        # ç§»é™¤ _id å’Œ ObjectIdï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
        from bson import ObjectId
        from datetime import datetime as dt, date
        logger.info(f"ğŸ”§ [å€ºåˆ¸åˆ—è¡¨] å¼€å§‹å¤„ç† {len(items)} æ¡æ•°æ®çš„åºåˆ—åŒ–")
        for idx, it in enumerate(items):
            try:
                if isinstance(it, dict):
                    # ç§»é™¤ _id å­—æ®µ
                    if "_id" in it:
                        it.pop("_id", None)
                    # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æ˜¯å¯åºåˆ—åŒ–çš„
                    for key, value in list(it.items()):
                        if value is None:
                            continue
                        # å¤„ç† ObjectId
                        if isinstance(value, ObjectId):
                            it[key] = str(value)
                        # å¤„ç† datetime å’Œ date
                        elif isinstance(value, (dt, date)):
                            it[key] = value.isoformat()
                        # å¤„ç†å…¶ä»–ä¸å¯åºåˆ—åŒ–çš„ç±»å‹
                        elif not isinstance(value, (str, int, float, bool, list, dict)):
                            try:
                                it[key] = str(value)
                            except Exception:
                                it.pop(key, None)
                        # å¤„ç†åµŒå¥—å­—å…¸ä¸­çš„ ObjectId å’Œ datetime
                        elif isinstance(value, dict):
                            for k, v in list(value.items()):
                                if isinstance(v, ObjectId):
                                    value[k] = str(v)
                                elif isinstance(v, (dt, date)):
                                    value[k] = v.isoformat()
                        # å¤„ç†åˆ—è¡¨ä¸­çš„ ObjectId å’Œ datetime
                        elif isinstance(value, list):
                            for i, v in enumerate(value):
                                if isinstance(v, ObjectId):
                                    value[i] = str(v)
                                elif isinstance(v, (dt, date)):
                                    value[i] = v.isoformat()
                                elif isinstance(v, dict):
                                    for k, v2 in list(v.items()):
                                        if isinstance(v2, ObjectId):
                                            v[k] = str(v2)
                                        elif isinstance(v2, (dt, date)):
                                            v[k] = v2.isoformat()
            except Exception as ser_err:
                logger.error(f"âŒ [å€ºåˆ¸åˆ—è¡¨] åºåˆ—åŒ–ç¬¬ {idx} æ¡æ•°æ®å¤±è´¥: {ser_err}", exc_info=True)
                # å¦‚æœæŸæ¡æ•°æ®åºåˆ—åŒ–å¤±è´¥ï¼Œå°è¯•ç§»é™¤é—®é¢˜å­—æ®µæˆ–è·³è¿‡
                try:
                    # å°è¯•å°†æ‰€æœ‰å­—æ®µè½¬ä¸ºå­—ç¬¦ä¸²
                    for k, v in list(it.items()):
                        try:
                            if not isinstance(v, (str, int, float, bool, list, dict)):
                                it[k] = str(v)
                        except:
                            it.pop(k, None)
                except:
                    # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œä»åˆ—è¡¨ä¸­ç§»é™¤è¿™æ¡æ•°æ®
                    logger.warning(f"âš ï¸ [å€ºåˆ¸åˆ—è¡¨] ç§»é™¤æ— æ³•åºåˆ—åŒ–çš„æ•°æ®é¡¹ {idx}")
                    items[idx] = None
        # ç§»é™¤ None é¡¹
        items = [it for it in items if it is not None]
        logger.info(f"âœ… [å€ºåˆ¸åˆ—è¡¨] åºåˆ—åŒ–å®Œæˆï¼Œå‰©ä½™ {len(items)} æ¡æ•°æ®")

        # å…¼å®¹ limitï¼ˆè‹¥è°ƒç”¨æ–¹ä»ä¼ å…¥limitï¼Œåˆ™ä»ç„¶ç”Ÿæ•ˆäºå½“å‰é¡µä¸Šé™ï¼‰
        if limit and len(items) > limit:
            items = items[:limit]

        # æ„å»ºè¿”å›æ•°æ®
        logger.info(f"ğŸ”§ [å€ºåˆ¸åˆ—è¡¨] æ„å»ºå“åº”æ•°æ®: total={total}, items_count={len(items)}")
        try:
            response_data = {"success": True, "data": {"total": total, "page": page, "page_size": page_size, "items": items}}
            logger.info(f"âœ… [å€ºåˆ¸åˆ—è¡¨] å“åº”æ•°æ®æ„å»ºæˆåŠŸ")
        except Exception as build_err:
            logger.error(f"âŒ [å€ºåˆ¸åˆ—è¡¨] æ„å»ºå“åº”æ•°æ®å¤±è´¥: {build_err}", exc_info=True)
            raise
        
        # ç¼“å­˜ç»“æœï¼ˆåªç¼“å­˜æ•°æ®åº“ä¸­æœ‰æ•°æ®çš„æƒ…å†µï¼Œé¿å…ç¼“å­˜ç©ºç»“æœï¼‰
        if total > 0:
            try:
                _bond_list_cache[cache_key] = {
                    "data": response_data,
                    "timestamp": datetime.now()
                }
                # æ¸…ç†è¿‡æœŸç¼“å­˜ï¼ˆä¿æŒç¼“å­˜å¤§å°åˆç†ï¼‰
                if len(_bond_list_cache) > 1000:
                    now = datetime.now()
                    expired_keys = [k for k, v in _bond_list_cache.items() 
                                  if not _is_cache_valid(v)]
                    for k in expired_keys:
                        _bond_list_cache.pop(k, None)
            except Exception as cache_err:
                logger.warning(f"âš ï¸ [å€ºåˆ¸åˆ—è¡¨] ç¼“å­˜æ“ä½œå¤±è´¥: {cache_err}")

        logger.info(f"âœ… [å€ºåˆ¸åˆ—è¡¨] è¯·æ±‚å¤„ç†å®Œæˆï¼Œè¿”å›æ•°æ®")
        return response_data
    except HTTPException:
        # HTTPExceptionåº”è¯¥ç›´æ¥æŠ›å‡ºï¼Œä¸è¦æ•è·
        raise
    except Exception as e:
        logger.error(f"âŒ [å€ºåˆ¸åˆ—è¡¨] å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        import traceback
        error_trace = traceback.format_exc()
        logger.error(f"âŒ [å€ºåˆ¸åˆ—è¡¨] é”™è¯¯å †æ ˆ: {error_trace}")
        # ç¡®ä¿å˜é‡å·²å®šä¹‰
        try:
            page_val = page
        except:
            page_val = 1
        try:
            page_size_val = page_size
        except:
            page_size_val = 20
        # æŠ›å‡ºHTTPExceptionï¼Œè®©å…¨å±€å¼‚å¸¸å¤„ç†å™¨å¤„ç†
        raise HTTPException(
            status_code=500,
            detail=f"è·å–å€ºåˆ¸åˆ—è¡¨å¤±è´¥: {str(e)}"
        )


@router.get("/{code}/history")
async def get_bond_history(
    code: str,
    start: str = Query(..., description="å¼€å§‹æ—¥æœŸ YYYY-MM-DD"),
    end: str = Query(..., description="ç»“æŸæ—¥æœŸ YYYY-MM-DD"),
    period: str = Query("daily", description="å‘¨æœŸï¼Œé»˜è®¤ daily"),
    current_user: dict = Depends(get_current_user),
):
    db = get_mongo_db()
    svc = BondDataService(db)
    await svc.ensure_indexes()
    
    # ä¼˜å…ˆä»æ•°æ®åº“æŸ¥è¯¢å†å²æ•°æ®
    df = await svc.query_bond_daily(code, start, end)
    
    # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®æˆ–æ•°æ®ä¸å®Œæ•´ï¼Œä»æ¥å£è·å–å¹¶ä¿å­˜
    if df is None or df.empty:
        # ä»æ¥å£è·å–æ•°æ®
        result = get_cn_bond_data_unified(code, start, end, period)
        
        # å¦‚æœæ¥å£è¿”å›æˆåŠŸï¼Œå°è¯•ä¿å­˜åˆ°æ•°æ®åº“
        if not result.startswith("âŒ") and not result.startswith("âš ï¸"):
            try:
                # ä½¿ç”¨providerç›´æ¥è·å–DataFrameä»¥ä¾¿ä¿å­˜
                provider = AKShareBondProvider()
                norm = normalize_bond_code(code)
                code_std = norm.get("code_std") or code
                
                # è·å–å†å²æ•°æ®
                hist_df = await provider.get_historical_data(code_std, start, end, period)
                if hist_df is not None and not hist_df.empty:
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    await svc.save_bond_daily(code_std, hist_df)
            except Exception as e:
                # ä¿å­˜å¤±è´¥ä¸å½±å“è¿”å›æ•°æ®
                import logging
                logging.warning(f"ä¿å­˜å€ºåˆ¸å†å²æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        
        return {"success": not result.startswith("âŒ"), "data": result}
    else:
        # æ•°æ®åº“ä¸­æœ‰æ•°æ®ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼è¿”å›ï¼ˆä¸æ¥å£æ ¼å¼ä¿æŒä¸€è‡´ï¼‰
        try:
            preview = df.to_string(index=False)
            title = f"## å€ºåˆ¸ {code} å†å²æ•°æ® ({start} åˆ° {end})"
            result = f"{title}\n" + preview
            return {"success": True, "data": result, "from_db": True}
        except Exception as e:
            # è½¬æ¢å¤±è´¥ï¼Œå›é€€åˆ°æ¥å£
            result = get_cn_bond_data_unified(code, start, end, period)
            return {"success": not result.startswith("âŒ"), "data": result}


@router.get("/{code}/analytics")
async def get_bond_analytics(
    code: str,
    start: str = Query(..., description="å¼€å§‹æ—¥æœŸ YYYY-MM-DD"),
    end: str = Query(..., description="ç»“æŸæ—¥æœŸ YYYY-MM-DD"),
    current_user: dict = Depends(get_current_user),
):
    # ç°é˜¶æ®µçš„åˆ†æç»“æœéšå†å²æ•°æ®å­—ç¬¦ä¸²ä¸€èµ·åŒ…å«ï¼ˆå«MA/MACD/RSI/BOLLç­‰ï¼‰ï¼Œå…ˆå¤ç”¨å†å²æ•°æ®æ¥å£
    # ä¼˜å…ˆä»æ•°æ®åº“æŸ¥è¯¢å†å²æ•°æ®
    db = get_mongo_db()
    svc = BondDataService(db)
    await svc.ensure_indexes()
    
    df = await svc.query_bond_daily(code, start, end)
    
    # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®æˆ–æ•°æ®ä¸å®Œæ•´ï¼Œä»æ¥å£è·å–å¹¶ä¿å­˜
    if df is None or df.empty:
        # ä»æ¥å£è·å–æ•°æ®
        result = get_cn_bond_data_unified(code, start, end, period="daily")
        
        # å¦‚æœæ¥å£è¿”å›æˆåŠŸï¼Œå°è¯•ä¿å­˜åˆ°æ•°æ®åº“
        if not result.startswith("âŒ") and not result.startswith("âš ï¸"):
            try:
                # ä½¿ç”¨providerç›´æ¥è·å–DataFrameä»¥ä¾¿ä¿å­˜
                provider = AKShareBondProvider()
                norm = normalize_bond_code(code)
                code_std = norm.get("code_std") or code
                
                # è·å–å†å²æ•°æ®
                hist_df = await provider.get_historical_data(code_std, start, end, period="daily")
                if hist_df is not None and not hist_df.empty:
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    await svc.save_bond_daily(code_std, hist_df)
            except Exception as e:
                # ä¿å­˜å¤±è´¥ä¸å½±å“è¿”å›æ•°æ®
                import logging
                logging.warning(f"ä¿å­˜å€ºåˆ¸å†å²æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        
        return {"success": not result.startswith("âŒ"), "data": result}
    else:
        # æ•°æ®åº“ä¸­æœ‰æ•°æ®ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼è¿”å›ï¼ˆä¸æ¥å£æ ¼å¼ä¿æŒä¸€è‡´ï¼‰
        try:
            preview = df.to_string(index=False)
            title = f"## å€ºåˆ¸ {code} å†å²æ•°æ®ä¸åˆ†æ ({start} åˆ° {end})"
            result = f"{title}\n" + preview
            return {"success": True, "data": result, "from_db": True}
        except Exception as e:
            # è½¬æ¢å¤±è´¥ï¼Œå›é€€åˆ°æ¥å£
            result = get_cn_bond_data_unified(code, start, end, period="daily")
            return {"success": not result.startswith("âŒ"), "data": result}


@router.get("/{code}/info")
async def get_bond_info(
    code: str,
    current_user: dict = Depends(get_current_user),
):
    db = get_mongo_db()
    svc = BondDataService(db)
    await svc.ensure_indexes()
    
    # ä¼˜å…ˆä»æ•°æ®åº“æŸ¥è¯¢è¯¦æƒ…ä¿¡æ¯
    info = await svc.query_bond_info(code)
    
    # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ï¼Œä»æ¥å£è·å–å¹¶ä¿å­˜
    if not info or (isinstance(info, dict) and info.get("error")):
        info = get_cn_bond_info_unified(code)
        
        # å¦‚æœæ¥å£è¿”å›çš„æ•°æ®æ ¼å¼ä¸å¯¹ï¼Œå°è¯•ä»åŸºç¡€åˆ—è¡¨è·å–
        if isinstance(info, dict) and info.get("error"):
            # ä»åŸºç¡€åˆ—è¡¨ä¸­æŸ¥æ‰¾
            try:
                result = await svc.query_basic_list(q=code, page=1, page_size=1)
                items = result.get("items", [])
                if items and len(items) > 0:
                    info = items[0]
                    info.pop("_id", None)
            except Exception:
                pass
        
        # å¤„ç†æ¥å£è¿”å›çš„æ ¼å¼ï¼ˆå¦‚æœæ˜¯åµŒå¥—çš„dataå­—æ®µï¼‰
        if isinstance(info, dict) and "data" in info and isinstance(info.get("data"), list) and len(info.get("data", [])) > 0:
            # å°†dataä¸­çš„ç¬¬ä¸€æ¡è®°å½•å±•å¼€
            data_records = info.get("data", [])
            if data_records:
                # ä¿ç•™codeå’Œsourceç­‰å­—æ®µï¼Œåˆå¹¶dataä¸­çš„å­—æ®µ
                code_value = info.get("code", code)
                source_value = info.get("source", "akshare")
                data_item = data_records[0]
                info = {"code": code_value, "source": source_value}
                info.update(data_item)
        
        # å¦‚æœæˆåŠŸè·å–åˆ°æ•°æ®ä¸”æ²¡æœ‰é”™è¯¯ï¼Œä¿å­˜åˆ°æ•°æ®åº“
        if isinstance(info, dict) and not info.get("error"):
            try:
                await svc.save_bond_info_from_api(code, info)
            except Exception as e:
                # ä¿å­˜å¤±è´¥ä¸å½±å“è¿”å›æ•°æ®
                import logging
                logging.warning(f"ä¿å­˜å€ºåˆ¸è¯¦æƒ…åˆ°æ•°æ®åº“å¤±è´¥: {e}")
    
    ok = isinstance(info, dict) and not info.get("error")
    return {"success": ok, "data": info}


@router.get("/yield-curve")
async def get_bond_yield_curve(
    start: Optional[str] = Query(None, description="å¼€å§‹æ—¥æœŸ YYYY-MM-DDï¼Œå¯é€‰"),
    end: Optional[str] = Query(None, description="ç»“æŸæ—¥æœŸ YYYY-MM-DDï¼Œå¯é€‰"),
    curve_name: Optional[str] = Query(None, description="æ›²çº¿åç§°ï¼Œå¯é€‰"),
    format: str = Query("json", description="è¿”å›æ ¼å¼ï¼šjson|text"),
    current_user: dict = Depends(get_current_user),
):
    """è·å–æ”¶ç›Šç‡æ›²çº¿æ•°æ®ï¼Œæ”¯æŒJSONå’Œæ–‡æœ¬ä¸¤ç§æ ¼å¼"""
    db = get_mongo_db()
    svc = BondDataService(db)
    await svc.ensure_indexes()
    
    # ä¼˜å…ˆä»æ•°æ®åº“æŸ¥è¯¢æ”¶ç›Šç‡æ›²çº¿æ•°æ®
    df = await svc.query_yield_curve(start, end, curve_name)
    
    # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®æˆ–æ•°æ®ä¸å®Œæ•´ï¼Œä»æ¥å£è·å–å¹¶ä¿å­˜
    if df is None or df.empty:
        # ä»æ¥å£è·å–æ•°æ®
        result = get_cn_bond_yield_curve_unified(start, end)
        
        # å¦‚æœæ¥å£è¿”å›æˆåŠŸï¼Œå°è¯•ä¿å­˜åˆ°æ•°æ®åº“
        if not result.startswith("âŒ") and not result.startswith("âš ï¸"):
            try:
                # ä½¿ç”¨providerç›´æ¥è·å–DataFrameä»¥ä¾¿ä¿å­˜
                provider = AKShareBondProvider()
                yield_df = await provider.get_yield_curve(start_date=start, end_date=end)
                if yield_df is not None and not yield_df.empty:
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    await svc.save_yield_curve(yield_df)
                    # é‡æ–°æŸ¥è¯¢æ•°æ®åº“
                    df = await svc.query_yield_curve(start, end, curve_name)
            except Exception as e:
                # ä¿å­˜å¤±è´¥ä¸å½±å“è¿”å›æ•°æ®
                import logging
                logging.warning(f"ä¿å­˜æ”¶ç›Šç‡æ›²çº¿æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æ•°æ®ï¼Œè¿”å›æ–‡æœ¬æ ¼å¼
        if format == "text":
            return {"success": not result.startswith("âŒ"), "data": result}
        else:
            return {"success": False, "data": [], "message": "æ— æ•°æ®"}
    else:
        # æ•°æ®åº“ä¸­æœ‰æ•°æ®
        if format == "text":
            # æ–‡æœ¬æ ¼å¼ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
            try:
                preview = df.to_string(index=False)
                rng = f"{start or '-âˆ'} åˆ° {end or '+âˆ'}"
                title = f"## ä¸­å›½å€ºåˆ¸æ”¶ç›Šç‡æ›²çº¿ ({rng})"
                result = f"{title}\n" + preview
                return {"success": True, "data": result, "from_db": True}
            except Exception as e:
                # è½¬æ¢å¤±è´¥ï¼Œå›é€€åˆ°æ¥å£
                result = get_cn_bond_yield_curve_unified(start, end)
                return {"success": not result.startswith("âŒ"), "data": result}
        else:
            # JSONæ ¼å¼ï¼ˆæ–°æ ¼å¼ï¼Œç”¨äºå‰ç«¯å›¾è¡¨ï¼‰
            try:
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                records = df.to_dict(orient="records")
                
                # è·å–ç»Ÿè®¡ä¿¡æ¯
                curve_names = df["curve_name"].unique().tolist() if "curve_name" in df.columns else []
                tenors = sorted(df["tenor"].unique().tolist()) if "tenor" in df.columns else []
                dates = sorted(df["date"].unique().tolist()) if "date" in df.columns else []
                
                # æŒ‰æ—¥æœŸå’ŒæœŸé™ç»„ç»‡æ•°æ®ï¼Œä¾¿äºå›¾è¡¨å±•ç¤º
                chart_data = {}
                for record in records:
                    date = record.get("date")
                    tenor = record.get("tenor")
                    curve = record.get("curve_name") or "default"
                    yield_val = record.get("yield")
                    
                    if date not in chart_data:
                        chart_data[date] = {}
                    if curve not in chart_data[date]:
                        chart_data[date][curve] = {}
                    chart_data[date][curve][tenor] = yield_val
                
                return {
                    "success": True,
                    "data": {
                        "records": records,
                        "chart_data": chart_data,
                        "statistics": {
                            "total_records": len(records),
                            "curve_names": curve_names,
                            "tenors": tenors,
                            "date_range": {
                                "start": dates[0] if dates else None,
                                "end": dates[-1] if dates else None,
                                "count": len(dates)
                            }
                        }
                    },
                    "from_db": True
                }
            except Exception as e:
                logger.error(f"è½¬æ¢æ”¶ç›Šç‡æ›²çº¿æ•°æ®å¤±è´¥: {e}", exc_info=True)
                return {"success": False, "error": str(e)}


@router.post("/yield-curve/sync")
async def sync_bond_yield_curve(
    start: Optional[str] = Query(None, description="å¼€å§‹æ—¥æœŸ YYYY-MM-DDï¼Œå¯é€‰"),
    end: Optional[str] = Query(None, description="ç»“æŸæ—¥æœŸ YYYY-MM-DDï¼Œå¯é€‰"),
    current_user: dict = Depends(get_current_user),
):
    provider = AKShareBondProvider()
    df = await provider.get_yield_curve(start, end)
    db = get_mongo_db()
    svc = BondDataService(db)
    await svc.ensure_indexes()
    saved = await svc.save_yield_curve(df)
    return {"success": True, "data": {"saved": saved, "rows": 0 if df is None else len(df)}}


@router.get("/collections")
async def list_bond_collections(
    current_user: dict = Depends(get_current_user),
):
    """è·å–æ‰€æœ‰å€ºåˆ¸ç›¸å…³æ•°æ®é›†åˆåˆ—è¡¨åŠå…¶è¯´æ˜"""
    collections = [
        {
            "name": "bond_basic_info",
            "display_name": "å€ºåˆ¸åŸºç¡€ä¿¡æ¯",
            "description": "å€ºåˆ¸çš„åŸºç¡€ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä»£ç ã€åç§°ã€ç±»åˆ«ã€å‘è¡Œäººã€æ¯ç¥¨ç‡ã€ä¸Šå¸‚æ—¥æœŸã€åˆ°æœŸæ—¥ç­‰",
            "route": "/bonds/collections/bond_basic_info",
            "fields": ["code", "name", "exchange", "category", "issuer", "coupon_rate", "list_date", "maturity_date", "type"],
        },
        {
            "name": "bond_daily",
            "display_name": "å€ºåˆ¸å†å²è¡Œæƒ…",
            "description": "å€ºåˆ¸çš„å†å²è¡Œæƒ…æ•°æ®ï¼ŒåŒ…æ‹¬æ—¥æœŸã€å¼€ç›˜ä»·ã€æœ€é«˜ä»·ã€æœ€ä½ä»·ã€æ”¶ç›˜ä»·ã€æˆäº¤é‡ç­‰",
            "route": "/bonds/collections/bond_daily",
            "fields": ["code", "date", "open", "high", "low", "close", "volume", "amount"],
        },
        {
            "name": "yield_curve_daily",
            "display_name": "æ”¶ç›Šç‡æ›²çº¿",
            "description": "å€ºåˆ¸æ”¶ç›Šç‡æ›²çº¿æ•°æ®ï¼ŒåŒ…æ‹¬æ—¥æœŸã€æ›²çº¿åç§°ã€æœŸé™ã€æ”¶ç›Šç‡ç­‰",
            "route": "/bonds/collections/yield_curve_daily",
            "fields": ["date", "curve_name", "tenor", "yield", "yield_type"],
        },
        {
            "name": "bond_spot_quotes",
            "display_name": "å€ºåˆ¸ç°è´§æŠ¥ä»·",
            "description": "å€ºåˆ¸ç°è´§æŠ¥ä»·æ•°æ®ï¼ŒåŒ…æ‹¬æœ€æ–°ä»·ã€æ¶¨è·Œé¢ã€æ¶¨è·Œå¹…ã€ä¹°å…¥ä»·ã€å–å‡ºä»·ç­‰",
            "route": "/bonds/collections/bond_spot_quotes",
            "fields": ["code", "timestamp", "category", "latest_price", "change", "change_percent", "buy", "sell", "volume", "amount"],
        },
        {
            "name": "bond_minute_quotes",
            "display_name": "å€ºåˆ¸åˆ†é’Ÿæ•°æ®",
            "description": "å€ºåˆ¸åˆ†é’Ÿçº§åˆ†æ—¶è¡Œæƒ…æ•°æ®ï¼ŒåŒ…æ‹¬æ—¶é—´ã€å¼€ç›˜ä»·ã€æœ€é«˜ä»·ã€æœ€ä½ä»·ã€æ”¶ç›˜ä»·ã€æˆäº¤é‡ç­‰",
            "route": "/bonds/collections/bond_minute_quotes",
            "fields": ["code", "datetime", "period", "open", "high", "low", "close", "volume", "amount"],
        },
        {
            "name": "bond_cb_profiles",
            "display_name": "å¯è½¬å€ºæ¡£æ¡ˆ",
            "description": "å¯è½¬å€ºçš„è¯¦ç»†æ¡£æ¡ˆä¿¡æ¯ï¼ŒåŒ…æ‹¬å€ºåˆ¸åŸºæœ¬ä¿¡æ¯ã€è½¬è‚¡æ¡æ¬¾ã€èµå›æ¡æ¬¾ç­‰",
            "route": "/bonds/collections/bond_cb_profiles",
            "fields": ["code", "name", "provider", "endpoint"],
        },
        {
            "name": "bond_cb_valuation_daily",
            "display_name": "å¯è½¬å€ºä¼°å€¼",
            "description": "å¯è½¬å€ºçš„ä»·å€¼åˆ†ææ•°æ®ï¼ŒåŒ…æ‹¬æ—¥æœŸã€æ”¶ç›˜ä»·ã€çº¯å€ºä»·å€¼ã€è½¬è‚¡ä»·å€¼ã€çº¯å€ºæº¢ä»·ç‡ã€è½¬è‚¡æº¢ä»·ç‡ç­‰",
            "route": "/bonds/collections/bond_cb_valuation_daily",
            "fields": ["code", "date", "close", "pure_bond_value", "convert_value", "pure_bond_premium", "convert_premium"],
        },
        {
            "name": "bond_cb_comparison",
            "display_name": "å¯è½¬å€ºæ¯”ä»·è¡¨",
            "description": "å¯è½¬å€ºä¸æ­£è‚¡çš„æ¯”ä»·æ•°æ®ï¼ŒåŒ…æ‹¬è½¬è‚¡ä»·ã€è½¬è‚¡ä»·å€¼ã€è½¬è‚¡æº¢ä»·ç‡ã€å¼ºèµè§¦å‘ä»·ã€å›å”®è§¦å‘ä»·ç­‰",
            "route": "/bonds/collections/bond_cb_comparison",
            "fields": ["code", "date", "convert_price", "convert_value", "convert_premium"],
        },
        {
            "name": "bond_cb_adjustments",
            "display_name": "å¯è½¬å€ºè½¬è‚¡ä»·æ ¼è°ƒæ•´",
            "description": "å¯è½¬å€ºè½¬è‚¡ä»·æ ¼çš„è°ƒæ•´è®°å½•ï¼ŒåŒ…æ‹¬è°ƒæ•´æ—¥æœŸã€è°ƒæ•´å‰è½¬è‚¡ä»·ã€è°ƒæ•´åè½¬è‚¡ä»·ç­‰",
            "route": "/bonds/collections/bond_cb_adjustments",
            "fields": ["code", "date", "before_price", "after_price"],
        },
        {
            "name": "bond_cb_redeems",
            "display_name": "å¯è½¬å€ºå¼ºèµ",
            "description": "å¯è½¬å€ºçš„å¼ºåˆ¶èµå›ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¼ºèµè§¦å‘ä»·ã€å¼ºèµçŠ¶æ€ã€å¼ºèµæ—¥æœŸç­‰",
            "route": "/bonds/collections/bond_cb_redeems",
            "fields": ["code", "redeem_price", "redeem_status", "redeem_date"],
        },
        {
            "name": "bond_issues",
            "display_name": "å€ºåˆ¸å‘è¡Œ",
            "description": "å€ºåˆ¸å‘è¡Œå…¬å‘Šä¿¡æ¯ï¼ŒåŒ…æ‹¬å›½å€ºã€åœ°æ–¹å€ºã€ä¼ä¸šå€ºã€å¯è½¬å€ºç­‰å„ç±»å€ºåˆ¸çš„å‘è¡Œä¿¡æ¯",
            "route": "/bonds/collections/bond_issues",
            "fields": ["code", "issue_type", "date", "issue_amount", "issue_price"],
        },
        {
            "name": "bond_buybacks",
            "display_name": "å€ºåˆ¸å›è´­",
            "description": "å€ºåˆ¸å›è´­æ•°æ®ï¼ŒåŒ…æ‹¬ä¸Šäº¤æ‰€å’Œæ·±äº¤æ‰€çš„è´¨æŠ¼å¼å›è´­è¡Œæƒ…",
            "route": "/bonds/collections/bond_buybacks",
            "fields": ["code", "exchange", "date", "price", "volume"],
        },
        {
            "name": "bond_buybacks_hist",
            "display_name": "å€ºåˆ¸å›è´­å†å²",
            "description": "å€ºåˆ¸å›è´­çš„å†å²æ•°æ®",
            "route": "/bonds/collections/bond_buybacks_hist",
            "fields": ["exchange", "date", "price", "volume"],
        },
        {
            "name": "bond_indices_daily",
            "display_name": "å€ºåˆ¸æŒ‡æ•°",
            "description": "å€ºåˆ¸æŒ‡æ•°æ•°æ®ï¼ŒåŒ…æ‹¬ä¸­å€ºç»¼åˆæŒ‡æ•°ã€ä¸­å€ºæ–°ç»¼åˆæŒ‡æ•°ã€é›†æ€å½•å¯è½¬å€ºç­‰æƒæŒ‡æ•°ç­‰",
            "route": "/bonds/collections/bond_indices_daily",
            "fields": ["index_id", "date", "value"],
        },
        {
            "name": "us_yield_daily",
            "display_name": "ç¾å›½å›½å€ºæ”¶ç›Šç‡",
            "description": "ä¸­ç¾å›½å€ºæ”¶ç›Šç‡å†å²æ•°æ®ï¼ŒåŒ…æ‹¬2å¹´ã€5å¹´ã€10å¹´ã€30å¹´ç­‰æœŸé™çš„æ”¶ç›Šç‡",
            "route": "/bonds/collections/us_yield_daily",
            "fields": ["date", "tenor", "yield"],
        },
        {
            "name": "bond_spot_quote_detail",
            "display_name": "ç°è´§æŠ¥ä»·æ˜ç»†",
            "description": "é“¶è¡Œé—´å¸‚åœºç°åˆ¸æŠ¥ä»·æ˜ç»†ï¼ŒåŒ…æ‹¬æŠ¥ä»·æœºæ„ã€å€ºåˆ¸ç®€ç§°ã€ä¹°å…¥å‡€ä»·ã€å–å‡ºå‡€ä»·ç­‰",
            "route": "/bonds/collections/bond_spot_quote_detail",
            "fields": ["code", "timestamp", "æŠ¥ä»·æœºæ„", "ä¹°å…¥å‡€ä»·", "å–å‡ºå‡€ä»·", "ä¹°å…¥æ”¶ç›Šç‡", "å–å‡ºæ”¶ç›Šç‡"],
        },
        {
            "name": "bond_spot_deals",
            "display_name": "ç°è´§æˆäº¤æ˜ç»†",
            "description": "é“¶è¡Œé—´å¸‚åœºç°åˆ¸æˆäº¤æ˜ç»†ï¼ŒåŒ…æ‹¬å€ºåˆ¸ç®€ç§°ã€æˆäº¤å‡€ä»·ã€æœ€æ–°æ”¶ç›Šç‡ã€æ¶¨è·Œç­‰",
            "route": "/bonds/collections/bond_spot_deals",
            "fields": ["code", "timestamp", "æˆäº¤å‡€ä»·", "æœ€æ–°æ”¶ç›Šç‡", "æ¶¨è·Œ", "åŠ æƒæ”¶ç›Šç‡", "äº¤æ˜“é‡"],
        },
        {
            "name": "bond_deal_summary",
            "display_name": "æˆäº¤æ¦‚è§ˆ",
            "description": "ä¸Šäº¤æ‰€å€ºåˆ¸æˆäº¤æ¦‚è§ˆï¼ŒåŒ…æ‹¬å€ºåˆ¸ç±»å‹ã€å½“æ—¥æˆäº¤ç¬”æ•°ã€å½“æ—¥æˆäº¤é‡‘é¢ç­‰",
            "route": "/bonds/collections/bond_deal_summary",
            "fields": ["date", "å€ºåˆ¸ç±»å‹", "å½“æ—¥æˆäº¤ç¬”æ•°", "å½“æ—¥æˆäº¤é‡‘é¢", "å½“å¹´æˆäº¤ç¬”æ•°", "å½“å¹´æˆäº¤é‡‘é¢"],
        },
        {
            "name": "bond_cash_summary",
            "display_name": "ç°åˆ¸å¸‚åœºæ¦‚è§ˆ",
            "description": "ä¸Šäº¤æ‰€å€ºåˆ¸ç°åˆ¸å¸‚åœºæ¦‚è§ˆï¼ŒåŒ…æ‹¬å€ºåˆ¸ç°è´§ã€æ‰˜ç®¡åªæ•°ã€æ‰˜ç®¡å¸‚å€¼ã€æ‰˜ç®¡é¢å€¼ç­‰",
            "route": "/bonds/collections/bond_cash_summary",
            "fields": ["date", "å€ºåˆ¸ç°è´§", "æ‰˜ç®¡åªæ•°", "æ‰˜ç®¡å¸‚å€¼", "æ‰˜ç®¡é¢å€¼"],
        },
        {
            "name": "bond_nafmii_debts",
            "display_name": "é“¶è¡Œé—´å¸‚åœºå€ºåŠ¡",
            "description": "é“¶è¡Œé—´å¸‚åœºéé‡‘èä¼ä¸šå€ºåŠ¡èèµ„å·¥å…·æ³¨å†Œä¿¡æ¯ï¼ŒåŒ…æ‹¬å€ºåˆ¸åç§°ã€å“ç§ã€é‡‘é¢ã€æ³¨å†Œé€šçŸ¥ä¹¦æ–‡å·ç­‰",
            "route": "/bonds/collections/bond_nafmii_debts",
            "fields": ["code", "å€ºåˆ¸åç§°", "å“ç§", "é‡‘é¢", "æ³¨å†Œé€šçŸ¥ä¹¦æ–‡å·", "æ›´æ–°æ—¥æœŸ", "é¡¹ç›®çŠ¶æ€"],
        },
        {
            "name": "bond_info_cm",
            "display_name": "ä¸­å€ºä¿¡æ¯",
            "description": "ä¸­å›½å¤–æ±‡äº¤æ˜“ä¸­å¿ƒå€ºåˆ¸ä¿¡æ¯ï¼ŒåŒ…æ‹¬å€ºåˆ¸æŸ¥è¯¢ç»“æœå’Œè¯¦ç»†ä¿¡æ¯",
            "route": "/bonds/collections/bond_info_cm",
            "fields": ["code", "endpoint", "å€ºåˆ¸ç®€ç§°", "å€ºåˆ¸ä»£ç ", "å‘è¡Œäºº/å—æ‰˜æœºæ„", "å€ºåˆ¸ç±»å‹"],
        },
        {
            "name": "bond_cov_list",
            "display_name": "å¯è½¬å€ºåˆ—è¡¨",
            "description": "ä¸œæ–¹è´¢å¯Œå¯è½¬å€ºæ•°æ®ä¸€è§ˆè¡¨ï¼ŒåŒ…æ‹¬å€ºåˆ¸ä»£ç ã€å€ºåˆ¸ç®€ç§°ã€ç”³è´­æ—¥æœŸã€è½¬è‚¡ä»·ç­‰",
            "route": "/bonds/collections/bond_cov_list",
            "fields": ["code", "å€ºåˆ¸ä»£ç ", "å€ºåˆ¸ç®€ç§°", "ç”³è´­æ—¥æœŸ", "è½¬è‚¡ä»·", "è½¬è‚¡ä»·å€¼", "è½¬è‚¡æº¢ä»·ç‡"],
        },
        {
            "name": "bond_cb_list_jsl",
            "display_name": "é›†æ€å½•å¯è½¬å€º",
            "description": "é›†æ€å½•å¯è½¬å€ºå®æ—¶æ•°æ®ï¼ŒåŒ…æ‹¬è¡Œæƒ…æ•°æ®å’ŒåŸºæœ¬ä¿¡æ¯",
            "route": "/bonds/collections/bond_cb_list_jsl",
            "fields": ["code", "è½¬å€ºåç§°", "ç°ä»·", "æ¶¨è·Œå¹…", "è½¬è‚¡ä»·", "è½¬è‚¡ä»·å€¼", "è½¬è‚¡æº¢ä»·ç‡"],
        },
        {
            "name": "bond_cb_summary",
            "display_name": "å¯è½¬å€ºå€ºåˆ¸æ¦‚å†µ",
            "description": "æ–°æµªè´¢ç»å¯è½¬å€ºå€ºåˆ¸æ¦‚å†µæ•°æ®",
            "route": "/bonds/collections/bond_cb_summary",
            "fields": ["code", "å€ºåˆ¸ç±»å‹", "ç¥¨é¢åˆ©ç‡", "å‘è¡Œä»·æ ¼", "å‘è¡Œè§„æ¨¡", "åˆ°æœŸæ—¥æœŸ"],
        },
        {
            "name": "bond_events",
            "display_name": "å€ºåˆ¸äº‹ä»¶",
            "description": "å€ºåˆ¸ç›¸å…³äº‹ä»¶è®°å½•ï¼ŒåŒ…æ‹¬è°ƒæ•´ã€èµå›ã€ä»˜æ¯ç­‰å„ç±»äº‹ä»¶",
            "route": "/bonds/collections/bond_events",
            "fields": ["code", "date", "event_type", "description"],
        },
        {
            "name": "yield_curve_map",
            "display_name": "æ”¶ç›Šç‡æ›²çº¿æ˜ å°„",
            "description": "æ”¶ç›Šç‡æ›²çº¿å¯è§†åŒ–æ˜ å°„æ•°æ®ï¼Œç”¨äºæ”¶ç›Šç‡æ›²çº¿çš„å›¾å½¢å±•ç¤º",
            "route": "/bonds/collections/yield_curve_map",
            "fields": ["date", "æ›²çº¿æ•°æ®"],
        },
    ]
    return {"success": True, "data": collections}


@router.get("/collections/{collection_name}")
async def get_collection_data(
    collection_name: str,
    page: int = Query(1, ge=1, description="é¡µç ï¼Œä»1å¼€å§‹"),
    page_size: int = Query(50, ge=1, le=500, description="æ¯é¡µæ•°é‡ï¼Œé»˜è®¤50"),
    sort_by: Optional[str] = Query(None, description="æ’åºå­—æ®µ"),
    sort_dir: str = Query("desc", description="æ’åºæ–¹å‘ï¼šasc|desc"),
    filter_field: Optional[str] = Query(None, description="è¿‡æ»¤å­—æ®µ"),
    filter_value: Optional[str] = Query(None, description="è¿‡æ»¤å€¼"),
    current_user: dict = Depends(get_current_user),
):
    """è·å–æŒ‡å®šé›†åˆçš„æ•°æ®ï¼ˆåˆ†é¡µï¼‰"""
    db = get_mongo_db()
    svc = BondDataService(db)
    
    # è·å–é›†åˆ
    collection_map = {
        "bond_basic_info": svc.col_basic,
        "bond_daily": svc.col_daily,
        "yield_curve_daily": svc.col_curve,
        "bond_spot_quotes": svc.col_spot,
        "bond_minute_quotes": svc.col_minute,
        "bond_cb_profiles": svc.col_cb_profiles,
        "bond_cb_valuation_daily": svc.col_cb_valuation,
        "bond_cb_comparison": svc.col_cb_comparison,
        "bond_cb_adjustments": svc.col_cb_adjustments,
        "bond_cb_redeems": svc.col_cb_redeems,
        "bond_issues": svc.col_issues,
        "bond_buybacks": svc.col_buybacks,
        "bond_buybacks_hist": svc.col_buybacks_hist,
        "bond_indices_daily": svc.col_indices,
        "us_yield_daily": svc.col_us_yield,
        "bond_spot_quote_detail": svc.col_spot_quote_detail,
        "bond_spot_deals": svc.col_spot_deals,
        "bond_deal_summary": svc.col_deal_summary,
        "bond_cash_summary": svc.col_cash_summary,
        "bond_nafmii_debts": svc.col_nafmii,
        "bond_info_cm": svc.col_info_cm,
        "bond_cov_list": svc.col_cov_list,
        "bond_cb_list_jsl": svc.col_cb_list_jsl,
        "bond_cb_summary": svc.col_cb_summary,
        "bond_events": svc.col_events,
        "yield_curve_map": svc.col_curve_map,
    }
    
    collection = collection_map.get(collection_name)
    if collection is None:
        return {"success": False, "error": f"é›†åˆ {collection_name} ä¸å­˜åœ¨"}
    
    try:
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        query = {}
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿ filter_field å’Œ filter_value éƒ½æ˜¯å­—ç¬¦ä¸²ä¸”éç©º
        # æ³¨æ„ï¼šéœ€è¦å…ˆæ£€æŸ¥ç±»å‹ï¼Œé¿å…å¯¹Collectionå¯¹è±¡è¿›è¡Œå¸ƒå°”è¿ç®—
        # ä½¿ç”¨try-exceptå’Œtype()æ£€æŸ¥ï¼Œå®Œå…¨é¿å…å¸ƒå°”è¿ç®—
        try:
            # å…ˆæ£€æŸ¥ç±»å‹ï¼Œä½¿ç”¨type()è€Œä¸æ˜¯isinstance()ä»¥é¿å…å¸ƒå°”è¿ç®—
            # å¿…é¡»å…ˆæ£€æŸ¥ç±»å‹ï¼Œä¸èƒ½å…ˆè¿›è¡Œå¸ƒå°”è¿ç®—ï¼ˆåŒ…æ‹¬orè¡¨è¾¾å¼ï¼‰
            filter_field_type = type(filter_field) if filter_field is not None else None
            filter_value_type = type(filter_value) if filter_value is not None else None
            
            # åªæœ‰å½“ä¸¤ä¸ªå‚æ•°éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹æ—¶æ‰å¤„ç†
            if filter_field_type is str and filter_value_type is str:
                # ç°åœ¨å¯ä»¥å®‰å…¨åœ°è°ƒç”¨strip()
                filter_field_stripped = filter_field.strip()
                filter_value_stripped = filter_value.strip()
                # æ£€æŸ¥stripåçš„ç»“æœæ˜¯å¦ä¸ºç©ºï¼ˆå­—ç¬¦ä¸²å¯ä»¥ç›´æ¥è¿›è¡Œå¸ƒå°”è¿ç®—ï¼‰
                if filter_field_stripped and filter_value_stripped:
                    # æ”¯æŒæ¨¡ç³ŠæŸ¥è¯¢
                    if filter_field_stripped in ["code", "name", "å€ºåˆ¸ç®€ç§°", "å€ºåˆ¸ä»£ç "]:
                        query[filter_field_stripped] = {"$regex": filter_value_stripped, "$options": "i"}
                    else:
                        query[filter_field_stripped] = filter_value_stripped
        except (AttributeError, NotImplementedError, TypeError) as filter_err:
            # å¦‚æœfilter_fieldæˆ–filter_valueä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼ˆå¯èƒ½æ˜¯Collectionå¯¹è±¡ï¼‰ï¼Œå¿½ç•¥è¿‡æ»¤
            logger.warning(f"âš ï¸ [é›†åˆæ•°æ®] è¿‡æ»¤å‚æ•°æ— æ•ˆï¼Œå°†å¿½ç•¥: {filter_err}")
            pass
        
        # è·å–æ€»æ•°
        total = await collection.count_documents(query)
        
        # æ„å»ºæ’åº
        # é»˜è®¤æŒ‰æ—¥æœŸå€’åºï¼Œå¦‚æœæ²¡æœ‰æ—¥æœŸå­—æ®µåˆ™æŒ‰_idå€’åº
        default_sort_key = None
        for date_field in ["date", "datetime", "timestamp", "æ›´æ–°æ—¥æœŸ", "å‘è¡Œæ—¥æœŸ", "ä¸Šå¸‚æ—¥æœŸ"]:
            # æ£€æŸ¥é›†åˆä¸­æ˜¯å¦æœ‰è¯¥å­—æ®µçš„æ–‡æ¡£
            test_doc = await collection.find_one({date_field: {"$exists": True}})
            if test_doc is not None:
                default_sort_key = date_field
                break
        
        # æ˜¾å¼é€‰æ‹©æ’åºé”®ï¼Œé¿å…ä½¿ç”¨ or é“¾å¼•å‘çš„éšå¼å¸ƒå°”æ±‚å€¼
        if sort_by is not None:
            sort_key = sort_by
        elif default_sort_key is not None:
            sort_key = default_sort_key
        else:
            sort_key = "_id"
        sort_direction = -1 if sort_dir == "desc" else 1
        
        # åˆ†é¡µæŸ¥è¯¢
        skip = (page - 1) * page_size
        cursor = collection.find(query).sort(sort_key, sort_direction).skip(skip).limit(page_size)
        items = []
        
        async for doc in cursor:
            # ç§»é™¤ _id æˆ–è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            if "_id" in doc:
                doc["_id"] = str(doc["_id"])
            items.append(doc)
        
        # è·å–å­—æ®µä¿¡æ¯ï¼ˆä»ç¬¬ä¸€æ¡è®°å½•æ¨æ–­ï¼‰
        fields_info = []
        if items:
            sample = items[0]
            for key, value in sample.items():
                if key != "_id":
                    field_type = type(value).__name__
                    if field_type == "int":
                        field_type = "æ•´æ•°"
                    elif field_type == "float":
                        field_type = "æµ®ç‚¹æ•°"
                    elif field_type == "bool":
                        field_type = "å¸ƒå°”å€¼"
                    elif field_type == "list":
                        field_type = "åˆ—è¡¨"
                    elif field_type == "dict":
                        field_type = "å¯¹è±¡"
                    else:
                        field_type = "å­—ç¬¦ä¸²"
                    fields_info.append({
                        "name": key,
                        "type": field_type,
                        "example": str(value)[:50] if value is not None else None,
                    })
        
        return {
            "success": True,
            "data": {
                "items": items,
                "total": total,
                "page": page,
                "page_size": page_size,
                "fields": fields_info,
            },
        }
    except Exception as e:
        logger.error(f"è·å–é›†åˆ {collection_name} æ•°æ®å¤±è´¥: {e}", exc_info=True)
        return {"success": False, "error": str(e)}


@router.get("/collections/{collection_name}/stats")
async def get_collection_stats(
    collection_name: str,
    current_user: dict = Depends(get_current_user),
):
    """è·å–æŒ‡å®šé›†åˆçš„ç»Ÿè®¡ä¿¡æ¯"""
    db = get_mongo_db()
    svc = BondDataService(db)
    
    collection_map = {
        "bond_basic_info": svc.col_basic,
        "bond_daily": svc.col_daily,
        "yield_curve_daily": svc.col_curve,
        "bond_spot_quotes": svc.col_spot,
        "bond_minute_quotes": svc.col_minute,
        "bond_cb_profiles": svc.col_cb_profiles,
        "bond_cb_valuation_daily": svc.col_cb_valuation,
        "bond_cb_comparison": svc.col_cb_comparison,
        "bond_cb_adjustments": svc.col_cb_adjustments,
        "bond_cb_redeems": svc.col_cb_redeems,
        "bond_issues": svc.col_issues,
        "bond_buybacks": svc.col_buybacks,
        "bond_buybacks_hist": svc.col_buybacks_hist,
        "bond_indices_daily": svc.col_indices,
        "us_yield_daily": svc.col_us_yield,
        "bond_spot_quote_detail": svc.col_spot_quote_detail,
        "bond_spot_deals": svc.col_spot_deals,
        "bond_deal_summary": svc.col_deal_summary,
        "bond_cash_summary": svc.col_cash_summary,
        "bond_nafmii_debts": svc.col_nafmii,
        "bond_info_cm": svc.col_info_cm,
        "bond_cov_list": svc.col_cov_list,
        "bond_cb_list_jsl": svc.col_cb_list_jsl,
        "bond_cb_summary": svc.col_cb_summary,
        "bond_events": svc.col_events,
        "yield_curve_map": svc.col_curve_map,
    }
    
    collection = collection_map.get(collection_name)
    if collection is None:
        return {"success": False, "error": f"é›†åˆ {collection_name} ä¸å­˜åœ¨"}
    
    try:
        logger.info(f"ğŸ“Š [é›†åˆç»Ÿè®¡] å¼€å§‹è·å–é›†åˆ {collection_name} çš„ç»Ÿè®¡ä¿¡æ¯")
        
        # æ€»è®°å½•æ•°
        try:
            total_count = await collection.count_documents({})
            logger.info(f"ğŸ“Š [é›†åˆç»Ÿè®¡] é›†åˆ {collection_name} æ€»è®°å½•æ•°: {total_count}")
        except Exception as count_err:
            logger.error(f"âŒ [é›†åˆç»Ÿè®¡] è·å–æ€»è®°å½•æ•°å¤±è´¥: {count_err}", exc_info=True)
            total_count = 0
        
        # è·å–æœ€æ—©å’Œæœ€æ™šçš„æ—¥æœŸï¼ˆå¦‚æœæœ‰dateæˆ–datetimeå­—æ®µï¼‰
        stats = {
            "total_count": total_count,
            "collection_name": collection_name,
        }
        
        # å°è¯•è·å–æ—¥æœŸèŒƒå›´
        date_fields = ["date", "datetime", "timestamp", "æ›´æ–°æ—¥æœŸ", "å‘è¡Œæ—¥æœŸ", "ä¸Šå¸‚æ—¥æœŸ"]
        for date_field in date_fields:
            try:
                # æŸ¥æ‰¾æœ‰è¯¥å­—æ®µçš„æ–‡æ¡£
                first_doc = await collection.find_one({date_field: {"$exists": True}}, sort=[(date_field, 1)])
                last_doc = await collection.find_one({date_field: {"$exists": True}}, sort=[(date_field, -1)])
                
                if first_doc is not None and last_doc is not None:
                    first_date = first_doc.get(date_field)
                    last_date = last_doc.get(date_field)
                    if first_date:
                        stats["earliest_date"] = str(first_date)[:10]
                    if last_date:
                        stats["latest_date"] = str(last_date)[:10]
                    stats["date_field"] = date_field
                    logger.info(f"ğŸ“Š [é›†åˆç»Ÿè®¡] æ‰¾åˆ°æ—¥æœŸå­—æ®µ {date_field}: {stats.get('earliest_date')} - {stats.get('latest_date')}")
                    break
            except Exception as date_err:
                logger.debug(f"âš ï¸ [é›†åˆç»Ÿè®¡] è·å–æ—¥æœŸå­—æ®µ {date_field} å¤±è´¥: {date_err}")
                continue
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰categoryå­—æ®µï¼‰
        try:
            pipeline = [
                {"$group": {"_id": "$category", "count": {"$sum": 1}}},
                {"$sort": {"count": -1}},
            ]
            category_stats = []
            async for doc in collection.aggregate(pipeline):
                category_id = doc.get("_id")
                count = doc.get("count", 0)
                category_stats.append({
                    "category": str(category_id) if category_id is not None else "æœªçŸ¥",
                    "count": int(count)
                })
            if len(category_stats) > 0:
                stats["category_stats"] = category_stats
                logger.info(f"ğŸ“Š [é›†åˆç»Ÿè®¡] æ‰¾åˆ° {len(category_stats)} ä¸ªç±»åˆ«")
        except Exception as cat_err:
            logger.debug(f"âš ï¸ [é›†åˆç»Ÿè®¡] è·å–ç±»åˆ«ç»Ÿè®¡å¤±è´¥: {cat_err}")
            pass
        
        # æŒ‰äº¤æ˜“æ‰€ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰exchangeå­—æ®µï¼‰
        try:
            pipeline = [
                {"$group": {"_id": "$exchange", "count": {"$sum": 1}}},
                {"$sort": {"count": -1}},
            ]
            exchange_stats = []
            async for doc in collection.aggregate(pipeline):
                exchange_id = doc.get("_id")
                count = doc.get("count", 0)
                exchange_stats.append({
                    "exchange": str(exchange_id) if exchange_id is not None else "æœªçŸ¥",
                    "count": int(count)
                })
            if len(exchange_stats) > 0:
                stats["exchange_stats"] = exchange_stats
                logger.info(f"ğŸ“Š [é›†åˆç»Ÿè®¡] æ‰¾åˆ° {len(exchange_stats)} ä¸ªäº¤æ˜“æ‰€")
        except Exception as exch_err:
            logger.debug(f"âš ï¸ [é›†åˆç»Ÿè®¡] è·å–äº¤æ˜“æ‰€ç»Ÿè®¡å¤±è´¥: {exch_err}")
            pass
        
        logger.info(f"âœ… [é›†åˆç»Ÿè®¡] é›†åˆ {collection_name} ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ")
        return {"success": True, "data": stats}
    except HTTPException:
        # HTTPExceptionåº”è¯¥ç›´æ¥æŠ›å‡º
        raise
    except Exception as e:
        logger.error(f"âŒ [é›†åˆç»Ÿè®¡] è·å–é›†åˆ {collection_name} ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
        import traceback
        error_trace = traceback.format_exc()
        logger.error(f"âŒ [é›†åˆç»Ÿè®¡] é”™è¯¯å †æ ˆ: {error_trace}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}"
        )


# å€ºåˆ¸åˆ†æç›¸å…³æ¨¡å‹
class BondAnalysisRequest(BaseModel):
    bond_code: str
    parameters: Optional[Dict[str, Any]] = {}


@router.post("/analysis")
async def start_bond_analysis(
    request: BondAnalysisRequest,
    background_tasks: BackgroundTasks,
    current_user: dict = Depends(get_current_user),
):
    """æäº¤å€ºåˆ¸åˆ†æä»»åŠ¡"""
    try:
        logger.info(f"ğŸ¯ æ”¶åˆ°å€ºåˆ¸åˆ†æè¯·æ±‚: {request.bond_code}")
        
        # éªŒè¯å€ºåˆ¸ä»£ç æ ¼å¼
        bond_code = request.bond_code.strip()
        import re
        if not re.match(r'^\d{6}\.(SH|SZ|IB)$', bond_code, re.IGNORECASE):
            raise HTTPException(status_code=400, detail="å€ºåˆ¸ä»£ç æ ¼å¼ä¸æ­£ç¡®ï¼Œåº”ä¸ºï¼šä»£ç .äº¤æ˜“æ‰€ï¼ˆå¦‚ï¼š110062.SHï¼‰")
        
        # åˆ›å»ºä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # å¯¼å…¥åˆ†ææœåŠ¡
        from app.services.bond_analysis_service import get_bond_analysis_service
        service = get_bond_analysis_service()
        
        # åˆ›å»ºä»»åŠ¡è®°å½•
        result = await service.create_analysis_task(
            user_id=current_user["id"],
            task_id=task_id,
            request=request
        )
        
        # åœ¨åå°æ‰§è¡Œåˆ†æä»»åŠ¡
        async def run_analysis_task():
            try:
                logger.info(f"ğŸš€ [BackgroundTask] å¼€å§‹æ‰§è¡Œå€ºåˆ¸åˆ†æä»»åŠ¡: {task_id}")
                await service.execute_analysis_background(task_id, current_user["id"], request)
                logger.info(f"âœ… [BackgroundTask] å€ºåˆ¸åˆ†æä»»åŠ¡å®Œæˆ: {task_id}")
            except Exception as e:
                logger.error(f"âŒ [BackgroundTask] å€ºåˆ¸åˆ†æä»»åŠ¡å¤±è´¥: {task_id}, é”™è¯¯: {e}", exc_info=True)
        
        background_tasks.add_task(run_analysis_task)
        
        return {
            "success": True,
            "data": {"task_id": task_id},
            "message": "åˆ†æä»»åŠ¡å·²åœ¨åå°å¯åŠ¨"
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ æäº¤å€ºåˆ¸åˆ†æä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/analysis/{task_id}/status")
async def get_bond_analysis_status(
    task_id: str,
    current_user: dict = Depends(get_current_user),
):
    """è·å–å€ºåˆ¸åˆ†æä»»åŠ¡çŠ¶æ€"""
    try:
        from app.services.bond_analysis_service import get_bond_analysis_service
        service = get_bond_analysis_service()
        
        status = await service.get_task_status(task_id)
        
        if not status:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        return {
            "success": True,
            "data": status
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ è·å–å€ºåˆ¸åˆ†æä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/analysis/{task_id}/result")
async def get_bond_analysis_result(
    task_id: str,
    current_user: dict = Depends(get_current_user),
):
    """è·å–å€ºåˆ¸åˆ†æç»“æœ"""
    try:
        from app.services.bond_analysis_service import get_bond_analysis_service
        service = get_bond_analysis_service()
        
        result = await service.get_task_result(task_id)
        
        if not result:
            raise HTTPException(status_code=404, detail="åˆ†æç»“æœä¸å­˜åœ¨")
        
        return {
            "success": True,
            "data": result
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ è·å–å€ºåˆ¸åˆ†æç»“æœå¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/{code}/history/sync")
async def sync_bond_history_to_db(
    code: str,
    start: str = Query(..., description="å¼€å§‹æ—¥æœŸ YYYY-MM-DD"),
    end: str = Query(..., description="ç»“æŸæ—¥æœŸ YYYY-MM-DD"),
    current_user: dict = Depends(get_current_user),
):
    provider = AKShareBondProvider()
    df = await provider.get_historical_data(code, start, end, period="daily")
    db = get_mongo_db()
    svc = BondDataService(db)
    await svc.ensure_indexes()
    norm = normalize_bond_code(code)
    code_std = norm.get("code_std") or code
    saved = await svc.save_bond_daily(code_std, df)
    return {"success": True, "data": {"saved": saved, "rows": 0 if df is None else len(df)}}

from typing import Optional, Iterable, Dict, Any
from datetime import datetime
import pandas as pd
from motor.motor_asyncio import AsyncIOMotorDatabase
from pymongo import UpdateOne


class BondDataService:
    def __init__(self, db: AsyncIOMotorDatabase) -> None:
        self.db = db
        self.col_basic = db.get_collection("bond_basic_info")
        self.col_daily = db.get_collection("bond_daily")
        self.col_curve = db.get_collection("yield_curve_daily")
        self.col_events = db.get_collection("bond_events")
        self.col_spot = db.get_collection("bond_spot_quotes")
        self.col_indices = db.get_collection("bond_indices_daily")
        self.col_us_yield = db.get_collection("us_yield_daily")
        self.col_cb_profiles = db.get_collection("bond_cb_profiles")
        self.col_buybacks = db.get_collection("bond_buybacks")
        # æœªè¦†ç›–ç«¯ç‚¹çš„é›†åˆ
        self.col_issues = db.get_collection("bond_issues")
        self.col_cb_adjustments = db.get_collection("bond_cb_adjustments")
        self.col_cb_redeems = db.get_collection("bond_cb_redeems")
        self.col_cb_summary = db.get_collection("bond_cb_summary")
        self.col_cb_valuation = db.get_collection("bond_cb_valuation_daily")
        self.col_cb_comparison = db.get_collection("bond_cb_comparison")
        self.col_spot_quote_detail = db.get_collection("bond_spot_quote_detail")
        self.col_spot_deals = db.get_collection("bond_spot_deals")
        self.col_deal_summary = db.get_collection("bond_deal_summary")
        self.col_cash_summary = db.get_collection("bond_cash_summary")
        self.col_nafmii = db.get_collection("bond_nafmii_debts")
        self.col_info_cm = db.get_collection("bond_info_cm")
        self.col_curve_map = db.get_collection("yield_curve_map")
        self.col_buybacks_hist = db.get_collection("bond_buybacks_hist")
        self.col_cb_list_jsl = db.get_collection("bond_cb_list_jsl")
        self.col_cov_list = db.get_collection("bond_cov_list")
        self.col_minute = db.get_collection("bond_minute_quotes")

    async def _safe_create_index(self, collection, index_spec, unique=False, sparse=False, name=None):
        """å®‰å…¨åˆ›å»ºç´¢å¼•ï¼Œå¦‚æœç´¢å¼•å·²å­˜åœ¨åˆ™è·³è¿‡"""
        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šåç§°ï¼Œå°è¯•ä»ç´¢å¼•è§„èŒƒç”Ÿæˆ
            if name is None:
                if isinstance(index_spec, list):
                    # å¤åˆç´¢å¼•
                    name_parts = [f"{field}_{direction}" for field, direction in index_spec]
                    name = "_".join(name_parts)
                else:
                    # å•å­—æ®µç´¢å¼•
                    name = f"{index_spec}_1"
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
            existing_indexes = await collection.list_indexes().to_list(length=None)
            for idx in existing_indexes:
                if idx.get("name") == name:
                    # æ£€æŸ¥ç´¢å¼•å®šä¹‰æ˜¯å¦åŒ¹é…
                    idx_keys = list(idx.get("key", {}).keys())
                    if isinstance(index_spec, list):
                        expected_keys = [field for field, _ in index_spec]
                    else:
                        expected_keys = [index_spec]
                    
                    idx_unique = idx.get("unique", False)
                    idx_sparse = idx.get("sparse", False)
                    
                    # å¦‚æœé”®åŒ¹é…ä¸”å±æ€§åŒ¹é…ï¼Œåˆ™è·³è¿‡
                    if idx_keys == expected_keys and idx_unique == unique and idx_sparse == sparse:
                        return
                    # å¦‚æœé”®åŒ¹é…ä½†å±æ€§ä¸åŒ¹é…ï¼Œéœ€è¦åˆ é™¤æ—§ç´¢å¼•
                    elif idx_keys == expected_keys:
                        try:
                            await collection.drop_index(name)
                        except Exception as drop_err:
                            pass  # å¿½ç•¥åˆ é™¤å¤±è´¥
            
            # åˆ›å»ºç´¢å¼•
            if isinstance(index_spec, list):
                await collection.create_index(index_spec, unique=unique, sparse=sparse, name=name)
            else:
                await collection.create_index(index_spec, unique=unique, sparse=sparse, name=name)
        except Exception as e:
            # å¿½ç•¥ç´¢å¼•å·²å­˜åœ¨çš„é”™è¯¯å’Œç´¢å¼•å†²çªé”™è¯¯
            import logging
            logger = logging.getLogger("webapi")
            error_str = str(e).lower()
            if any(keyword in error_str for keyword in ["already exists", "duplicate key", "indexkeyspecsconflict", "index key specs conflict"]):
                # å¦‚æœæ˜¯ç´¢å¼•å†²çªï¼Œå°è¯•åˆ é™¤æ—§ç´¢å¼•å¹¶é‡æ–°åˆ›å»º
                try:
                    if name:
                        await collection.drop_index(name)
                        # é‡æ–°åˆ›å»ºç´¢å¼•
                        if isinstance(index_spec, list):
                            await collection.create_index(index_spec, unique=unique, sparse=sparse, name=name)
                        else:
                            await collection.create_index(index_spec, unique=unique, sparse=sparse, name=name)
                except Exception as retry_err:
                    # å¦‚æœé‡è¯•ä¹Ÿå¤±è´¥ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­
                    logger.warning(f"âš ï¸ å¤„ç†ç´¢å¼•å†²çªåé‡è¯•åˆ›å»ºå¤±è´¥: {retry_err}")
            else:
                logger.warning(f"âš ï¸ åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")

    async def ensure_indexes(self) -> None:
        await self.col_basic.create_index("code", unique=True)
        await self.col_basic.create_index("category")
        await self.col_basic.create_index("maturity_date")
        await self.col_basic.create_index("exchange")
        await self.col_basic.create_index("list_date")
        await self.col_basic.create_index("coupon_rate")
        await self.col_basic.create_index("name")
        await self.col_daily.create_index([("code", 1), ("date", 1)], unique=True)
        # æ”¶ç›Šç‡æ›²çº¿ç´¢å¼•ï¼šä½¿ç”¨ (date, tenor, curve_name, yield_type?) ä½œä¸ºå”¯ä¸€é”®
        # curve_name ä¸ºç©ºå­—ç¬¦ä¸²è¡¨ç¤ºæœªåˆ†ç±»çš„æ›²çº¿
        # yield_type å¯é€‰ï¼Œç”¨äºåŒºåˆ†åˆ°æœŸæ”¶ç›Šç‡ã€å³æœŸæ”¶ç›Šç‡ç­‰
        await self._safe_create_index(
            self.col_curve,
            [("date", 1), ("tenor", 1), ("curve_name", 1), ("yield_type", 1)],
            unique=True,
            sparse=True,
            name="date_1_tenor_1_curve_name_1_yield_type_1"
        )
        await self._safe_create_index(
            self.col_curve,
            [("date", 1), ("tenor", 1), ("curve_name", 1)],
            unique=True,
            name="date_1_tenor_1_curve_name_1"
        )
        await self.col_curve.create_index("date")
        await self.col_curve.create_index("curve_name")
        # æ³¨æ„ï¼šè¿™ä¸ªéå”¯ä¸€ç´¢å¼•å¯èƒ½ä¸ä¹‹å‰çš„å”¯ä¸€ç´¢å¼•å†²çªï¼Œä½¿ç”¨å®‰å…¨åˆ›å»ºæ–¹æ³•
        await self._safe_create_index(
            self.col_curve,
            [("date", 1), ("tenor", 1)],
            unique=False,
            name="date_1_tenor_1_query"
        )
        await self.col_events.create_index([("code", 1), ("date", 1), ("event_type", 1)], unique=True)
        await self.col_spot.create_index([("code", 1), ("timestamp", 1), ("category", 1)], unique=True)
        await self.col_indices.create_index([("index_id", 1), ("date", 1)], unique=True)
        # us_yield é›†åˆçš„ç´¢å¼•å¯èƒ½ä¸å…¶ä»–é›†åˆå†²çªï¼Œä½¿ç”¨å®‰å…¨åˆ›å»º
        await self._safe_create_index(
            self.col_us_yield,
            [("date", 1), ("tenor", 1)],
            unique=True,
            name="date_1_tenor_1"
        )
        await self.col_cb_profiles.create_index("code", unique=True)
        await self.col_buybacks.create_index([("exchange", 1), ("date", 1), ("code", 1)], unique=True)
        # æ–°å¢ç´¢å¼•
        await self.col_issues.create_index([("issue_type", 1), ("code", 1), ("date", 1)], unique=True)
        await self.col_cb_adjustments.create_index([("code", 1), ("date", 1)], unique=True)
        await self.col_cb_redeems.create_index([("code", 1), ("date", 1)], unique=True)
        await self.col_cb_summary.create_index("code", unique=True)
        await self.col_cb_valuation.create_index([("code", 1), ("date", 1)], unique=True)
        await self.col_cb_comparison.create_index([("date", 1), ("code", 1)], unique=True)
        await self.col_spot_quote_detail.create_index([("code", 1), ("timestamp", 1), ("æŠ¥ä»·æœºæ„", 1)], unique=True)
        await self.col_spot_deals.create_index([("code", 1), ("timestamp", 1)], unique=True)
        await self.col_deal_summary.create_index([("date", 1), ("å€ºåˆ¸ç±»å‹", 1)], unique=True)
        await self.col_cash_summary.create_index([("date", 1), ("å€ºåˆ¸ç°è´§", 1)], unique=True)
        # NAFMII ä½¿ç”¨æ³¨å†Œé€šçŸ¥ä¹¦æ–‡å·ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼ˆéƒ¨åˆ†è®°å½•æ—  reg_no æ—¶ä»å¯å†™å…¥ï¼Œä½†å»ºè®®æ•°æ®æºå°½é‡æä¾›ï¼‰
        await self._safe_create_index(self.col_nafmii, "reg_no", unique=True, sparse=True, name="reg_no_1")
        await self.col_info_cm.create_index("code")
        # å”¯ä¸€ç´¢å¼•ï¼šæ¯ä¸ªå€ºåˆ¸çš„æ¯ä¸ªæ¥å£åªæœ‰ä¸€æ¡è®°å½•
        await self.col_info_cm.create_index([("code", 1), ("endpoint", 1)], unique=True)
        await self.col_curve_map.create_index("date", unique=True)
        await self.col_buybacks_hist.create_index([("exchange", 1), ("date", 1)], unique=True)
        await self.col_cb_list_jsl.create_index("code", unique=True)
        await self.col_cov_list.create_index("code", unique=True)
        # åˆ†é’Ÿæ•°æ®ç´¢å¼•ï¼šä½¿ç”¨ (code, datetime, period) ä½œä¸ºå”¯ä¸€é”®
        await self.col_minute.create_index([("code", 1), ("datetime", 1), ("period", 1)], unique=True)
        await self.col_minute.create_index("code")
        await self.col_minute.create_index("datetime")
        # åˆ†é’Ÿæ•°æ®çš„æŸ¥è¯¢ç´¢å¼•ï¼Œå¯èƒ½ä¸å”¯ä¸€ç´¢å¼•å†²çªï¼Œä½¿ç”¨å®‰å…¨åˆ›å»º
        await self._safe_create_index(
            self.col_minute,
            [("code", 1), ("datetime", 1)],
            unique=False,
            name="code_1_datetime_1_query"
        )

    async def save_yield_curve(self, df: pd.DataFrame) -> int:
        """ä¿å­˜æ”¶ç›Šç‡æ›²çº¿æ•°æ®åˆ°æ•°æ®åº“ï¼ˆè¿‡æ»¤éæ•°å€¼æ•°æ®ï¼‰"""
        import logging
        logger = logging.getLogger("webapi")
        
        if df is None or df.empty:
            logger.warning("âš ï¸ [æ”¶ç›Šç‡æ›²çº¿ä¿å­˜] DataFrameä¸ºç©º")
            return 0
        
        ops = []
        valid_count = 0
        skipped_count = 0
        
        for _, r in df.iterrows():
            try:
                date_val = r.get("date")
                tenor_val = r.get("tenor")
                yield_val = r.get("yield")
                
                # éªŒè¯æ—¥æœŸ
                if pd.isna(date_val) or not date_val:
                    skipped_count += 1
                    continue
                
                # éªŒè¯æœŸé™ï¼ˆtenorï¼‰
                if pd.isna(tenor_val) or not tenor_val:
                    skipped_count += 1
                    continue
                
                # éªŒè¯æ”¶ç›Šç‡å€¼ï¼ˆyieldï¼‰- å¿…é¡»æ˜¯æ•°å€¼
                if pd.isna(yield_val):
                    skipped_count += 1
                    continue
                
                # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                try:
                    # å¦‚æœå·²ç»æ˜¯æ•°å€¼ç±»å‹ï¼Œç›´æ¥è½¬æ¢
                    if isinstance(yield_val, (int, float)):
                        yield_float = float(yield_val)
                    elif isinstance(yield_val, str):
                        # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                        # å¦‚æœåŒ…å«éæ•°å­—å­—ç¬¦ï¼ˆæ¯”å¦‚ä¸­æ–‡ï¼‰ï¼Œä¼šæŠ›å‡ºå¼‚å¸¸
                        yield_float = float(yield_val.strip())
                    else:
                        # å°è¯•å¼ºåˆ¶è½¬æ¢
                        yield_float = float(yield_val)
                    
                    # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°å€¼ï¼ˆä¸æ˜¯ NaN æˆ– Infï¼‰
                    if pd.isna(yield_float) or not pd.isfinite(yield_float):
                        skipped_count += 1
                        continue
                        
                except (ValueError, TypeError) as e:
                    # æ— æ³•è½¬æ¢ä¸ºæ•°å€¼ï¼Œè·³è¿‡è¿™æ¡è®°å½•
                    skipped_count += 1
                    logger.debug(f"âš ï¸ [æ”¶ç›Šç‡æ›²çº¿ä¿å­˜] è·³è¿‡éæ•°å€¼æ•°æ®: date={date_val}, tenor={tenor_val}, yield={yield_val}, error={e}")
                    continue
                
                # æ„å»ºæ–‡æ¡£ï¼ˆåŒ…å«æ›²çº¿åç§°ï¼Œå¦‚æœå­˜åœ¨ï¼‰
                doc = {
                    "date": str(date_val),
                    "tenor": str(tenor_val).strip(),
                    "yield": yield_float,
                    "source": "akshare",
                }
                
                # å¦‚æœå­˜åœ¨æ›²çº¿åç§°ï¼Œä¹Ÿä¿å­˜
                if "curve_name" in r:
                    curve_name = r.get("curve_name")
                    if curve_name and not pd.isna(curve_name):
                        doc["curve_name"] = str(curve_name).strip()
                
                # å¦‚æœå­˜åœ¨æ”¶ç›Šç‡ç±»å‹ï¼Œä¹Ÿä¿å­˜ï¼ˆç”¨äºåŒºåˆ†åˆ°æœŸæ”¶ç›Šç‡ã€å³æœŸæ”¶ç›Šç‡ç­‰ï¼‰
                if "yield_type" in r:
                    yield_type = r.get("yield_type")
                    if yield_type and not pd.isna(yield_type):
                        doc["yield_type"] = str(yield_type).strip()
                
                # ä½¿ç”¨ (date, tenor, curve_name, yield_type?) ä½œä¸ºå”¯ä¸€é”®
                # å¦‚æœæ²¡æœ‰ curve_nameï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä½œä¸ºé»˜è®¤å€¼
                unique_key = {
                    "date": doc["date"],
                    "tenor": doc["tenor"],
                    "curve_name": doc.get("curve_name", "")
                }
                # å¦‚æœæœ‰ yield_typeï¼Œä¹ŸåŠ å…¥åˆ°å”¯ä¸€é”®ä¸­
                if "yield_type" in doc:
                    unique_key["yield_type"] = doc["yield_type"]
                
                ops.append(
                    UpdateOne(
                        unique_key,
                        {"$set": doc},
                        upsert=True
                    )
                )
                valid_count += 1
                
            except Exception as e:
                skipped_count += 1
                logger.warning(f"âš ï¸ [æ”¶ç›Šç‡æ›²çº¿ä¿å­˜] å¤„ç†è¡Œæ•°æ®å¤±è´¥: {e}, row={dict(r)}")
                continue
        
        if not ops:
            logger.warning(f"âš ï¸ [æ”¶ç›Šç‡æ›²çº¿ä¿å­˜] æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜ï¼ˆè·³è¿‡ {skipped_count} æ¡æ— æ•ˆæ•°æ®ï¼Œæ€»è®¡ {len(df)} æ¡ï¼‰")
            return 0
        
        try:
            # æ‰§è¡Œæ‰¹é‡å†™å…¥
            res = await self.col_curve.bulk_write(ops, ordered=False)
            upserted = res.upserted_count or 0
            modified = res.modified_count or 0
            total_saved = upserted + modified
            
            logger.info(f"ğŸ’¾ [æ”¶ç›Šç‡æ›²çº¿ä¿å­˜] æ‰¹é‡å†™å…¥å®Œæˆ: æ–°å¢={upserted}, æ›´æ–°={modified}, æ€»è®¡={total_saved}, æœ‰æ•ˆæ•°æ®={valid_count}, è·³è¿‡={skipped_count}, æ€»è¡Œæ•°={len(df)}")
            
            return total_saved
        except Exception as e:
            logger.error(f"âŒ [æ”¶ç›Šç‡æ›²çº¿ä¿å­˜] æ‰¹é‡å†™å…¥å¤±è´¥: {e}", exc_info=True)
            return 0

    async def save_bond_daily(self, code: str, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            doc = {k: r.get(k) for k in df.columns}
            doc["code"] = code
            doc["date"] = str(doc.get("date"))
            ops.append(
                UpdateOne({"code": doc["code"], "date": doc["date"]}, {"$set": doc}, upsert=True)
            )
        if ops:
            res = await self.col_daily.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_basic_list(self, items: Iterable[Dict[str, Any]]) -> int:
        """ä¿å­˜å€ºåˆ¸åŸºç¡€ä¿¡æ¯åˆ—è¡¨åˆ°æ•°æ®åº“ï¼ˆä½¿ç”¨codeä½œä¸ºå”¯ä¸€é”®ï¼Œå­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥ï¼‰"""
        import logging
        logger = logging.getLogger("webapi")
        
        ops = []
        valid_count = 0
        skipped_count = 0
        
        for it in items:
            code = str(it.get("code") or "").strip()
            if not code:
                skipped_count += 1
                continue
            
            # è§„èŒƒåŒ–codeï¼ˆç¡®ä¿æ ¼å¼ä¸€è‡´ï¼‰
            from tradingagents.utils.instrument_validator import normalize_bond_code
            norm = normalize_bond_code(code)
            code_std = norm.get("code_std") or code
            
            # æ„å»ºæ–‡æ¡£ï¼Œç§»é™¤Noneå€¼
            doc = {
                "code": code_std,
                "name": it.get("name"),
                "exchange": it.get("exchange"),
                "category": (it.get("category") or "").lower() or None,
                "issuer": it.get("issuer"),
                "list_date": str(it.get("list_date")) if it.get("list_date") else None,
                "maturity_date": str(it.get("maturity_date")) if it.get("maturity_date") else None,
                "coupon_rate": it.get("coupon_rate"),
                "type": it.get("type"),
                "raw_code": it.get("raw_code"),
                "source": it.get("source", "akshare"),
                "updated_at": datetime.now().isoformat(),  # æ·»åŠ æ›´æ–°æ—¶é—´
            }
            
            # ç§»é™¤Noneå€¼ï¼Œé¿å…è¦†ç›–å·²æœ‰å­—æ®µä¸ºNone
            doc = {k: v for k, v in doc.items() if v is not None}
            
            # ä½¿ç”¨codeä½œä¸ºå”¯ä¸€é”®ï¼Œå­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥
            ops.append(UpdateOne(
                {"code": code_std},
                {"$set": doc, "$setOnInsert": {"created_at": datetime.now().isoformat()}},
                upsert=True
            ))
            valid_count += 1
        
        if not ops:
            logger.warning(f"âš ï¸ [å€ºåˆ¸æ•°æ®ä¿å­˜] æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜ï¼ˆè·³è¿‡ {skipped_count} æ¡æ— æ•ˆæ•°æ®ï¼‰")
            return 0
        
        try:
            # æ‰§è¡Œæ‰¹é‡å†™å…¥
            res = await self.col_basic.bulk_write(ops, ordered=False)
            upserted = res.upserted_count or 0
            modified = res.modified_count or 0
            matched = res.matched_count or 0
            
            # ç»Ÿè®¡å¤„ç†çš„æ•°æ®æ•°é‡ï¼š
            # - upserted_count: æ–°æ’å…¥çš„æ•°æ®
            # - modified_count: æ›´æ–°çš„æ•°æ®
            # - matched_count: åŒ¹é…åˆ°çš„æ•°æ®ï¼ˆå³ä½¿æ²¡æœ‰ä¿®æ”¹ä¹Ÿä¼šè¢«è®¡æ•°ï¼‰
            # å¦‚æœ matched_count > (upserted + modified)ï¼Œè¯´æ˜æœ‰äº›æ•°æ®å·²å­˜åœ¨ä¸”æ— éœ€æ›´æ–°
            total_processed = upserted + modified
            # å¦‚æœ matched_count å¤§äº processedï¼Œè¯´æ˜æœ‰äº›æ•°æ®å·²å­˜åœ¨ä½†æ²¡æœ‰å˜åŒ–
            if matched > total_processed:
                # å®é™…å¤„ç†çš„æ•°é‡åº”è¯¥åŒ…æ‹¬å·²å­˜åœ¨ä½†æœªæ›´æ–°çš„æ•°æ®
                total_processed = matched
            
            logger.info(f"ğŸ’¾ [å€ºåˆ¸æ•°æ®ä¿å­˜] æ‰¹é‡å†™å…¥å®Œæˆ: æ–°å¢={upserted}, æ›´æ–°={modified}, åŒ¹é…={matched}, æ€»è®¡={total_processed}, æœ‰æ•ˆæ•°æ®={valid_count}, è·³è¿‡={skipped_count}")
            
            # å¦‚æœä¿å­˜æ•°é‡å¼‚å¸¸ï¼Œè®°å½•è­¦å‘Š
            if total_processed == 0 and valid_count > 0:
                logger.warning(f"âš ï¸ [å€ºåˆ¸æ•°æ®ä¿å­˜] è­¦å‘Šï¼šæœ‰ {valid_count} æ¡æœ‰æ•ˆæ•°æ®ï¼Œä½†ä¿å­˜æ•°é‡ä¸º0ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®æ ¼å¼é—®é¢˜")
            
            return total_processed if total_processed > 0 else (upserted + modified)
        except Exception as e:
            logger.error(f"âŒ [å€ºåˆ¸æ•°æ®ä¿å­˜] æ‰¹é‡å†™å…¥å¤±è´¥: {e}", exc_info=True)
            return 0

    async def query_basic_list(
        self,
        q: Optional[str] = None,
        category: Optional[str] = None,
        exchange: Optional[str] = None,
        only_not_matured: bool = False,
        page: int = 1,
        page_size: int = 20,
        sort_by: Optional[str] = None,
        sort_dir: str = "asc",
    ) -> Dict[str, Any]:
        filt: Dict[str, Any] = {}
        if q:
            q_regex = {"$regex": q, "$options": "i"}
            filt["$or"] = [{"code": q_regex}, {"name": q_regex}]
        if category:
            filt["category"] = str(category).lower()
        if exchange:
            filt["exchange"] = str(exchange).upper()
        # ä»…å¯¹åˆ©ç‡å€ºå¯ç”¨æœªåˆ°æœŸè¿‡æ»¤
        if only_not_matured and (not category or str(category).lower() == "interest"):
            try:
                import datetime as _dt
                today = _dt.datetime.utcnow().strftime("%Y-%m-%d")
            except Exception:
                today = "1970-01-01"
            filt["maturity_date"] = {"$gte": today}

        total = await self.col_basic.count_documents(filt)
        if total == 0:
            return {"total": 0, "items": []}
        skip = max(0, (page - 1) * page_size)
        allowed = {"code", "name", "maturity_date", "list_date", "coupon_rate"}
        field = (sort_by or "code").lower()
        if field not in allowed:
            field = "code"
        direc = 1 if str(sort_dir).lower() != "desc" else -1
        cursor = self.col_basic.find(filt).sort([(field, direc)]).skip(skip).limit(page_size)
        items = []
        async for doc in cursor:
            # ç§»é™¤ _id å­—æ®µï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
            if "_id" in doc:
                doc.pop("_id", None)
            items.append(doc)
        return {"total": total, "items": items}

    async def save_spot_quotes(self, df: pd.DataFrame, category: str) -> int:
        """ä¿å­˜å€ºåˆ¸ç°è´§æŠ¥ä»·æ•°æ®ï¼ˆæ ¹æ®AKShareæ¥å£å­—æ®µç»“æ„æ˜ å°„ï¼‰"""
        import logging
        logger = logging.getLogger("webapi")
        
        if df is None or df.empty:
            return 0
        
        ops = []
        valid_count = 0
        skipped_count = 0
        
        for _, r in df.iterrows():
            try:
                # æå–ä»£ç ï¼ˆæ”¯æŒå¤šç§å­—æ®µåï¼‰
                code = str(r.get("code") or r.get("å€ºåˆ¸ä»£ç ") or r.get("å¯è½¬å€ºä»£ç ") or r.get("ä»£ç ") or r.get("symbol") or "").strip()
                if not code:
                    skipped_count += 1
                    continue
                
                # è§„èŒƒåŒ–ä»£ç 
                from tradingagents.utils.instrument_validator import normalize_bond_code
                norm = normalize_bond_code(code)
                code_std = norm.get("code_std") or code
                
                # æå–æ—¶é—´æˆ³
                timestamp = (
                    r.get("timestamp") or r.get("ticktime") or 
                    r.get("time") or r.get("æ—¥æœŸ") or 
                    r.get("date") or datetime.now().strftime("%H:%M:%S")
                )
                timestamp = str(timestamp).strip()
                
                # å­—æ®µæ˜ å°„è¡¨ï¼šAKShareå­—æ®µ -> æ ‡å‡†å­—æ®µ
                field_mapping = {
                    # ä»·æ ¼ç›¸å…³
                    "æœ€æ–°ä»·": "latest_price",
                    "trade": "latest_price",
                    "price": "latest_price",
                    # æ¶¨è·Œç›¸å…³
                    "æ¶¨è·Œé¢": "change",
                    "pricechange": "change",
                    "æ¶¨è·Œå¹…": "change_percent",
                    "changepercent": "change_percent",
                    # ä¹°å–ä»·
                    "ä¹°å…¥": "buy",
                    "å–å‡º": "sell",
                    # æ˜¨æ”¶å’Œä»Šå¼€
                    "æ˜¨æ”¶": "prev_close",
                    "preclose": "prev_close",
                    "ä»Šå¼€": "open",
                    "open": "open",
                    # æœ€é«˜æœ€ä½
                    "æœ€é«˜": "high",
                    "high": "high",
                    "æœ€ä½": "low",
                    "low": "low",
                    # æˆäº¤ç›¸å…³
                    "æˆäº¤é‡": "volume",
                    "volume": "volume",
                    "æˆäº¤é¢": "amount",
                    "amount": "amount",
                    # åç§°
                    "åç§°": "name",
                    "name": "name",
                }
                
                # æ„å»ºæ ‡å‡†æ–‡æ¡£
                doc = {
                    "code": code_std,
                    "timestamp": timestamp,
                    "category": category,
                    "source": "akshare",
                }
                
                # æ˜ å°„å­—æ®µ
                for ak_field, std_field in field_mapping.items():
                    if ak_field in r.index or ak_field in r:
                        value = r.get(ak_field)
                        if value is not None and not pd.isna(value):
                            doc[std_field] = value
                
                # å¦‚æœnameå­—æ®µè¿˜æœªè®¾ç½®ï¼Œå°è¯•ä»å…¶ä»–å­—æ®µè·å–
                if "name" not in doc:
                    name = str(r.get("åç§°") or r.get("name") or "").strip()
                    if name:
                        doc["name"] = name
                
                # ç§»é™¤Noneå€¼
                doc = {k: v for k, v in doc.items() if v is not None}
                
                # ä½¿ç”¨ (code, timestamp, category) ä½œä¸ºå”¯ä¸€é”®
                ops.append(
                    UpdateOne(
                        {"code": code_std, "timestamp": timestamp, "category": category},
                        {"$set": doc},
                        upsert=True
                    )
                )
                valid_count += 1
                
            except Exception as e:
                skipped_count += 1
                logger.warning(f"âš ï¸ [ç°è´§æŠ¥ä»·ä¿å­˜] å¤„ç†è¡Œæ•°æ®å¤±è´¥: {e}")
                continue
        
        if not ops:
            logger.warning(f"âš ï¸ [ç°è´§æŠ¥ä»·ä¿å­˜] æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜ï¼ˆè·³è¿‡ {skipped_count} æ¡ï¼‰")
            return 0
        
        try:
            res = await self.col_spot.bulk_write(ops, ordered=False)
            upserted = res.upserted_count or 0
            modified = res.modified_count or 0
            total_saved = upserted + modified
            
            logger.info(f"ğŸ’¾ [ç°è´§æŠ¥ä»·ä¿å­˜] æ‰¹é‡å†™å…¥å®Œæˆ: æ–°å¢={upserted}, æ›´æ–°={modified}, æ€»è®¡={total_saved}, æœ‰æ•ˆ={valid_count}, è·³è¿‡={skipped_count}")
            
            return total_saved
        except Exception as e:
            logger.error(f"âŒ [ç°è´§æŠ¥ä»·ä¿å­˜] æ‰¹é‡å†™å…¥å¤±è´¥: {e}", exc_info=True)
            return 0

    async def save_indices(self, df: pd.DataFrame, index_id: str, value_column: str = "value") -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            date = str(r.get("date"))
            val = r.get(value_column)
            doc = {"index_id": index_id, "date": date, value_column: val, "source": "akshare"}
            ops.append(UpdateOne({"index_id": index_id, "date": date}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_indices.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_us_yields(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            doc = {
                "date": str(r.get("date")),
                "tenor": r.get("tenor"),
                "yield": None if pd.isna(r.get("yield")) else float(r.get("yield")),
                "source": "akshare",
            }
            ops.append(UpdateOne({"date": doc["date"], "tenor": doc["tenor"]}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_us_yield.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_cb_profiles(self, profiles: Iterable[Dict[str, Any]]) -> int:
        ops = []
        for p in profiles:
            code = p.get("code")
            if not code:
                continue
            ops.append(UpdateOne({"code": code}, {"$set": p}, upsert=True))
        if ops:
            res = await self.col_cb_profiles.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_buybacks(self, df: pd.DataFrame, exchange: str) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            doc = r.to_dict()
            doc["exchange"] = exchange
            date = str(doc.get("date") or doc.get("æ—¥æœŸ") or doc.get("å…¬å‘Šæ—¥æœŸ") or "")
            code = str(doc.get("code") or doc.get("è¯åˆ¸ä»£ç ") or doc.get("å€ºåˆ¸ä»£ç ") or "").strip()
            if date:
                doc["date"] = date
            if code:
                doc["code"] = code
            ops.append(
                UpdateOne({"exchange": exchange, "date": doc.get("date"), "code": doc.get("code")}, {"$set": doc}, upsert=True)
            )
        if ops:
            res = await self.col_buybacks.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    # ========== é€šç”¨è¾…åŠ© ==========
    @staticmethod
    def _norm_code(row: dict) -> str:
        for k in (
            "code",
            "è¯åˆ¸ä»£ç ",
            "å€ºåˆ¸ä»£ç ",
            "å¯è½¬å€ºä»£ç ",
            "æŸ¥è¯¢ä»£ç ",
            "bondCode",
            "symbol",
            "ä»£ç ",
        ):
            v = row.get(k)
            if v is not None and str(v).strip():
                return str(v).strip()
        return ""

    @staticmethod
    def _norm_date(row: dict) -> str:
        for k in ("date", "æ—¥æœŸ", "æ•°æ®æ—¥æœŸ", "å…¬å‘Šæ—¥æœŸ", "list_date", "ä¸Šå¸‚æ—¥æœŸ"):
            v = row.get(k)
            if v is not None and str(v).strip():
                try:
                    import pandas as pd  # local import
                    return pd.to_datetime(v).strftime("%Y-%m-%d")
                except Exception:
                    return str(v).strip()
        return ""

    # ========== CNINFO å‘è¡Œ ==========
    async def save_cninfo_issues(self, df: pd.DataFrame, issue_type: str, endpoint: str) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            date = self._norm_date(row)
            doc = row
            doc.update({"issue_type": issue_type, "endpoint": endpoint, "code": code, "date": date, "source": "akshare"})
            filt = {"issue_type": issue_type, "endpoint": endpoint, "code": code, "date": date}
            ops.append(UpdateOne(filt, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_issues.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    # ========== å¯è½¬å€ºäº‹ä»¶/ä¼°å€¼ ==========
    async def save_cb_adjustments(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            date = self._norm_date(row)
            filt = {"code": code, "date": date}
            doc = row
            doc.update({"code": code, "date": date, "source": "akshare"})
            ops.append(UpdateOne(filt, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_cb_adjustments.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_cb_redeems(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            date = self._norm_date(row)
            filt = {"code": code, "date": date}
            doc = row
            doc.update({"code": code, "date": date, "source": "akshare"})
            ops.append(UpdateOne(filt, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_cb_redeems.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_cb_summary(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            filt = {"code": code}
            doc = row
            doc.update({"code": code, "source": "akshare"})
            ops.append(UpdateOne(filt, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_cb_summary.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_cb_valuation(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            date = self._norm_date(row)
            filt = {"code": code, "date": date}
            doc = row
            doc.update({"code": code, "date": date, "source": "akshare"})
            ops.append(UpdateOne(filt, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_cb_valuation.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_cb_comparison(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            date = self._norm_date(row)
            code = self._norm_code(row)
            if not code:
                code = str(row.get("å€ºåˆ¸ç®€ç§°") or row.get("åç§°") or "").strip()
            filt = {"date": date, "code": code}
            doc = row
            doc.update({"date": date, "code": code, "source": "akshare"})
            ops.append(UpdateOne(filt, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_cb_comparison.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    # ========== æŠ¥ä»·/æˆäº¤/æ±‡æ€» ==========
    async def save_spot_quote_detail(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            if not code:
                code = str(row.get("å€ºåˆ¸ç®€ç§°") or row.get("åç§°") or row.get("æŠ¥ä»·å“ç§") or "").strip()
            ts = str(row.get("timestamp") or row.get("time") or row.get("æ—¶é—´") or row.get("æ—¥æœŸ") or "")
            doc = row
            dealer = str(row.get("æŠ¥ä»·æœºæ„") or "").strip()
            doc.update({"code": code, "timestamp": ts, "source": "akshare"})
            if dealer:
                ops.append(UpdateOne({"code": code, "timestamp": ts, "æŠ¥ä»·æœºæ„": dealer}, {"$set": doc}, upsert=True))
            else:
                ops.append(UpdateOne({"code": code, "timestamp": ts}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_spot_quote_detail.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_spot_deals(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            if not code:
                code = str(row.get("å€ºåˆ¸ç®€ç§°") or row.get("åç§°") or row.get("æŠ¥ä»·å“ç§") or "").strip()
            ts = str(row.get("timestamp") or row.get("time") or row.get("æ—¶é—´") or row.get("æ—¥æœŸ") or "")
            doc = row
            doc.update({"code": code, "timestamp": ts, "source": "akshare"})
            ops.append(UpdateOne({"code": code, "timestamp": ts}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_spot_deals.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_deal_summary(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            date = self._norm_date(row)
            doc = row
            doc.update({"date": date, "source": "akshare"})
            bt = row.get("å€ºåˆ¸ç±»å‹")
            if bt is not None:
                ops.append(UpdateOne({"date": date, "å€ºåˆ¸ç±»å‹": bt}, {"$set": doc}, upsert=True))
            else:
                ops.append(UpdateOne({"date": date}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_deal_summary.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_cash_summary(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            date = self._norm_date(row)
            doc = row
            doc.update({"date": date, "source": "akshare"})
            ops.append(UpdateOne({"date": date}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_cash_summary.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    # ========== NAFMII / ä¸­å€ºä¿¡æ¯ ==========
    async def save_nafmii(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            date = self._norm_date(row)
            doc = row
            reg_no = str(row.get("æ³¨å†Œé€šçŸ¥ä¹¦æ–‡å·") or row.get("reg_no") or "").strip()
            doc.update({"code": code, "date": date, "source": "akshare"})
            if reg_no:
                doc["reg_no"] = reg_no
                filt = {"reg_no": reg_no}
            else:
                # å›é€€åˆ°åç§°+æ—¥æœŸ+code çš„ç»„åˆ
                filt = {"code": code, "date": date, "å€ºåˆ¸åç§°": row.get("å€ºåˆ¸åç§°")}
            ops.append(UpdateOne(filt, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_nafmii.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_info_cm(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            doc = row
            doc.update({"code": code, "source": "akshare", "endpoint": "bond_info_cm"})
            ops.append(UpdateOne({"code": code, "endpoint": "bond_info_cm"}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_info_cm.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_yield_curve_map(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            date = self._norm_date(row)
            doc = row
            doc.update({"date": date, "source": "akshare"})
            ops.append(UpdateOne({"date": date}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_curve_map.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_buybacks_history(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            date = self._norm_date(row)
            exch = str(row.get("exchange") or row.get("äº¤æ˜“æ‰€") or "").strip()
            doc = row
            doc.update({"date": date, "exchange": exch, "source": "akshare"})
            ops.append(UpdateOne({"exchange": exch, "date": date}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_buybacks_hist.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_cb_list_jsl(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            doc = row
            doc.update({"code": code, "source": "akshare"})
            ops.append(UpdateOne({"code": code}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_cb_list_jsl.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_cov_list(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            doc = row
            doc.update({"code": code, "source": "akshare"})
            ops.append(UpdateOne({"code": code}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_cov_list.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_info_cm_query(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            doc = row
            doc.update({"code": code, "source": "akshare", "endpoint": "bond_info_cm_query"})
            ops.append(UpdateOne({"code": code, "endpoint": "bond_info_cm_query"}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_info_cm.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0)
        return 0

    async def save_info_cm_detail(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        # è‹¥ä¸º name/value å½¢å¼ï¼Œåˆå¹¶ä¸ºå•æ–‡æ¡£
        try:
            cols = [c.lower() for c in df.columns]
        except Exception:
            cols = []
        if ("name" in cols) and ("value" in cols):
            mapping = {}
            for _, r in df.iterrows():
                name_key = r.get("name") if "name" in df.columns else r.get(df.columns[0])
                value_val = r.get("value") if "value" in df.columns else r.get(df.columns[1])
                if name_key is not None and str(name_key).strip():
                    mapping[str(name_key).strip()] = value_val
            # æå–codeï¼ˆä¼˜å…ˆä½¿ç”¨æ˜ç¡®å®šä¹‰çš„ç¼–ç å­—æ®µï¼‰
            code = (
                str(mapping.get("bondCode") or mapping.get("å€ºåˆ¸ä»£ç ") or mapping.get("bondDefinedCode") or mapping.get("æŸ¥è¯¢ä»£ç ") or "").strip()
            )
            if not code:
                # å›é€€ï¼šå°è¯•ä»æ˜ å°„ä¸­æ‰¾å¸¸è§é”®
                for k in ("code", "è¯åˆ¸ä»£ç ", "å¯è½¬å€ºä»£ç "):
                    if mapping.get(k):
                        code = str(mapping.get(k)).strip()
                        break
            doc = mapping
            doc.update({"code": code, "source": "akshare", "endpoint": "bond_info_detail_cm"})
            res = await self.col_info_cm.update_one(
                {"code": code, "endpoint": "bond_info_detail_cm"},
                {"$set": doc},
                upsert=True,
            )
            return 1 if (res.upserted_id or res.modified_count) else 0
        else:
            # å›é€€ï¼šé€è¡Œå†™å…¥ï¼ˆåŒä¸€ä¸ªcodeä¼šè¢«åç»­è¡Œè¦†ç›–ï¼Œä»ä¿æŒå•æ–‡æ¡£ï¼‰
            ops = []
            for _, r in df.iterrows():
                row = r.to_dict()
                code = self._norm_code(row)
                if not code:
                    code = str(row.get("bondCode") or row.get("å€ºåˆ¸ä»£ç ") or row.get("bondDefinedCode") or row.get("æŸ¥è¯¢ä»£ç ") or "").strip()
                doc = row
                doc.update({"code": code, "source": "akshare", "endpoint": "bond_info_detail_cm"})
                ops.append(UpdateOne({"code": code, "endpoint": "bond_info_detail_cm"}, {"$set": doc}, upsert=True))
            if ops:
                res = await self.col_info_cm.bulk_write(ops, ordered=False)
                return (res.upserted_count or 0) + (res.modified_count or 0)
            return 0

    async def query_bond_daily(self, code: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """æŸ¥è¯¢å€ºåˆ¸å†å²æ•°æ®"""
        from tradingagents.utils.instrument_validator import normalize_bond_code
        norm = normalize_bond_code(code)
        code_std = norm.get("code_std") or code
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        query = {
            "code": code_std,
            "date": {"$gte": start_date, "$lte": end_date}
        }
        
        # æŸ¥è¯¢æ•°æ®
        cursor = self.col_daily.find(query).sort("date", 1)
        docs = [doc async for doc in cursor]
        
        if not docs:
            return None
        
        # è½¬æ¢ä¸ºDataFrame
        for doc in docs:
            doc.pop("_id", None)
        
        df = pd.DataFrame(docs)
        return df if not df.empty else None

    async def query_yield_curve(self, start_date: Optional[str] = None, end_date: Optional[str] = None, curve_name: Optional[str] = None) -> Optional[pd.DataFrame]:
        """æŸ¥è¯¢æ”¶ç›Šç‡æ›²çº¿æ•°æ®
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            curve_name: æ›²çº¿åç§°ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚"ä¸­å€ºå›½å€ºæ”¶ç›Šç‡æ›²çº¿"
        
        Returns:
            DataFrame: åŒ…å« date, curve_name, tenor, yield åˆ—çš„ DataFrame
        """
        query = {}
        if start_date:
            query["date"] = {"$gte": start_date}
        if end_date:
            if "date" in query:
                query["date"]["$lte"] = end_date
            else:
                query["date"] = {"$lte": end_date}
        if curve_name:
            query["curve_name"] = str(curve_name).strip()
        
        cursor = self.col_curve.find(query).sort("date", 1).sort("tenor", 1)
        docs = [doc async for doc in cursor]
        
        if not docs:
            return None
        
        # è½¬æ¢ä¸ºDataFrame
        for doc in docs:
            doc.pop("_id", None)
            # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„å­—æ®µéƒ½å­˜åœ¨
            if "curve_name" not in doc:
                doc["curve_name"] = ""
        
        df = pd.DataFrame(docs)
        return df if not df.empty else None

    async def save_bond_info_from_api(self, code: str, info_dict: Dict[str, Any]) -> int:
        """å°†ä»æ¥å£è·å–çš„å€ºåˆ¸è¯¦æƒ…ä¿¡æ¯ä¿å­˜åˆ°æ•°æ®åº“"""
        from tradingagents.utils.instrument_validator import normalize_bond_code
        norm = normalize_bond_code(code)
        code_std = norm.get("code_std") or code
        
        # æ›´æ–°åŸºç¡€ä¿¡æ¯è¡¨
        basic_doc = {
            "code": code_std,
            "name": info_dict.get("name"),
            "exchange": info_dict.get("exchange"),
            "category": info_dict.get("category"),
            "issuer": info_dict.get("issuer"),
            "list_date": info_dict.get("list_date"),
            "maturity_date": info_dict.get("maturity_date"),
            "coupon_rate": info_dict.get("coupon_rate"),
            "source": info_dict.get("source", "akshare"),
        }
        
        # ç§»é™¤Noneå€¼
        basic_doc = {k: v for k, v in basic_doc.items() if v is not None}
        
        await self.col_basic.update_one(
            {"code": code_std},
            {"$set": basic_doc},
            upsert=True
        )
        
        # å¦‚æœæœ‰è¯¦ç»†ä¿¡æ¯ï¼Œä¿å­˜åˆ°è¯¦ç»†ä¿¡æ¯è¡¨
        # å°†info_dictè½¬æ¢ä¸ºDataFrameæ ¼å¼ä»¥ä¾¿ä¿å­˜
        if info_dict and len(info_dict) > 1:  # ä¸æ­¢codeå­—æ®µ
            detail_df = pd.DataFrame([info_dict])
            saved = await self.save_info_cm_detail(detail_df)
            return saved
        
        return 1

    async def query_bond_info(self, code: str) -> Optional[Dict[str, Any]]:
        """æŸ¥è¯¢å€ºåˆ¸è¯¦æƒ…ä¿¡æ¯ï¼Œåˆå¹¶åŸºç¡€ä¿¡æ¯å’Œè¯¦ç»†ä¿¡æ¯"""
        # æ ‡å‡†åŒ–ä»£ç 
        from tradingagents.utils.instrument_validator import normalize_bond_code
        norm = normalize_bond_code(code)
        code_std = norm.get("code_std") or code
        
        # å…ˆæŸ¥è¯¢åŸºç¡€ä¿¡æ¯
        basic_info = await self.col_basic.find_one({"code": code_std})
        
        # æŸ¥è¯¢è¯¦ç»†ä¿¡æ¯ï¼ˆbond_info_detail_cmï¼‰- æ”¯æŒå¤šç§ä»£ç æ ¼å¼æŸ¥è¯¢
        detail_info = await self.col_info_cm.find_one({
            "code": code_std,
            "endpoint": "bond_info_detail_cm"
        })
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ç”¨åŸå§‹ä»£ç æŸ¥è¯¢
        if not detail_info:
            detail_info = await self.col_info_cm.find_one({
                "$or": [
                    {"code": code},
                    {"code": norm.get("digits")},
                    {"å€ºåˆ¸ä»£ç ": code},
                    {"å€ºåˆ¸ä»£ç ": code_std},
                    {"å€ºåˆ¸ä»£ç ": norm.get("digits")},
                ],
                "endpoint": "bond_info_detail_cm"
            })
        
        # åˆå¹¶ä¿¡æ¯
        result = {}
        
        # å¦‚æœæœ‰åŸºç¡€ä¿¡æ¯ï¼Œåˆå¹¶åˆ°ç»“æœä¸­
        if basic_info:
            basic_info.pop("_id", None)
            result.update(basic_info)
        
        # å¦‚æœæœ‰è¯¦ç»†ä¿¡æ¯ï¼Œåˆå¹¶åˆ°ç»“æœä¸­ï¼ˆè¯¦ç»†ä¿¡æ¯å­—æ®µä¼˜å…ˆï¼‰
        if detail_info:
            detail_info.pop("_id", None)
            detail_info.pop("endpoint", None)  # ç§»é™¤å†…éƒ¨å­—æ®µ
            detail_info.pop("source", None)  # å…ˆç§»é™¤ï¼Œåé¢ç»Ÿä¸€è®¾ç½®
            
            # å®šä¹‰å­—æ®µæ˜ å°„è¡¨ï¼šè¯¦ç»†ä¿¡æ¯å­—æ®µ -> æ ‡å‡†å­—æ®µ
            field_mapping = {
                # åç§°ç›¸å…³
                "å€ºåˆ¸åç§°": "name",
                "åç§°": "name",
                "å€ºåˆ¸å…¨ç§°": "name",
                # å‘è¡Œäººç›¸å…³
                "å‘è¡Œäºº": "issuer",
                "å‘è¡Œä¸»ä½“": "issuer",
                "å‘è¡Œäººå…¨ç§°": "issuer",
                # æ¯ç¥¨ç‡ç›¸å…³
                "ç¥¨é¢åˆ©ç‡": "coupon_rate",
                "æ¯ç¥¨ç‡": "coupon_rate",
                "åˆ©ç‡": "coupon_rate",
                "ç¥¨æ¯": "coupon_rate",
                # ä¸Šå¸‚æ—¥æœŸç›¸å…³
                "ä¸Šå¸‚æ—¥æœŸ": "list_date",
                "ä¸Šå¸‚æ—¥": "list_date",
                "ä¸Šå¸‚æ—¶é—´": "list_date",
                # åˆ°æœŸæ—¥ç›¸å…³
                "åˆ°æœŸæ—¥": "maturity_date",
                "åˆ°æœŸæ—¥æœŸ": "maturity_date",
                "å€ºåˆ¸åˆ°æœŸæ—¥": "maturity_date",
                "åˆ°æœŸæ—¶é—´": "maturity_date",
                # äº¤æ˜“æ‰€ç›¸å…³
                "äº¤æ˜“æ‰€": "exchange",
                "ä¸Šå¸‚äº¤æ˜“æ‰€": "exchange",
                "äº¤æ˜“å¸‚åœº": "exchange",
            }
            
            # å°†è¯¦ç»†ä¿¡æ¯ä¸­çš„å­—æ®µåˆå¹¶
            for key, value in detail_info.items():
                if value is not None and value != "" and str(value).strip() != "nan":
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ˜ å°„å­—æ®µå
                    mapped_key = field_mapping.get(key, key)
                    
                    # å¦‚æœç›®æ ‡å­—æ®µå·²ç»æœ‰å€¼ï¼Œè·³è¿‡
                    if mapped_key in result and result[mapped_key]:
                        continue
                    
                    # æ ¹æ®å­—æ®µç±»å‹è¿›è¡Œè½¬æ¢
                    if mapped_key == "name":
                        result["name"] = str(value).strip()
                    elif mapped_key == "issuer":
                        result["issuer"] = str(value).strip()
                    elif mapped_key == "coupon_rate":
                        try:
                            if isinstance(value, str):
                                value_str = value.strip().strip('%')
                                result["coupon_rate"] = float(value_str)
                            else:
                                result["coupon_rate"] = float(value)
                        except (ValueError, TypeError):
                            pass
                    elif mapped_key == "list_date":
                        try:
                            if hasattr(value, 'strftime'):  # pandas Timestamp æˆ– datetime
                                result["list_date"] = value.strftime("%Y-%m-%d")
                            elif isinstance(value, str) and len(value) >= 10:
                                # å°è¯•æ ¼å¼åŒ–æ—¥æœŸå­—ç¬¦ä¸²
                                result["list_date"] = value[:10].replace("/", "-")
                            else:
                                result["list_date"] = str(value)
                        except Exception:
                            result["list_date"] = str(value) if value else None
                    elif mapped_key == "maturity_date":
                        try:
                            if hasattr(value, 'strftime'):  # pandas Timestamp æˆ– datetime
                                result["maturity_date"] = value.strftime("%Y-%m-%d")
                            elif isinstance(value, str) and len(value) >= 10:
                                # å°è¯•æ ¼å¼åŒ–æ—¥æœŸå­—ç¬¦ä¸²
                                result["maturity_date"] = value[:10].replace("/", "-")
                            else:
                                result["maturity_date"] = str(value)
                        except Exception:
                            result["maturity_date"] = str(value) if value else None
                    elif mapped_key == "exchange":
                        # æ ‡å‡†åŒ–äº¤æ˜“æ‰€ä»£ç 
                        exchange_val = str(value).strip().upper()
                        if "ä¸Šæµ·" in exchange_val or "ä¸Šäº¤æ‰€" in exchange_val or exchange_val in ["SH", "SHANGHAI"]:
                            result["exchange"] = "SH"
                        elif "æ·±åœ³" in exchange_val or "æ·±äº¤æ‰€" in exchange_val or exchange_val in ["SZ", "SHENZHEN"]:
                            result["exchange"] = "SZ"
                        else:
                            result["exchange"] = exchange_val
                    else:
                        # å…¶ä»–å­—æ®µç›´æ¥æ·»åŠ ï¼Œä½†ä¸è¦†ç›–å·²æœ‰å­—æ®µ
                        if mapped_key not in result:
                            result[mapped_key] = value
        
        # ç¡®ä¿codeå­—æ®µå­˜åœ¨
        if "code" not in result:
            result["code"] = code_std
        
        # ç¡®ä¿sourceå­—æ®µå­˜åœ¨
        if "source" not in result:
            result["source"] = "database"
        
        # å¦‚æœæœ‰ä»»ä½•å­—æ®µï¼Œè¿”å›ç»“æœ
        return result if result else None

    async def save_bond_minute_quotes(self, code: str, df: pd.DataFrame, period: str = "1") -> int:
        """ä¿å­˜å€ºåˆ¸åˆ†é’Ÿæ•°æ®
        
        Args:
            code: å€ºåˆ¸ä»£ç ï¼ˆæ ‡å‡†åŒ–åï¼‰
            df: åŒ…å«åˆ†é’Ÿæ•°æ®çš„DataFrameï¼Œå¿…é¡»åŒ…å«datetimeæˆ–æ—¶é—´åˆ—
            period: æ•°æ®å‘¨æœŸï¼ˆ"1", "5", "15", "30", "60"ï¼‰
        
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        import logging
        logger = logging.getLogger("webapi")
        
        if df is None or df.empty:
            return 0
        
        ops = []
        valid_count = 0
        skipped_count = 0
        
        # è§„èŒƒåŒ–ä»£ç 
        from tradingagents.utils.instrument_validator import normalize_bond_code
        norm = normalize_bond_code(code)
        code_std = norm.get("code_std") or code
        
        # å­—æ®µæ˜ å°„è¡¨ï¼šAKShareå­—æ®µ -> æ ‡å‡†å­—æ®µ
        field_mapping = {
            # æ—¶é—´ç›¸å…³
            "æ—¶é—´": "datetime",
            "date": "datetime",
            "datetime": "datetime",
            # ä»·æ ¼ç›¸å…³
            "å¼€ç›˜": "open",
            "open": "open",
            "æœ€é«˜": "high",
            "high": "high",
            "æœ€ä½": "low",
            "low": "low",
            "æ”¶ç›˜": "close",
            "close": "close",
            "æœ€æ–°ä»·": "close",
            # æˆäº¤ç›¸å…³
            "æˆäº¤é‡": "volume",
            "volume": "volume",
            "æˆäº¤é¢": "amount",
            "amount": "amount",
            # å…¶ä»–
            "æ¶¨è·Œå¹…": "change_percent",
            "æ¶¨è·Œé¢": "change",
            "æŒ¯å¹…": "amplitude",
            "æ¢æ‰‹ç‡": "turnover_rate",
        }
        
        for _, r in df.iterrows():
            try:
                # æå–datetime
                datetime_val = None
                for dt_col in ["æ—¶é—´", "datetime", "date"]:
                    if dt_col in r.index or dt_col in r:
                        dt_val = r.get(dt_col)
                        if dt_val is not None and not pd.isna(dt_val):
                            try:
                                if isinstance(dt_val, pd.Timestamp):
                                    datetime_val = dt_val.strftime("%Y-%m-%d %H:%M:%S")
                                elif isinstance(dt_val, str):
                                    # å°è¯•è§£ææ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²
                                    datetime_val = pd.to_datetime(dt_val).strftime("%Y-%m-%d %H:%M:%S")
                                else:
                                    datetime_val = str(dt_val)
                                break
                            except Exception:
                                continue
                
                if not datetime_val:
                    skipped_count += 1
                    continue
                
                # æ„å»ºæ–‡æ¡£
                doc = {
                    "code": code_std,
                    "datetime": datetime_val,
                    "period": str(period),
                    "source": "akshare",
                }
                
                # æ˜ å°„å­—æ®µ
                for ak_field, std_field in field_mapping.items():
                    if ak_field in r.index or ak_field in r:
                        value = r.get(ak_field)
                        if value is not None and not pd.isna(value):
                            try:
                                # æ•°å€¼ç±»å‹è½¬æ¢
                                if std_field in ["open", "high", "low", "close", "volume", "amount", 
                                                "change_percent", "change", "amplitude", "turnover_rate"]:
                                    doc[std_field] = float(value)
                                else:
                                    doc[std_field] = value
                            except (ValueError, TypeError):
                                pass
                
                # ç§»é™¤Noneå€¼
                doc = {k: v for k, v in doc.items() if v is not None}
                
                # ä½¿ç”¨ (code, datetime, period) ä½œä¸ºå”¯ä¸€é”®
                ops.append(
                    UpdateOne(
                        {"code": code_std, "datetime": datetime_val, "period": period},
                        {"$set": doc},
                        upsert=True
                    )
                )
                valid_count += 1
                
            except Exception as e:
                skipped_count += 1
                logger.warning(f"âš ï¸ [åˆ†é’Ÿæ•°æ®ä¿å­˜] å¤„ç†è¡Œæ•°æ®å¤±è´¥: {e}")
                continue
        
        if not ops:
            logger.warning(f"âš ï¸ [åˆ†é’Ÿæ•°æ®ä¿å­˜] æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜ï¼ˆè·³è¿‡ {skipped_count} æ¡ï¼‰")
            return 0
        
        try:
            res = await self.col_minute.bulk_write(ops, ordered=False)
            upserted = res.upserted_count or 0
            modified = res.modified_count or 0
            total_saved = upserted + modified
            
            logger.info(f"ğŸ’¾ [åˆ†é’Ÿæ•°æ®ä¿å­˜] æ‰¹é‡å†™å…¥å®Œæˆ: æ–°å¢={upserted}, æ›´æ–°={modified}, æ€»è®¡={total_saved}, æœ‰æ•ˆ={valid_count}, è·³è¿‡={skipped_count}")
            
            return total_saved
        except Exception as e:
            logger.error(f"âŒ [åˆ†é’Ÿæ•°æ®ä¿å­˜] æ‰¹é‡å†™å…¥å¤±è´¥: {e}", exc_info=True)
            return 0

from typing import Optional, Iterable, Dict, Any
from datetime import datetime
import datetime as dt
import pandas as pd
from motor.motor_asyncio import AsyncIOMotorDatabase
from pymongo import UpdateOne
from loguru import logger


class BondDataService:
    def __init__(self, db: AsyncIOMotorDatabase) -> None:
        self.db = db
        self.col_basic = db.get_collection("bond_basic_info")
        self.col_daily = db.get_collection("bond_daily")
        self.col_curve = db.get_collection("yield_curve_daily")
        self.col_events = db.get_collection("bond_events")
        self.col_spot = db.get_collection("bond_spot_quotes")
        self.col_indices = db.get_collection("bond_indices_daily")
        self.col_us_yield = db.get_collection("us_yield_daily")
        self.col_cb_profiles = db.get_collection("bond_cb_profiles")
        self.col_buybacks = db.get_collection("bond_buybacks")
        # æœªè¦†ç›–ç«¯ç‚¹çš„é›†åˆ
        self.col_issues = db.get_collection("bond_issues")
        self.col_cb_adjustments = db.get_collection("bond_cb_adjustments")
        self.col_cb_redeems = db.get_collection("bond_cb_redeems")
        self.col_cb_summary = db.get_collection("bond_cb_summary")
        self.col_cb_valuation = db.get_collection("bond_cb_valuation_daily")
        self.col_cb_comparison = db.get_collection("bond_cb_comparison")
        self.col_spot_quote_detail = db.get_collection("bond_spot_quote_detail")
        self.col_spot_deals = db.get_collection("bond_spot_deals")
        self.col_deal_summary = db.get_collection("bond_deal_summary")
        self.col_cash_summary = db.get_collection("bond_cash_summary")
        self.col_nafmii = db.get_collection("bond_nafmii_debts")
        self.col_info_cm = db.get_collection("bond_info_cm")
        self.col_curve_map = db.get_collection("yield_curve_map")
        self.col_buybacks_hist = db.get_collection("bond_buybacks_hist")
        self.col_cb_list_jsl = db.get_collection("bond_cb_list_jsl")
        self.col_cov_list = db.get_collection("bond_cov_list")
        self.col_minute = db.get_collection("bond_minute_quotes")

    async def _safe_create_index(self, collection, index_spec, unique=False, sparse=False, name=None):
        """å®‰å…¨åˆ›å»ºç´¢å¼•ï¼Œå¦‚æœç´¢å¼•å·²å­˜åœ¨åˆ™è·³è¿‡"""
        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šåç§°ï¼Œå°è¯•ä»ç´¢å¼•è§„èŒƒç”Ÿæˆ
            if name is None:
                if isinstance(index_spec, list):
                    # å¤åˆç´¢å¼•
                    name_parts = [f"{field}_{direction}" for field, direction in index_spec]
                    name = "_".join(name_parts)
                else:
                    # å•å­—æ®µç´¢å¼•
                    name = f"{index_spec}_1"
            
            # æ ‡å‡†åŒ–ç´¢å¼•é”®æ ¼å¼ç”¨äºæ¯”è¾ƒ
            if isinstance(index_spec, str):
                target_key = [(index_spec, 1)]
            elif isinstance(index_spec, list):
                target_key = index_spec
            else:
                target_key = list(index_spec)
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
            indexes = await collection.list_indexes().to_list(length=None)
            
            # æŸ¥æ‰¾ç›¸åŒåç§°çš„ç´¢å¼•
            existing_by_name = next((idx for idx in indexes if idx.get('name') == name), None)
            
            # æŸ¥æ‰¾ç›¸åŒé”®çš„ç´¢å¼•ï¼ˆä¸è®ºåç§°ï¼‰
            existing_by_key = None
            for idx in indexes:
                if idx.get('name') == '_id_':  # è·³è¿‡é»˜è®¤_idç´¢å¼•
                    continue
                idx_key = list(idx.get('key', {}).items())
                if idx_key == target_key:
                    existing_by_key = idx
                    break
            
            if existing_by_name:
                # æœ‰ç›¸åŒåç§°çš„ç´¢å¼•ï¼Œæ£€æŸ¥è§„æ ¼æ˜¯å¦ä¸€è‡´
                existing_unique = existing_by_name.get('unique', False)
                existing_sparse = existing_by_name.get('sparse', False)
                
                if existing_unique == unique and existing_sparse == sparse:
                    # ç´¢å¼•å·²å­˜åœ¨ä¸”è§„æ ¼ä¸€è‡´ï¼Œæ— éœ€é‡å»º
                    logger.debug(f"âœ“ ç´¢å¼• {name} å·²å­˜åœ¨ä¸”è§„æ ¼ä¸€è‡´")
                    return
                else:
                    # ç´¢å¼•è§„æ ¼ä¸ä¸€è‡´ï¼Œéœ€è¦åˆ é™¤é‡å»º
                    logger.info(f"ğŸ”„ ç´¢å¼• {name} è§„æ ¼ä¸ä¸€è‡´ï¼Œåˆ é™¤æ—§ç´¢å¼•å¹¶é‡å»º")
                    try:
                        await collection.drop_index(name)
                    except Exception as drop_err:
                        logger.warning(f"âš ï¸ åˆ é™¤ç´¢å¼• {name} å¤±è´¥: {drop_err}")
            
            elif existing_by_key:
                # ç›¸åŒé”®ä½†ä¸åŒåç§°çš„ç´¢å¼•å·²å­˜åœ¨
                old_name = existing_by_key.get('name')
                existing_unique = existing_by_key.get('unique', False)
                existing_sparse = existing_by_key.get('sparse', False)
                
                if existing_unique == unique and existing_sparse == sparse:
                    # é”®å’Œè§„æ ¼éƒ½ä¸€è‡´ï¼Œåªæ˜¯åç§°ä¸åŒï¼Œä¿ç•™æ—§ç´¢å¼•å³å¯
                    logger.debug(f"âœ“ ç´¢å¼•é”®å·²å­˜åœ¨ (æ—§åç§°: {old_name})ï¼Œè§„æ ¼ä¸€è‡´ï¼Œè·³è¿‡åˆ›å»º")
                    return
                else:
                    # éœ€è¦æ›¿æ¢æ—§ç´¢å¼•
                    logger.info(f"ğŸ”„ ç´¢å¼•é”®å·²å­˜åœ¨ä½†è§„æ ¼ä¸åŒ (æ—§åç§°: {old_name})ï¼Œåˆ é™¤å¹¶é‡å»º")
                    try:
                        await collection.drop_index(old_name)
                    except Exception as drop_err:
                        logger.warning(f"âš ï¸ åˆ é™¤æ—§ç´¢å¼• {old_name} å¤±è´¥: {drop_err}")
            
            # åˆ›å»ºç´¢å¼•
            if isinstance(index_spec, str):
                await collection.create_index(index_spec, unique=unique, sparse=sparse, name=name)
            else:
                await collection.create_index(index_spec, unique=unique, sparse=sparse, name=name)
            
            logger.debug(f"âœ“ æˆåŠŸåˆ›å»ºç´¢å¼• {name}")
            
        except Exception as e:
            error_msg = str(e)
            # å¤„ç†å„ç§ç´¢å¼•é”™è¯¯
            if 'IndexOptionsConflict' in error_msg or 'Index already exists with a different name' in error_msg:
                logger.debug(f"âš ï¸ ç´¢å¼•å·²å­˜åœ¨ï¼ˆä¸åŒåç§°ï¼‰ï¼Œè·³è¿‡: {error_msg}")
                # ç´¢å¼•å®é™…ä¸Šå·²ç»å­˜åœ¨ï¼Œåªæ˜¯åç§°ä¸åŒï¼Œå¯ä»¥å¿½ç•¥
                return
            elif 'IndexKeySpecsConflict' in error_msg:
                logger.warning(f"âš ï¸ ç´¢å¼•é”®å†²çª: {error_msg}")
            else:
                logger.warning(f"âš ï¸ åˆ›å»ºç´¢å¼• {name} å¤±è´¥: {e}")

    async def ensure_indexes(self) -> None:
        await self._safe_create_index(
            self.col_basic,
            "code",
            unique=True,
            name="basic_code_1"
        )
        await self.col_basic.create_index("category")
        await self.col_basic.create_index("maturity_date")
        await self.col_basic.create_index("exchange")
        await self.col_basic.create_index("list_date")
        await self.col_basic.create_index("coupon_rate")
        await self.col_basic.create_index("name")
        await self._safe_create_index(
            self.col_daily,
            [("code", 1), ("date", 1)],
            unique=True,
            name="daily_code_1_date_1"
        )
        # æ”¶ç›Šç‡æ›²çº¿ç´¢å¼•ï¼šä½¿ç”¨ (date, tenor, curve_name, yield_type?) ä½œä¸ºå”¯ä¸€é”®
        # curve_name ä¸ºç©ºå­—ç¬¦ä¸²è¡¨ç¤ºæœªåˆ†ç±»çš„æ›²çº¿
        # yield_type å¯é€‰ï¼Œç”¨äºåŒºåˆ†åˆ°æœŸæ”¶ç›Šç‡ã€å³æœŸæ”¶ç›Šç‡ç­‰
        await self._safe_create_index(
            self.col_curve,
            [("date", 1), ("tenor", 1), ("curve_name", 1), ("yield_type", 1)],
            unique=True,
            sparse=True,
            name="date_1_tenor_1_curve_name_1_yield_type_1"
        )
        await self._safe_create_index(
            self.col_curve,
            [("date", 1), ("tenor", 1), ("curve_name", 1)],
            unique=True,
            name="date_1_tenor_1_curve_name_1"
        )
        await self.col_curve.create_index("date")
        await self.col_curve.create_index("curve_name")
        # æ³¨æ„ï¼šè¿™ä¸ªéå”¯ä¸€ç´¢å¼•å¯èƒ½ä¸ä¹‹å‰çš„å”¯ä¸€ç´¢å¼•å†²çªï¼Œä½¿ç”¨å®‰å…¨åˆ›å»ºæ–¹æ³•
        await self._safe_create_index(
            self.col_curve,
            [("date", 1), ("tenor", 1)],
            unique=False,
            name="date_1_tenor_1_query"
        )
        await self._safe_create_index(
            self.col_events,
            [("code", 1), ("date", 1), ("event_type", 1)],
            unique=True,
            name="code_1_date_1_event_type_1"
        )
        await self._safe_create_index(
            self.col_spot,
            [("code", 1), ("timestamp", 1), ("category", 1)],
            unique=True,
            name="code_1_timestamp_1_category_1"
        )
        await self._safe_create_index(
            self.col_indices,
            [("index_id", 1), ("date", 1)],
            unique=True,
            name="index_id_1_date_1"
        )
        # us_yield é›†åˆçš„ç´¢å¼•å¯èƒ½ä¸å…¶ä»–é›†åˆå†²çªï¼Œä½¿ç”¨å®‰å…¨åˆ›å»º
        await self._safe_create_index(
            self.col_us_yield,
            [("date", 1), ("tenor", 1)],
            unique=True,
            name="date_1_tenor_1"
        )
        await self._safe_create_index(
            self.col_cb_profiles,
            "code",
            unique=True,
            name="code_1"
        )
        await self._safe_create_index(
            self.col_buybacks,
            [("exchange", 1), ("date", 1), ("code", 1)],
            unique=True,
            name="exchange_1_date_1_code_1"
        )
        # æ–°å¢ç´¢å¼•
        await self._safe_create_index(
            self.col_issues,
            [("issue_type", 1), ("code", 1), ("date", 1)],
            unique=True,
            name="issue_type_1_code_1_date_1"
        )
        await self._safe_create_index(
            self.col_cb_adjustments,
            [("code", 1), ("date", 1)],
            unique=True,
            name="cb_adj_code_1_date_1"
        )
        await self._safe_create_index(
            self.col_cb_redeems,
            [("code", 1), ("date", 1)],
            unique=True,
            name="cb_redeem_code_1_date_1"
        )
        await self._safe_create_index(
            self.col_cb_summary,
            "code",
            unique=True,
            name="cb_summary_code_1"
        )
        await self._safe_create_index(
            self.col_cb_valuation,
            [("code", 1), ("date", 1)],
            unique=True,
            name="cb_val_code_1_date_1"
        )
        await self._safe_create_index(
            self.col_cb_comparison,
            [("date", 1), ("code", 1)],
            unique=True,
            name="cb_comp_date_1_code_1"
        )
        await self._safe_create_index(
            self.col_spot_quote_detail,
            [("code", 1), ("timestamp", 1), ("æŠ¥ä»·æœºæ„", 1)],
            unique=True,
            name="spot_quote_detail_unique"
        )
        await self._safe_create_index(
            self.col_spot_deals,
            [("code", 1), ("timestamp", 1)],
            unique=True,
            name="spot_deals_code_1_ts_1"
        )
        await self._safe_create_index(
            self.col_deal_summary,
            [("date", 1), ("å€ºåˆ¸ç±»å‹", 1)],
            unique=True,
            name="deal_summary_date_1_type_1"
        )
        await self._safe_create_index(
            self.col_cash_summary,
            [("date", 1), ("å€ºåˆ¸ç°è´§", 1)],
            unique=True,
            name="cash_summary_date_1_type_1"
        )
        # NAFMII ä½¿ç”¨æ³¨å†Œé€šçŸ¥ä¹¦æ–‡å·ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼ˆéƒ¨åˆ†è®°å½•æ—  reg_no æ—¶ä»å¯å†™å…¥ï¼Œä½†å»ºè®®æ•°æ®æºå°½é‡æä¾›ï¼‰
        await self._safe_create_index(self.col_nafmii, "reg_no", unique=True, sparse=True, name="reg_no_1")
        await self.col_info_cm.create_index("code")
        # å”¯ä¸€ç´¢å¼•ï¼šæ¯ä¸ªå€ºåˆ¸çš„æ¯ä¸ªæ¥å£åªæœ‰ä¸€æ¡è®°å½•
        await self._safe_create_index(
            self.col_info_cm,
            [("code", 1), ("endpoint", 1)],
            unique=True,
            name="info_cm_code_1_endpoint_1"
        )
        await self._safe_create_index(
            self.col_curve_map,
            "date",
            unique=True,
            name="curve_map_date_1"
        )
        await self._safe_create_index(
            self.col_buybacks_hist,
            [("exchange", 1), ("date", 1)],
            unique=True,
            name="buybacks_hist_exch_1_date_1"
        )
        await self._safe_create_index(
            self.col_cb_list_jsl,
            "code",
            unique=True,
            name="cb_list_jsl_code_1"
        )
        await self._safe_create_index(
            self.col_cov_list,
            "code",
            unique=True,
            name="cov_list_code_1"
        )
        # åˆ†é’Ÿæ•°æ®ç´¢å¼•ï¼šä½¿ç”¨ (code, datetime, period) ä½œä¸ºå”¯ä¸€é”®
        await self._safe_create_index(
            self.col_minute,
            [("code", 1), ("datetime", 1), ("period", 1)],
            unique=True,
            name="minute_code_1_dt_1_period_1"
        )
        await self.col_minute.create_index("code")
        await self.col_minute.create_index("datetime")
        # åˆ†é’Ÿæ•°æ®çš„æŸ¥è¯¢ç´¢å¼•ï¼Œå¯èƒ½ä¸å”¯ä¸€ç´¢å¼•å†²çªï¼Œä½¿ç”¨å®‰å…¨åˆ›å»º
        await self._safe_create_index(
            self.col_minute,
            [("code", 1), ("datetime", 1)],
            unique=False,
            name="code_1_datetime_1_query"
        )

    async def save_yield_curve(self, df: pd.DataFrame) -> int:
        """ä¿å­˜æ”¶ç›Šç‡æ›²çº¿æ•°æ®åˆ°æ•°æ®åº“ï¼ˆè¿‡æ»¤éæ•°å€¼æ•°æ®ï¼‰"""
        import logging
        import numpy as np
        logger = logging.getLogger("webapi")
        
        if df is None or df.empty:
            logger.warning("[æ”¶ç›Šç‡æ›²çº¿ä¿å­˜] DataFrameä¸ºç©º")
            return 0
        
        ops = []
        valid_count = 0
        skipped_count = 0
        
        for _, r in df.iterrows():
            try:
                date_val = r.get("date")
                tenor_val = r.get("tenor")
                yield_val = r.get("yield")
                
                # éªŒè¯æ—¥æœŸ
                if pd.isna(date_val) or not date_val:
                    skipped_count += 1
                    continue
                
                # éªŒè¯æœŸé™ï¼ˆtenorï¼‰
                if pd.isna(tenor_val) or not tenor_val:
                    skipped_count += 1
                    continue
                
                # éªŒè¯æ”¶ç›Šç‡å€¼ï¼ˆyieldï¼‰- å¿…é¡»æ˜¯æ•°å€¼
                if pd.isna(yield_val):
                    skipped_count += 1
                    continue
                
                # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                try:
                    # å¦‚æœå·²ç»æ˜¯æ•°å€¼ç±»å‹ï¼Œç›´æ¥è½¬æ¢
                    if isinstance(yield_val, (int, float)):
                        yield_float = float(yield_val)
                    elif isinstance(yield_val, str):
                        # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                        # å¦‚æœåŒ…å«éæ•°å­—å­—ç¬¦ï¼ˆæ¯”å¦‚ä¸­æ–‡ï¼‰ï¼Œä¼šæŠ›å‡ºå¼‚å¸¸
                        yield_float = float(yield_val.strip())
                    else:
                        # å°è¯•å¼ºåˆ¶è½¬æ¢
                        yield_float = float(yield_val)
                    
                    # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°å€¼ï¼ˆä¸æ˜¯ NaN æˆ– Infï¼‰
                    if pd.isna(yield_float) or not np.isfinite(yield_float):
                        skipped_count += 1
                        continue
                        
                except (ValueError, TypeError) as e:
                    # æ— æ³•è½¬æ¢ä¸ºæ•°å€¼ï¼Œè·³è¿‡è¿™æ¡è®°å½•
                    skipped_count += 1
                    logger.debug(f"[æ”¶ç›Šç‡æ›²çº¿ä¿å­˜] è·³è¿‡éæ•°å€¼æ•°æ®: date={date_val}, tenor={tenor_val}, yield={yield_val}, error={e}")
                    continue
                
                # æ„å»ºæ–‡æ¡£ï¼ˆåŒ…å«æ›²çº¿åç§°ï¼Œå¦‚æœå­˜åœ¨ï¼‰
                doc = {
                    "date": str(date_val),
                    "tenor": str(tenor_val).strip(),
                    "yield": yield_float,
                    "source": "akshare",
                }
                
                # å¦‚æœå­˜åœ¨æ›²çº¿åç§°ï¼Œä¹Ÿä¿å­˜
                if "curve_name" in r:
                    curve_name = r.get("curve_name")
                    if curve_name and not pd.isna(curve_name):
                        doc["curve_name"] = str(curve_name).strip()
                
                # å¦‚æœå­˜åœ¨æ”¶ç›Šç‡ç±»å‹ï¼Œä¹Ÿä¿å­˜ï¼ˆç”¨äºåŒºåˆ†åˆ°æœŸæ”¶ç›Šç‡ã€å³æœŸæ”¶ç›Šç‡ç­‰ï¼‰
                if "yield_type" in r:
                    yield_type = r.get("yield_type")
                    if yield_type and not pd.isna(yield_type):
                        doc["yield_type"] = str(yield_type).strip()
                
                # ä½¿ç”¨ (date, tenor, curve_name, yield_type?) ä½œä¸ºå”¯ä¸€é”®
                # å¦‚æœæ²¡æœ‰ curve_nameï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä½œä¸ºé»˜è®¤å€¼
                unique_key = {
                    "date": doc["date"],
                    "tenor": doc["tenor"],
                    "curve_name": doc.get("curve_name", "")
                }
                # å¦‚æœæœ‰ yield_typeï¼Œä¹ŸåŠ å…¥åˆ°å”¯ä¸€é”®ä¸­
                if "yield_type" in doc:
                    unique_key["yield_type"] = doc["yield_type"]
                
                ops.append(
                    UpdateOne(
                        unique_key,
                        {"$set": doc},
                        upsert=True
                    )
                )
                valid_count += 1
                
            except Exception as e:
                skipped_count += 1
                logger.warning(f"[æ”¶ç›Šç‡æ›²çº¿ä¿å­˜] å¤„ç†è¡Œæ•°æ®å¤±è´¥: {e}, row={dict(r)}")
                continue
        
        logger.info(f"[æ”¶ç›Šç‡æ›²çº¿ä¿å­˜] å¤„ç†å®Œæˆ: opsæ•°é‡={len(ops)}, valid_count={valid_count}, skipped_count={skipped_count}")
        
        if not ops:
            logger.warning(f"[æ”¶ç›Šç‡æ›²çº¿ä¿å­˜] æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜ï¼ˆè·³è¿‡ {skipped_count} æ¡æ— æ•ˆæ•°æ®ï¼Œæ€»è®¡ {len(df)} æ¡ï¼‰")
            return 0
        
        try:
            # æ‰§è¡Œæ‰¹é‡å†™å…¥
            res = await self.col_curve.bulk_write(ops, ordered=False)
            upserted = res.upserted_count or 0
            modified = res.modified_count or 0
            matched = res.matched_count or 0
            total_saved = upserted + modified + matched
            
            logger.info(f"[æ”¶ç›Šç‡æ›²çº¿ä¿å­˜] æ‰¹é‡å†™å…¥å®Œæˆ: æ–°å¢={upserted}, æ›´æ–°={modified}, åŒ¹é…={matched}, æ€»è®¡={total_saved}, æœ‰æ•ˆæ•°æ®={valid_count}, è·³è¿‡={skipped_count}, æ€»è¡Œæ•°={len(df)}")
            logger.info(f"[æ”¶ç›Šç‡æ›²çº¿ä¿å­˜] è¯¦ç»†ç»“æœ: upserted_count={res.upserted_count}, modified_count={res.modified_count}, matched_count={res.matched_count}")
            
            return total_saved
        except Exception as e:
            # å¤„ç†BulkWriteError - éƒ¨åˆ†æˆåŠŸä¹Ÿå¯ä»¥æå–ç»“æœ
            from pymongo.errors import BulkWriteError
            if isinstance(e, BulkWriteError):
                # BulkWriteErrorä¹ŸåŒ…å«éƒ¨åˆ†æˆåŠŸçš„ç»“æœ
                result = e.details
                upserted = result.get('nUpserted', 0)
                modified = result.get('nModified', 0)
                matched = result.get('nMatched', 0)
                total_saved = upserted + modified + matched
                
                write_errors = result.get('writeErrors', [])
                logger.warning(f"[æ”¶ç›Šç‡æ›²çº¿ä¿å­˜] æ‰¹é‡å†™å…¥éƒ¨åˆ†æˆåŠŸ: æ–°å¢={upserted}, æ›´æ–°={modified}, åŒ¹é…={matched}, æ€»è®¡={total_saved}, é”™è¯¯æ•°={len(write_errors)}")
                
                # è®°å½•å‰å‡ ä¸ªé”™è¯¯ç¤ºä¾‹
                if write_errors:
                    for i, err in enumerate(write_errors[:3]):
                        logger.warning(f"[æ”¶ç›Šç‡æ›²çº¿ä¿å­˜] é”™è¯¯ç¤ºä¾‹ {i+1}: {err.get('errmsg', 'Unknown error')}")
                
                # å³ä½¿æœ‰é”™è¯¯ï¼Œä¹Ÿè¿”å›æˆåŠŸä¿å­˜çš„æ•°é‡
                if total_saved > 0:
                    return total_saved
            
            logger.error(f"[æ”¶ç›Šç‡æ›²çº¿ä¿å­˜] æ‰¹é‡å†™å…¥å¤±è´¥: {e}", exc_info=True)
            return 0

    async def save_bond_daily(self, code: str, df: pd.DataFrame) -> int:
        import logging
        logger = logging.getLogger("webapi")
        
        if df is None or df.empty:
            logger.warning(f"âš ï¸ [æ—¥çº¿æ•°æ®] æ•°æ®ä¸ºç©ºï¼Œcode={code}")
            return 0
        
        # è§„èŒƒåŒ–å€ºåˆ¸ä»£ç 
        from tradingagents.utils.instrument_validator import normalize_bond_code
        norm = normalize_bond_code(code)
        code_std = norm.get("code_std") or code
        
        logger.info(f"ğŸ“Š [æ—¥çº¿æ•°æ®] å¼€å§‹ä¿å­˜ {len(df)} æ¡æ•°æ®, code={code}, code_std={code_std}")
        ops = []
        for _, r in df.iterrows():
            doc = {k: r.get(k) for k in df.columns}
            doc["code"] = code_std  # ä½¿ç”¨è§„èŒƒåŒ–çš„ä»£ç 
            doc["date"] = str(doc.get("date"))
            ops.append(
                UpdateOne({"code": doc["code"], "date": doc["date"]}, {"$set": doc}, upsert=True)
            )
        if ops:
            res = await self.col_daily.bulk_write(ops, ordered=False)
            # upserted_count: æ–°æ’å…¥çš„æ–‡æ¡£æ•°
            # modified_count: å®é™…ä¿®æ”¹çš„æ–‡æ¡£æ•°  
            # matched_count: åŒ¹é…åˆ°çš„æ–‡æ¡£æ•°ï¼ˆåŒ…æ‹¬å†…å®¹ç›¸åŒæœªä¿®æ”¹çš„ï¼‰
            # è¿”å›ï¼šæ–°å¢ + æ›´æ–° + åŒ¹é…ï¼ˆå†…å®¹ç›¸åŒçš„ä¹Ÿç®—æˆåŠŸå¤„ç†ï¼‰
            saved = (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
            logger.info(f"ğŸ’¾ [æ—¥çº¿æ•°æ®] ä¿å­˜å®Œæˆ: æ–°å¢={res.upserted_count}, æ›´æ–°={res.modified_count}, åŒ¹é…={res.matched_count}, æ€»è®¡={saved}")
            return saved
        return 0

    async def save_basic_list(self, items: Iterable[Dict[str, Any]]) -> int:
        """ä¿å­˜å€ºåˆ¸åŸºç¡€ä¿¡æ¯åˆ—è¡¨åˆ°æ•°æ®åº“ï¼ˆä½¿ç”¨codeä½œä¸ºå”¯ä¸€é”®ï¼Œå­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥ï¼‰"""
        import logging
        logger = logging.getLogger("webapi")
        
        ops = []
        valid_count = 0
        skipped_count = 0
        
        for it in items:
            code = str(it.get("code") or "").strip()
            if not code:
                skipped_count += 1
                continue
            
            # è§„èŒƒåŒ–codeï¼ˆç¡®ä¿æ ¼å¼ä¸€è‡´ï¼‰
            from tradingagents.utils.instrument_validator import normalize_bond_code
            norm = normalize_bond_code(code)
            code_std = norm.get("code_std") or code
            
            # è·å–å¹¶è§„èŒƒåŒ–categoryå­—æ®µ
            category_val = it.get("category")
            if category_val and str(category_val).strip():
                category_normalized = str(category_val).strip().lower()
            else:
                category_normalized = "other"  # é»˜è®¤å€¼ï¼Œä¸å†ä½¿ç”¨None
            
            # è°ƒè¯•æ—¥å¿—ï¼šè®°å½•å‰å‡ æ¡æ•°æ®çš„categoryå€¼
            if valid_count < 3:
                logger.debug(f"ğŸ” [å€ºåˆ¸æ•°æ®ä¿å­˜] æ ·æœ¬æ•°æ® {valid_count+1}: code={code_std}, raw_category={category_val}, normalized_category={category_normalized}")
            
            # æ„å»ºæ–‡æ¡£
            doc = {
                "code": code_std,
                "name": it.get("name"),
                "exchange": it.get("exchange"),
                "category": category_normalized,  # ç¡®ä¿categoryå­—æ®µæ€»æ˜¯æœ‰å€¼
                "issuer": it.get("issuer"),
                "list_date": str(it.get("list_date")) if it.get("list_date") else None,
                "maturity_date": str(it.get("maturity_date")) if it.get("maturity_date") else None,
                "coupon_rate": it.get("coupon_rate"),
                "type": it.get("type"),
                "raw_code": it.get("raw_code"),
                "source": it.get("source", "akshare"),
                "updated_at": datetime.now().isoformat(),  # æ·»åŠ æ›´æ–°æ—¶é—´
            }
            
            # ç§»é™¤Noneå€¼ï¼ˆä½†ä¿ç•™categoryå­—æ®µï¼Œå› ä¸ºå®ƒæ€»æ˜¯æœ‰å€¼ï¼‰
            doc = {k: v for k, v in doc.items() if v is not None}
            
            # ä½¿ç”¨codeä½œä¸ºå”¯ä¸€é”®ï¼Œå­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥
            ops.append(UpdateOne(
                {"code": code_std},
                {"$set": doc, "$setOnInsert": {"created_at": datetime.now().isoformat()}},
                upsert=True
            ))
            valid_count += 1
        
        if not ops:
            logger.warning(f"âš ï¸ [å€ºåˆ¸æ•°æ®ä¿å­˜] æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜ï¼ˆè·³è¿‡ {skipped_count} æ¡æ— æ•ˆæ•°æ®ï¼‰")
            return 0
        
        try:
            # æ‰§è¡Œæ‰¹é‡å†™å…¥
            res = await self.col_basic.bulk_write(ops, ordered=False)
            upserted = res.upserted_count or 0
            modified = res.modified_count or 0
            matched = res.matched_count or 0
            
            # ç»Ÿè®¡å¤„ç†çš„æ•°æ®æ•°é‡ï¼š
            # - upserted_count: æ–°æ’å…¥çš„æ•°æ®
            # - modified_count: æ›´æ–°çš„æ•°æ®
            # - matched_count: åŒ¹é…åˆ°çš„æ•°æ®ï¼ˆå³ä½¿æ²¡æœ‰ä¿®æ”¹ä¹Ÿä¼šè¢«è®¡æ•°ï¼‰
            # å¦‚æœ matched_count > (upserted + modified)ï¼Œè¯´æ˜æœ‰äº›æ•°æ®å·²å­˜åœ¨ä¸”æ— éœ€æ›´æ–°
            total_processed = upserted + modified
            # å¦‚æœ matched_count å¤§äº processedï¼Œè¯´æ˜æœ‰äº›æ•°æ®å·²å­˜åœ¨ä½†æ²¡æœ‰å˜åŒ–
            if matched > total_processed:
                # å®é™…å¤„ç†çš„æ•°é‡åº”è¯¥åŒ…æ‹¬å·²å­˜åœ¨ä½†æœªæ›´æ–°çš„æ•°æ®
                total_processed = matched
            
            logger.info(f"ğŸ’¾ [å€ºåˆ¸æ•°æ®ä¿å­˜] æ‰¹é‡å†™å…¥å®Œæˆ: æ–°å¢={upserted}, æ›´æ–°={modified}, åŒ¹é…={matched}, æ€»è®¡={total_processed}, æœ‰æ•ˆæ•°æ®={valid_count}, è·³è¿‡={skipped_count}")
            
            # æ·»åŠ è°ƒè¯•ï¼šæŸ¥è¯¢æ•°æ®åº“ä¸­çš„categoryåˆ†å¸ƒ
            try:
                pipeline = [{"$group": {"_id": "$category", "count": {"$sum": 1}}}]
                category_stats = []
                async for doc in self.col_basic.aggregate(pipeline):
                    category_stats.append(f"{doc.get('_id', 'null')}: {doc.get('count', 0)}")
                logger.info(f"ğŸ“Š [å€ºåˆ¸æ•°æ®ä¿å­˜] æ•°æ®åº“categoryåˆ†å¸ƒ: {', '.join(category_stats)}")
            except Exception as stats_err:
                logger.warning(f"âš ï¸ [å€ºåˆ¸æ•°æ®ä¿å­˜] æ— æ³•è·å–categoryç»Ÿè®¡: {stats_err}")
            
            # å¦‚æœä¿å­˜æ•°é‡å¼‚å¸¸ï¼Œè®°å½•è­¦å‘Š
            if total_processed == 0 and valid_count > 0:
                logger.warning(f"âš ï¸ [å€ºåˆ¸æ•°æ®ä¿å­˜] è­¦å‘Šï¼šæœ‰ {valid_count} æ¡æœ‰æ•ˆæ•°æ®ï¼Œä½†ä¿å­˜æ•°é‡ä¸º0ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®æ ¼å¼é—®é¢˜")
            
            return total_processed if total_processed > 0 else (upserted + modified)
        except Exception as e:
            logger.error(f"âŒ [å€ºåˆ¸æ•°æ®ä¿å­˜] æ‰¹é‡å†™å…¥å¤±è´¥: {e}", exc_info=True)
            return 0

    async def query_basic_list(
        self,
        q: Optional[str] = None,
        category: Optional[str] = None,
        exchange: Optional[str] = None,
        only_not_matured: bool = False,
        page: int = 1,
        page_size: int = 20,
        sort_by: Optional[str] = None,
        sort_dir: str = "asc",
    ) -> Dict[str, Any]:
        import logging
        logger = logging.getLogger("webapi")
        
        filt: Dict[str, Any] = {}
        if q:
            q_regex = {"$regex": q, "$options": "i"}
            filt["$or"] = [{"code": q_regex}, {"name": q_regex}]
        if category:
            filt["category"] = str(category).lower()
        if exchange:
            filt["exchange"] = str(exchange).upper()
        # ä»…å¯¹åˆ©ç‡å€ºå¯ç”¨æœªåˆ°æœŸè¿‡æ»¤
        if only_not_matured and (not category or str(category).lower() == "interest"):
            try:
                import datetime as _dt
                today = _dt.datetime.utcnow().strftime("%Y-%m-%d")
            except Exception:
                today = "1970-01-01"
            filt["maturity_date"] = {"$gte": today}

        # è°ƒè¯•æ—¥å¿—ï¼šè®°å½•æŸ¥è¯¢æ¡ä»¶
        logger.debug(f"ğŸ” [å€ºåˆ¸æŸ¥è¯¢] æŸ¥è¯¢æ¡ä»¶: {filt}")
        
        total = await self.col_basic.count_documents(filt)
        logger.debug(f"ğŸ“Š [å€ºåˆ¸æŸ¥è¯¢] æŸ¥è¯¢ç»“æœæ€»æ•°: {total}")
        if total == 0:
            return {"total": 0, "items": []}
        skip = max(0, (page - 1) * page_size)
        allowed = {"code", "name", "maturity_date", "list_date", "coupon_rate"}
        field = (sort_by or "code").lower()
        if field not in allowed:
            field = "code"
        direc = 1 if str(sort_dir).lower() != "desc" else -1
        cursor = self.col_basic.find(filt).sort([(field, direc)]).skip(skip).limit(page_size)
        items = []
        async for doc in cursor:
            # ç§»é™¤ _id å­—æ®µï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
            if "_id" in doc:
                doc.pop("_id", None)
            items.append(doc)
        return {"total": total, "items": items}

    async def save_spot_quotes(self, df: pd.DataFrame, category: str) -> int:
        """ä¿å­˜å€ºåˆ¸ç°è´§æŠ¥ä»·æ•°æ®ï¼ˆæ ¹æ®AKShareæ¥å£å­—æ®µç»“æ„æ˜ å°„ï¼‰"""
        import logging
        logger = logging.getLogger("webapi")
        
        if df is None or df.empty:
            return 0
        
        ops = []
        valid_count = 0
        skipped_count = 0
        
        for _, r in df.iterrows():
            try:
                # æå–ä»£ç ï¼ˆæ”¯æŒå¤šç§å­—æ®µåï¼‰
                code = str(r.get("code") or r.get("å€ºåˆ¸ä»£ç ") or r.get("å¯è½¬å€ºä»£ç ") or r.get("ä»£ç ") or r.get("symbol") or "").strip()
                if not code:
                    skipped_count += 1
                    continue
                
                # è§„èŒƒåŒ–ä»£ç 
                from tradingagents.utils.instrument_validator import normalize_bond_code
                norm = normalize_bond_code(code)
                code_std = norm.get("code_std") or code
                
                # æå–æ—¶é—´æˆ³å’Œæ—¥æœŸ
                timestamp = (
                    r.get("timestamp") or r.get("ticktime") or 
                    r.get("time") or datetime.now().strftime("%H:%M:%S")
                )
                timestamp = str(timestamp).strip()
                
                # æå–æ—¥æœŸ
                date = r.get("date") or r.get("æ—¥æœŸ") or datetime.now().strftime("%Y-%m-%d")
                date = str(date).strip()[:10]  # åªä¿ç•™æ—¥æœŸéƒ¨åˆ†
                
                # å­—æ®µæ˜ å°„è¡¨ï¼šAKShareå­—æ®µ -> æ ‡å‡†å­—æ®µ
                field_mapping = {
                    # ä»·æ ¼ç›¸å…³
                    "æœ€æ–°ä»·": "latest_price",
                    "trade": "latest_price",
                    "price": "latest_price",
                    # æ¶¨è·Œç›¸å…³
                    "æ¶¨è·Œé¢": "change",
                    "pricechange": "change",
                    "æ¶¨è·Œå¹…": "change_percent",
                    "changepercent": "change_percent",
                    # ä¹°å–ä»·
                    "ä¹°å…¥": "buy",
                    "å–å‡º": "sell",
                    # æ˜¨æ”¶å’Œä»Šå¼€
                    "æ˜¨æ”¶": "prev_close",
                    "preclose": "prev_close",
                    "ä»Šå¼€": "open",
                    "open": "open",
                    # æœ€é«˜æœ€ä½
                    "æœ€é«˜": "high",
                    "high": "high",
                    "æœ€ä½": "low",
                    "low": "low",
                    # æˆäº¤ç›¸å…³
                    "æˆäº¤é‡": "volume",
                    "volume": "volume",
                    "æˆäº¤é¢": "amount",
                    "amount": "amount",
                    # åç§°
                    "åç§°": "name",
                    "name": "name",
                }
                
                # æ„å»ºæ ‡å‡†æ–‡æ¡£
                doc = {
                    "code": code_std,
                    "date": date,
                    "timestamp": timestamp,
                    "category": category,
                    "source": "akshare",
                }
                
                # æ˜ å°„å­—æ®µ
                for ak_field, std_field in field_mapping.items():
                    if ak_field in r.index or ak_field in r:
                        value = r.get(ak_field)
                        if value is not None and not pd.isna(value):
                            doc[std_field] = value
                
                # å¦‚æœnameå­—æ®µè¿˜æœªè®¾ç½®ï¼Œå°è¯•ä»å…¶ä»–å­—æ®µè·å–
                if "name" not in doc:
                    name = str(r.get("åç§°") or r.get("name") or "").strip()
                    if name:
                        doc["name"] = name
                
                # ä¿å­˜æ‰€æœ‰åŸå§‹å­—æ®µï¼ˆç”¨äºè°ƒè¯•ï¼‰
                doc["_raw"] = dict(r)
                
                # ç§»é™¤Noneå€¼
                doc = {k: v for k, v in doc.items() if v is not None}
                
                # ä½¿ç”¨ (code, date, category) ä½œä¸ºå”¯ä¸€é”®ï¼ˆåŒä¸€å¤©åŒä¸€å€ºåˆ¸åŒä¸€ç±»åˆ«åªä¿ç•™æœ€æ–°æ•°æ®ï¼‰
                ops.append(
                    UpdateOne(
                        {"code": code_std, "date": date, "category": category},
                        {"$set": doc},
                        upsert=True
                    )
                )
                valid_count += 1
                
            except Exception as e:
                skipped_count += 1
                logger.warning(f"[ç°è´§æŠ¥ä»·ä¿å­˜] å¤„ç†è¡Œæ•°æ®å¤±è´¥: {e}")
                continue
        
        if not ops:
            logger.warning(f"[ç°è´§æŠ¥ä»·ä¿å­˜] æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜ï¼ˆè·³è¿‡ {skipped_count} æ¡ï¼‰")
            return 0
        
        try:
            res = await self.col_spot.bulk_write(ops, ordered=False)
            upserted = res.upserted_count or 0
            modified = res.modified_count or 0
            matched = res.matched_count or 0
            total_saved = upserted + modified + matched
            
            logger.info(f"[ç°è´§æŠ¥ä»·ä¿å­˜] æ‰¹é‡å†™å…¥å®Œæˆ: æ–°å¢={upserted}, æ›´æ–°={modified}, åŒ¹é…={matched}, æ€»è®¡={total_saved}, æœ‰æ•ˆ={valid_count}, è·³è¿‡={skipped_count}")
            
            return total_saved
        except Exception as e:
            logger.error(f"[ç°è´§æŠ¥ä»·ä¿å­˜] æ‰¹é‡å†™å…¥å¤±è´¥: {e}", exc_info=True)
            return 0

    async def save_indices(self, df: pd.DataFrame, index_id: str, value_column: str = "value") -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            date = str(r.get("date"))
            val = r.get(value_column)
            doc = {"index_id": index_id, "date": date, value_column: val, "source": "akshare"}
            ops.append(UpdateOne({"index_id": index_id, "date": date}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_indices.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    async def save_us_yields(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            doc = {
                "date": str(r.get("date")),
                "tenor": r.get("tenor"),
                "yield": None if pd.isna(r.get("yield")) else float(r.get("yield")),
                "source": "akshare",
            }
            ops.append(UpdateOne({"date": doc["date"], "tenor": doc["tenor"]}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_us_yield.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    async def save_cb_profiles(self, profiles: Iterable[Dict[str, Any]]) -> int:
        ops = []
        for p in profiles:
            code = p.get("code")
            if not code:
                continue
            ops.append(UpdateOne({"code": code}, {"$set": p}, upsert=True))
        if ops:
            res = await self.col_cb_profiles.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    async def save_buybacks(self, df: pd.DataFrame, exchange: str) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            doc = r.to_dict()
            doc["exchange"] = exchange
            date = str(doc.get("date") or doc.get("æ—¥æœŸ") or doc.get("å…¬å‘Šæ—¥æœŸ") or "")
            code = str(doc.get("code") or doc.get("è¯åˆ¸ä»£ç ") or doc.get("å€ºåˆ¸ä»£ç ") or "").strip()
            if date:
                doc["date"] = date
            if code:
                doc["code"] = code
            ops.append(
                UpdateOne({"exchange": exchange, "date": doc.get("date"), "code": doc.get("code")}, {"$set": doc}, upsert=True)
            )
        if ops:
            res = await self.col_buybacks.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    # ========== é€šç”¨è¾…åŠ© ==========
    @staticmethod
    def _norm_code(row: dict) -> str:
        for k in (
            "code",
            "è¯åˆ¸ä»£ç ",
            "å€ºåˆ¸ä»£ç ",
            "å¯è½¬å€ºä»£ç ",
            "æŸ¥è¯¢ä»£ç ",
            "bondCode",
            "symbol",
            "ä»£ç ",
        ):
            v = row.get(k)
            if v is not None and str(v).strip():
                return str(v).strip()
        return ""

    @staticmethod
    def _norm_date(row: dict) -> str:
        for k in ("date", "æ—¥æœŸ", "æ•°æ®æ—¥æœŸ", "å…¬å‘Šæ—¥æœŸ", "list_date", "ä¸Šå¸‚æ—¥æœŸ"):
            v = row.get(k)
            if v is not None and str(v).strip():
                try:
                    import pandas as pd  # local import
                    return pd.to_datetime(v).strftime("%Y-%m-%d")
                except Exception:
                    return str(v).strip()
        return ""

    # ========== CNINFO å‘è¡Œ ==========
    async def save_cninfo_issues(self, df: pd.DataFrame, issue_type: str, endpoint: str) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            date = self._norm_date(row)
            doc = row
            doc.update({"issue_type": issue_type, "endpoint": endpoint, "code": code, "date": date, "source": "akshare"})
            filt = {"issue_type": issue_type, "endpoint": endpoint, "code": code, "date": date}
            ops.append(UpdateOne(filt, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_issues.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    # ========== å¯è½¬å€ºäº‹ä»¶/ä¼°å€¼ ==========
    async def save_cb_adjustments(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            date = self._norm_date(row)
            filt = {"code": code, "date": date}
            doc = row
            doc.update({"code": code, "date": date, "source": "akshare"})
            ops.append(UpdateOne(filt, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_cb_adjustments.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    async def save_cb_redeems(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            date = self._norm_date(row)
            filt = {"code": code, "date": date}
            doc = row
            doc.update({"code": code, "date": date, "source": "akshare"})
            ops.append(UpdateOne(filt, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_cb_redeems.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    async def save_cb_summary(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            filt = {"code": code}
            doc = row
            doc.update({"code": code, "source": "akshare"})
            ops.append(UpdateOne(filt, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_cb_summary.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    async def save_cb_valuation(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            date = self._norm_date(row)
            filt = {"code": code, "date": date}
            doc = row
            doc.update({"code": code, "date": date, "source": "akshare"})
            ops.append(UpdateOne(filt, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_cb_valuation.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    async def save_cb_comparison(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            date = self._norm_date(row)
            code = self._norm_code(row)
            if not code:
                code = str(row.get("å€ºåˆ¸ç®€ç§°") or row.get("åç§°") or "").strip()
            filt = {"date": date, "code": code}
            doc = row
            doc.update({"date": date, "code": code, "source": "akshare"})
            ops.append(UpdateOne(filt, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_cb_comparison.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    # ========== æŠ¥ä»·/æˆäº¤/æ±‡æ€» ==========
    async def save_spot_quote_detail(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            if not code:
                code = str(row.get("å€ºåˆ¸ç®€ç§°") or row.get("åç§°") or row.get("æŠ¥ä»·å“ç§") or "").strip()
            ts = str(row.get("timestamp") or row.get("time") or row.get("æ—¶é—´") or row.get("æ—¥æœŸ") or "")
            doc = row
            dealer = str(row.get("æŠ¥ä»·æœºæ„") or "").strip()
            doc.update({"code": code, "timestamp": ts, "source": "akshare"})
            if dealer:
                ops.append(UpdateOne({"code": code, "timestamp": ts, "æŠ¥ä»·æœºæ„": dealer}, {"$set": doc}, upsert=True))
            else:
                ops.append(UpdateOne({"code": code, "timestamp": ts}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_spot_quote_detail.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0


    async def save_deal_summary(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            date = self._norm_date(row)
            # è½¬æ¢æ‰€æœ‰æ—¥æœŸç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…MongoDBç¼–ç é”™è¯¯
            doc = {}
            for k, v in row.items():
                if isinstance(v, (pd.Timestamp, dt.date, dt.datetime)):
                    doc[k] = pd.to_datetime(v).strftime("%Y-%m-%d")
                else:
                    doc[k] = v
            doc.update({"date": date, "source": "akshare"})
            bt = row.get("å€ºåˆ¸ç±»å‹")
            if bt is not None:
                ops.append(UpdateOne({"date": date, "å€ºåˆ¸ç±»å‹": bt}, {"$set": doc}, upsert=True))
            else:
                ops.append(UpdateOne({"date": date}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_deal_summary.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    async def save_cash_summary(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            date = self._norm_date(row)
            # è½¬æ¢æ‰€æœ‰æ—¥æœŸç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…MongoDBç¼–ç é”™è¯¯
            doc = {}
            for k, v in row.items():
                if isinstance(v, (pd.Timestamp, dt.date, dt.datetime)):
                    doc[k] = pd.to_datetime(v).strftime("%Y-%m-%d")
                else:
                    doc[k] = v
            doc.update({"date": date, "source": "akshare"})
            ops.append(UpdateOne({"date": date}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_cash_summary.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    # ========== NAFMII / ä¸­å€ºä¿¡æ¯ ==========
    async def save_nafmii(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            date = self._norm_date(row)
            doc = row
            reg_no = str(row.get("æ³¨å†Œé€šçŸ¥ä¹¦æ–‡å·") or row.get("reg_no") or "").strip()
            doc.update({"code": code, "date": date, "source": "akshare"})
            if reg_no:
                doc["reg_no"] = reg_no
                filt = {"reg_no": reg_no}
            else:
                # å›é€€åˆ°åç§°+æ—¥æœŸ+code çš„ç»„åˆ
                filt = {"code": code, "date": date, "å€ºåˆ¸åç§°": row.get("å€ºåˆ¸åç§°")}
            ops.append(UpdateOne(filt, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_nafmii.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    async def save_info_cm(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            doc = row
            doc.update({"code": code, "source": "akshare", "endpoint": "bond_info_cm"})
            ops.append(UpdateOne({"code": code, "endpoint": "bond_info_cm"}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_info_cm.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    async def save_yield_curve_map(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            date = self._norm_date(row)
            doc = row
            doc.update({"date": date, "source": "akshare"})
            ops.append(UpdateOne({"date": date}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_curve_map.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    async def save_buybacks_history(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            date = self._norm_date(row)
            exch = str(row.get("exchange") or row.get("äº¤æ˜“æ‰€") or "").strip()
            doc = row
            doc.update({"date": date, "exchange": exch, "source": "akshare"})
            ops.append(UpdateOne({"exchange": exch, "date": date}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_buybacks_hist.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    async def save_cb_list_jsl(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            doc = row
            doc.update({"code": code, "source": "akshare"})
            ops.append(UpdateOne({"code": code}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_cb_list_jsl.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    async def save_cov_list(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            doc = row
            doc.update({"code": code, "source": "akshare"})
            ops.append(UpdateOne({"code": code}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_cov_list.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    async def save_info_cm_query(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        ops = []
        for _, r in df.iterrows():
            row = r.to_dict()
            code = self._norm_code(row)
            doc = row
            doc.update({"code": code, "source": "akshare", "endpoint": "bond_info_cm_query"})
            ops.append(UpdateOne({"code": code, "endpoint": "bond_info_cm_query"}, {"$set": doc}, upsert=True))
        if ops:
            res = await self.col_info_cm.bulk_write(ops, ordered=False)
            return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
        return 0

    async def save_info_cm_detail(self, df: pd.DataFrame) -> int:
        if df is None or df.empty:
            return 0
        # è‹¥ä¸º name/value å½¢å¼ï¼Œåˆå¹¶ä¸ºå•æ–‡æ¡£
        try:
            cols = [c.lower() for c in df.columns]
        except Exception:
            cols = []
        if ("name" in cols) and ("value" in cols):
            mapping = {}
            for _, r in df.iterrows():
                name_key = r.get("name") if "name" in df.columns else r.get(df.columns[0])
                value_val = r.get("value") if "value" in df.columns else r.get(df.columns[1])
                if name_key is not None and str(name_key).strip():
                    mapping[str(name_key).strip()] = value_val
            # æå–codeï¼ˆä¼˜å…ˆä½¿ç”¨æ˜ç¡®å®šä¹‰çš„ç¼–ç å­—æ®µï¼‰
            code = (
                str(mapping.get("bondCode") or mapping.get("å€ºåˆ¸ä»£ç ") or mapping.get("bondDefinedCode") or mapping.get("æŸ¥è¯¢ä»£ç ") or "").strip()
            )
            if not code:
                # å›é€€ï¼šå°è¯•ä»æ˜ å°„ä¸­æ‰¾å¸¸è§é”®
                for k in ("code", "è¯åˆ¸ä»£ç ", "å¯è½¬å€ºä»£ç "):
                    if mapping.get(k):
                        code = str(mapping.get(k)).strip()
                        break
            doc = mapping
            doc.update({"code": code, "source": "akshare", "endpoint": "bond_info_detail_cm"})
            res = await self.col_info_cm.update_one(
                {"code": code, "endpoint": "bond_info_detail_cm"},
                {"$set": doc},
                upsert=True,
            )
            return 1 if (res.upserted_id or res.modified_count) else 0
        else:
            # å›é€€ï¼šé€è¡Œå†™å…¥ï¼ˆåŒä¸€ä¸ªcodeä¼šè¢«åç»­è¡Œè¦†ç›–ï¼Œä»ä¿æŒå•æ–‡æ¡£ï¼‰
            ops = []
            for _, r in df.iterrows():
                row = r.to_dict()
                code = self._norm_code(row)
                if not code:
                    code = str(row.get("bondCode") or row.get("å€ºåˆ¸ä»£ç ") or row.get("bondDefinedCode") or row.get("æŸ¥è¯¢ä»£ç ") or "").strip()
                doc = row
                doc.update({"code": code, "source": "akshare", "endpoint": "bond_info_detail_cm"})
                ops.append(UpdateOne({"code": code, "endpoint": "bond_info_detail_cm"}, {"$set": doc}, upsert=True))
            if ops:
                res = await self.col_info_cm.bulk_write(ops, ordered=False)
                return (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
            return 0

    async def query_bond_daily(self, code: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """æŸ¥è¯¢å€ºåˆ¸å†å²æ•°æ®"""
        from tradingagents.utils.instrument_validator import normalize_bond_code
        norm = normalize_bond_code(code)
        code_std = norm.get("code_std") or code
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        query = {
            "code": code_std,
            "date": {"$gte": start_date, "$lte": end_date}
        }
        
        # æŸ¥è¯¢æ•°æ®
        cursor = self.col_daily.find(query).sort("date", 1)
        docs = [doc async for doc in cursor]
        
        if not docs:
            return None
        
        # è½¬æ¢ä¸ºDataFrame
        for doc in docs:
            doc.pop("_id", None)
        
        df = pd.DataFrame(docs)
        return df if not df.empty else None

    async def query_yield_curve(
        self, 
        start_date: Optional[str] = None, 
        end_date: Optional[str] = None, 
        curve_name: Optional[str] = None,
        tenor: Optional[str] = None,
        limit: int = 100
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢æ”¶ç›Šç‡æ›²çº¿æ•°æ®
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            curve_name: æ›²çº¿åç§°ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚"ä¸­å€ºå›½å€ºæ”¶ç›Šç‡æ›²çº¿"
            tenor: æœŸé™ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚"10å¹´"
            limit: è¿”å›æ•°é‡é™åˆ¶
        
        Returns:
            Dict: {"total": int, "items": list}
        """
        import logging
        logger = logging.getLogger("webapi")
        
        try:
            query = {}
            if start_date:
                query["date"] = {"$gte": start_date}
            if end_date:
                if "date" in query:
                    query["date"]["$lte"] = end_date
                else:
                    query["date"] = {"$lte": end_date}
            if curve_name:
                query["curve_name"] = str(curve_name).strip()
            if tenor:
                query["tenor"] = str(tenor).strip()
            
            total = await self.col_curve.count_documents(query)
            
            if total == 0:
                return {"total": 0, "items": []}
            
            cursor = self.col_curve.find(query).sort("date", -1).sort("tenor", 1).limit(limit)
            items = []
            async for doc in cursor:
                if "_id" in doc:
                    doc.pop("_id", None)
                # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„å­—æ®µéƒ½å­˜åœ¨
                if "curve_name" not in doc:
                    doc["curve_name"] = ""
                items.append(doc)
            
            logger.debug(f"ğŸ“Š [æ”¶ç›Šç‡æ›²çº¿] æŸ¥è¯¢æˆåŠŸ: {len(items)}/{total}")
            return {"total": total, "items": items}
        except Exception as e:
            logger.error(f"âŒ [æ”¶ç›Šç‡æ›²çº¿] æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
            return {"total": 0, "items": []}

    async def save_bond_info_from_api(self, code: str, info_dict: Dict[str, Any]) -> int:
        """å°†ä»æ¥å£è·å–çš„å€ºåˆ¸è¯¦æƒ…ä¿¡æ¯ä¿å­˜åˆ°æ•°æ®åº“"""
        from tradingagents.utils.instrument_validator import normalize_bond_code
        norm = normalize_bond_code(code)
        code_std = norm.get("code_std") or code
        
        # æ›´æ–°åŸºç¡€ä¿¡æ¯è¡¨
        basic_doc = {
            "code": code_std,
            "name": info_dict.get("name"),
            "exchange": info_dict.get("exchange"),
            "category": info_dict.get("category"),
            "issuer": info_dict.get("issuer"),
            "list_date": info_dict.get("list_date"),
            "maturity_date": info_dict.get("maturity_date"),
            "coupon_rate": info_dict.get("coupon_rate"),
            "source": info_dict.get("source", "akshare"),
        }
        
        # ç§»é™¤Noneå€¼
        basic_doc = {k: v for k, v in basic_doc.items() if v is not None}
        
        await self.col_basic.update_one(
            {"code": code_std},
            {"$set": basic_doc},
            upsert=True
        )
        
        # å¦‚æœæœ‰è¯¦ç»†ä¿¡æ¯ï¼Œä¿å­˜åˆ°è¯¦ç»†ä¿¡æ¯è¡¨
        # å°†info_dictè½¬æ¢ä¸ºDataFrameæ ¼å¼ä»¥ä¾¿ä¿å­˜
        if info_dict and len(info_dict) > 1:  # ä¸æ­¢codeå­—æ®µ
            detail_df = pd.DataFrame([info_dict])
            saved = await self.save_info_cm_detail(detail_df)
            return saved
        
        return 1

    async def query_bond_info(self, code: str) -> Optional[Dict[str, Any]]:
        """æŸ¥è¯¢å€ºåˆ¸è¯¦æƒ…ä¿¡æ¯ï¼Œåˆå¹¶åŸºç¡€ä¿¡æ¯å’Œè¯¦ç»†ä¿¡æ¯"""
        # æ ‡å‡†åŒ–ä»£ç 
        from tradingagents.utils.instrument_validator import normalize_bond_code
        norm = normalize_bond_code(code)
        code_std = norm.get("code_std") or code
        
        # å…ˆæŸ¥è¯¢åŸºç¡€ä¿¡æ¯
        basic_info = await self.col_basic.find_one({"code": code_std})
        
        # æŸ¥è¯¢è¯¦ç»†ä¿¡æ¯ï¼ˆbond_info_detail_cmï¼‰- æ”¯æŒå¤šç§ä»£ç æ ¼å¼æŸ¥è¯¢
        detail_info = await self.col_info_cm.find_one({
            "code": code_std,
            "endpoint": "bond_info_detail_cm"
        })
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ç”¨åŸå§‹ä»£ç æŸ¥è¯¢
        if not detail_info:
            detail_info = await self.col_info_cm.find_one({
                "$or": [
                    {"code": code},
                    {"code": norm.get("digits")},
                    {"å€ºåˆ¸ä»£ç ": code},
                    {"å€ºåˆ¸ä»£ç ": code_std},
                    {"å€ºåˆ¸ä»£ç ": norm.get("digits")},
                ],
                "endpoint": "bond_info_detail_cm"
            })
        
        # åˆå¹¶ä¿¡æ¯
        result = {}
        
        # å¦‚æœæœ‰åŸºç¡€ä¿¡æ¯ï¼Œåˆå¹¶åˆ°ç»“æœä¸­
        if basic_info:
            basic_info.pop("_id", None)
            result.update(basic_info)
        
        # å¦‚æœæœ‰è¯¦ç»†ä¿¡æ¯ï¼Œåˆå¹¶åˆ°ç»“æœä¸­ï¼ˆè¯¦ç»†ä¿¡æ¯å­—æ®µä¼˜å…ˆï¼‰
        if detail_info:
            detail_info.pop("_id", None)
            detail_info.pop("endpoint", None)  # ç§»é™¤å†…éƒ¨å­—æ®µ
            detail_info.pop("source", None)  # å…ˆç§»é™¤ï¼Œåé¢ç»Ÿä¸€è®¾ç½®
            
            # å®šä¹‰å­—æ®µæ˜ å°„è¡¨ï¼šè¯¦ç»†ä¿¡æ¯å­—æ®µ -> æ ‡å‡†å­—æ®µ
            field_mapping = {
                # åç§°ç›¸å…³
                "å€ºåˆ¸åç§°": "name",
                "åç§°": "name",
                "å€ºåˆ¸å…¨ç§°": "name",
                # å‘è¡Œäººç›¸å…³
                "å‘è¡Œäºº": "issuer",
                "å‘è¡Œä¸»ä½“": "issuer",
                "å‘è¡Œäººå…¨ç§°": "issuer",
                # æ¯ç¥¨ç‡ç›¸å…³
                "ç¥¨é¢åˆ©ç‡": "coupon_rate",
                "æ¯ç¥¨ç‡": "coupon_rate",
                "åˆ©ç‡": "coupon_rate",
                "ç¥¨æ¯": "coupon_rate",
                # ä¸Šå¸‚æ—¥æœŸç›¸å…³
                "ä¸Šå¸‚æ—¥æœŸ": "list_date",
                "ä¸Šå¸‚æ—¥": "list_date",
                "ä¸Šå¸‚æ—¶é—´": "list_date",
                # åˆ°æœŸæ—¥ç›¸å…³
                "åˆ°æœŸæ—¥": "maturity_date",
                "åˆ°æœŸæ—¥æœŸ": "maturity_date",
                "å€ºåˆ¸åˆ°æœŸæ—¥": "maturity_date",
                "åˆ°æœŸæ—¶é—´": "maturity_date",
                # äº¤æ˜“æ‰€ç›¸å…³
                "äº¤æ˜“æ‰€": "exchange",
                "ä¸Šå¸‚äº¤æ˜“æ‰€": "exchange",
                "äº¤æ˜“å¸‚åœº": "exchange",
            }
            
            # å°†è¯¦ç»†ä¿¡æ¯ä¸­çš„å­—æ®µåˆå¹¶
            for key, value in detail_info.items():
                if value is not None and value != "" and str(value).strip() != "nan":
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ˜ å°„å­—æ®µå
                    mapped_key = field_mapping.get(key, key)
                    
                    # å¦‚æœç›®æ ‡å­—æ®µå·²ç»æœ‰å€¼ï¼Œè·³è¿‡
                    if mapped_key in result and result[mapped_key]:
                        continue
                    
                    # æ ¹æ®å­—æ®µç±»å‹è¿›è¡Œè½¬æ¢
                    if mapped_key == "name":
                        result["name"] = str(value).strip()
                    elif mapped_key == "issuer":
                        result["issuer"] = str(value).strip()
                    elif mapped_key == "coupon_rate":
                        try:
                            if isinstance(value, str):
                                value_str = value.strip().strip('%')
                                result["coupon_rate"] = float(value_str)
                            else:
                                result["coupon_rate"] = float(value)
                        except (ValueError, TypeError):
                            pass
                    elif mapped_key == "list_date":
                        try:
                            if hasattr(value, 'strftime'):  # pandas Timestamp æˆ– datetime
                                result["list_date"] = value.strftime("%Y-%m-%d")
                            elif isinstance(value, str) and len(value) >= 10:
                                # å°è¯•æ ¼å¼åŒ–æ—¥æœŸå­—ç¬¦ä¸²
                                result["list_date"] = value[:10].replace("/", "-")
                            else:
                                result["list_date"] = str(value)
                        except Exception:
                            result["list_date"] = str(value) if value else None
                    elif mapped_key == "maturity_date":
                        try:
                            if hasattr(value, 'strftime'):  # pandas Timestamp æˆ– datetime
                                result["maturity_date"] = value.strftime("%Y-%m-%d")
                            elif isinstance(value, str) and len(value) >= 10:
                                # å°è¯•æ ¼å¼åŒ–æ—¥æœŸå­—ç¬¦ä¸²
                                result["maturity_date"] = value[:10].replace("/", "-")
                            else:
                                result["maturity_date"] = str(value)
                        except Exception:
                            result["maturity_date"] = str(value) if value else None
                    elif mapped_key == "exchange":
                        # æ ‡å‡†åŒ–äº¤æ˜“æ‰€ä»£ç 
                        exchange_val = str(value).strip().upper()
                        if "ä¸Šæµ·" in exchange_val or "ä¸Šäº¤æ‰€" in exchange_val or exchange_val in ["SH", "SHANGHAI"]:
                            result["exchange"] = "SH"
                        elif "æ·±åœ³" in exchange_val or "æ·±äº¤æ‰€" in exchange_val or exchange_val in ["SZ", "SHENZHEN"]:
                            result["exchange"] = "SZ"
                        else:
                            result["exchange"] = exchange_val
                    else:
                        # å…¶ä»–å­—æ®µç›´æ¥æ·»åŠ ï¼Œä½†ä¸è¦†ç›–å·²æœ‰å­—æ®µ
                        if mapped_key not in result:
                            result[mapped_key] = value
        
        # ç¡®ä¿codeå­—æ®µå­˜åœ¨
        if "code" not in result:
            result["code"] = code_std
        
        # ç¡®ä¿sourceå­—æ®µå­˜åœ¨
        if "source" not in result:
            result["source"] = "database"
        
        # å¦‚æœæœ‰ä»»ä½•å­—æ®µï¼Œè¿”å›ç»“æœ
        return result if result else None

    async def save_bond_minute_quotes(self, code: str, df: pd.DataFrame, period: str = "1") -> int:
        """ä¿å­˜å€ºåˆ¸åˆ†é’Ÿæ•°æ®
        
        Args:
            code: å€ºåˆ¸ä»£ç ï¼ˆæ ‡å‡†åŒ–åï¼‰
            df: åŒ…å«åˆ†é’Ÿæ•°æ®çš„DataFrameï¼Œå¿…é¡»åŒ…å«datetimeæˆ–æ—¶é—´åˆ—
            period: æ•°æ®å‘¨æœŸï¼ˆ"1", "5", "15", "30", "60"ï¼‰
        
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        import logging
        logger = logging.getLogger("webapi")
        
        if df is None or df.empty:
            return 0
        
        ops = []
        valid_count = 0
        skipped_count = 0
        
        # è§„èŒƒåŒ–ä»£ç 
        from tradingagents.utils.instrument_validator import normalize_bond_code
        norm = normalize_bond_code(code)
        code_std = norm.get("code_std") or code
        
        # å­—æ®µæ˜ å°„è¡¨ï¼šAKShareå­—æ®µ -> æ ‡å‡†å­—æ®µ
        field_mapping = {
            # æ—¶é—´ç›¸å…³
            "æ—¶é—´": "datetime",
            "date": "datetime",
            "datetime": "datetime",
            # ä»·æ ¼ç›¸å…³
            "å¼€ç›˜": "open",
            "open": "open",
            "æœ€é«˜": "high",
            "high": "high",
            "æœ€ä½": "low",
            "low": "low",
            "æ”¶ç›˜": "close",
            "close": "close",
            "æœ€æ–°ä»·": "close",
            # æˆäº¤ç›¸å…³
            "æˆäº¤é‡": "volume",
            "volume": "volume",
            "æˆäº¤é¢": "amount",
            "amount": "amount",
            # å…¶ä»–
            "æ¶¨è·Œå¹…": "change_percent",
            "æ¶¨è·Œé¢": "change",
            "æŒ¯å¹…": "amplitude",
            "æ¢æ‰‹ç‡": "turnover_rate",
        }
        
        for _, r in df.iterrows():
            try:
                # æå–datetime
                datetime_val = None
                for dt_col in ["æ—¶é—´", "datetime", "date"]:
                    if dt_col in r.index or dt_col in r:
                        dt_val = r.get(dt_col)
                        if dt_val is not None and not pd.isna(dt_val):
                            try:
                                if isinstance(dt_val, pd.Timestamp):
                                    datetime_val = dt_val.strftime("%Y-%m-%d %H:%M:%S")
                                elif isinstance(dt_val, str):
                                    # å°è¯•è§£ææ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²
                                    datetime_val = pd.to_datetime(dt_val).strftime("%Y-%m-%d %H:%M:%S")
                                else:
                                    datetime_val = str(dt_val)
                                break
                            except Exception:
                                continue
                
                if not datetime_val:
                    skipped_count += 1
                    continue
                
                # æ„å»ºæ–‡æ¡£
                doc = {
                    "code": code_std,
                    "datetime": datetime_val,
                    "period": str(period),
                    "source": "akshare",
                }
                
                # æ˜ å°„å­—æ®µ
                for ak_field, std_field in field_mapping.items():
                    if ak_field in r.index or ak_field in r:
                        value = r.get(ak_field)
                        if value is not None and not pd.isna(value):
                            try:
                                # æ•°å€¼ç±»å‹è½¬æ¢
                                if std_field in ["open", "high", "low", "close", "volume", "amount", 
                                                "change_percent", "change", "amplitude", "turnover_rate"]:
                                    doc[std_field] = float(value)
                                else:
                                    doc[std_field] = value
                            except (ValueError, TypeError):
                                pass
                
                # ç§»é™¤Noneå€¼
                doc = {k: v for k, v in doc.items() if v is not None}
                
                # ä½¿ç”¨ (code, datetime, period) ä½œä¸ºå”¯ä¸€é”®
                ops.append(
                    UpdateOne(
                        {"code": code_std, "datetime": datetime_val, "period": period},
                        {"$set": doc},
                        upsert=True
                    )
                )
                valid_count += 1
                
            except Exception as e:
                skipped_count += 1
                logger.warning(f"âš ï¸ [åˆ†é’Ÿæ•°æ®ä¿å­˜] å¤„ç†è¡Œæ•°æ®å¤±è´¥: {e}")
                continue
        
        if not ops:
            logger.warning(f"âš ï¸ [åˆ†é’Ÿæ•°æ®ä¿å­˜] æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜ï¼ˆè·³è¿‡ {skipped_count} æ¡ï¼‰")
            return 0
        
        try:
            res = await self.col_minute.bulk_write(ops, ordered=False)
            upserted = res.upserted_count or 0
            modified = res.modified_count or 0
            total_saved = upserted + modified
            
            logger.info(f"ğŸ’¾ [åˆ†é’Ÿæ•°æ®ä¿å­˜] æ‰¹é‡å†™å…¥å®Œæˆ: æ–°å¢={upserted}, æ›´æ–°={modified}, æ€»è®¡={total_saved}, æœ‰æ•ˆ={valid_count}, è·³è¿‡={skipped_count}")
            
            return total_saved
        except Exception as e:
            logger.error(f"âŒ [åˆ†é’Ÿæ•°æ®ä¿å­˜] æ‰¹é‡å†™å…¥å¤±è´¥: {e}", exc_info=True)
            return 0

    async def save_cov_comparison(self, df: pd.DataFrame) -> int:
        """ä¿å­˜å¯è½¬å€ºæ¯”ä»·è¡¨æ•°æ®
        
        Args:
            df: å¯è½¬å€ºæ¯”ä»·è¡¨DataFrameï¼ˆæ¥è‡ªbond_cov_comparisonï¼‰
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        import logging
        logger = logging.getLogger("webapi")
        
        if df is None or df.empty:
            return 0
        
        ops = []
        timestamp = datetime.now().isoformat()
        
        for _, r in df.iterrows():
            code = str(r.get("è½¬å€ºä»£ç ") or r.get("å€ºåˆ¸ä»£ç ") or "").strip()
            if not code:
                continue
            
            from tradingagents.utils.instrument_validator import normalize_bond_code
            norm = normalize_bond_code(code)
            code_std = norm.get("code_std") or code
            
            # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨è½¬æ¢ä¸ºfloat
            def safe_float(value):
                """å®‰å…¨è½¬æ¢ä¸ºfloatï¼Œå¤„ç†NaNå’ŒNone"""
                if value is None or (isinstance(value, float) and pd.isna(value)):
                    return None
                try:
                    return float(value)
                except (ValueError, TypeError):
                    return None
            
            doc = {
                "code": code_std,
                "name": str(r.get("è½¬å€ºåç§°") or r.get("å€ºåˆ¸åç§°") or ""),
                "price": safe_float(r.get("è½¬å€ºæœ€æ–°ä»·")),
                "change_pct": safe_float(r.get("è½¬å€ºæ¶¨è·Œå¹…")),
                "stock_code": str(r.get("æ­£è‚¡ä»£ç ") or ""),
                "stock_name": str(r.get("æ­£è‚¡åç§°") or ""),
                "stock_price": safe_float(r.get("æ­£è‚¡æœ€æ–°ä»·")),
                "stock_change_pct": safe_float(r.get("æ­£è‚¡æ¶¨è·Œå¹…")),
                "convert_price": safe_float(r.get("è½¬è‚¡ä»·")),
                "convert_value": safe_float(r.get("è½¬è‚¡ä»·å€¼")),
                "convert_premium_rate": safe_float(r.get("è½¬è‚¡æº¢ä»·ç‡")),
                "pure_debt_premium_rate": safe_float(r.get("çº¯å€ºæº¢ä»·ç‡")),
                "put_trigger_price": safe_float(r.get("å›å”®è§¦å‘ä»·")),
                "redeem_trigger_price": safe_float(r.get("å¼ºèµè§¦å‘ä»·")),
                "maturity_redeem_price": safe_float(r.get("åˆ°æœŸèµå›ä»·")),
                "pure_debt_value": safe_float(r.get("çº¯å€ºä»·å€¼")),
                "start_convert_date": str(r.get("å¼€å§‹è½¬è‚¡æ—¥") or ""),
                "list_date": str(r.get("ä¸Šå¸‚æ—¥æœŸ") or ""),
                "apply_date": str(r.get("ç”³è´­æ—¥æœŸ") or ""),
                "timestamp": timestamp,
                "source": "akshare",
            }
            
            # ç§»é™¤Noneå€¼ï¼Œä½†ä¿ç•™0å€¼
            doc = {k: v for k, v in doc.items() if v is not None and v != ""}
            
            ops.append(UpdateOne(
                {"code": code_std},
                {"$set": doc},
                upsert=True
            ))
        
        if not ops:
            return 0
        
        try:
            res = await self.col_cb_comparison.bulk_write(ops, ordered=False)
            saved = (res.upserted_count or 0) + (res.modified_count or 0)
            logger.info(f"ğŸ’¾ [å¯è½¬å€ºæ¯”ä»·] ä¿å­˜ {saved} æ¡æ•°æ®")
            return saved
        except Exception as e:
            logger.error(f"âŒ [å¯è½¬å€ºæ¯”ä»·] ä¿å­˜å¤±è´¥: {e}", exc_info=True)
            return 0

    async def save_cov_value_analysis(self, code: str, df: pd.DataFrame) -> int:
        """ä¿å­˜å¯è½¬å€ºä»·å€¼åˆ†æå†å²æ•°æ®
        
        Args:
            code: å€ºåˆ¸ä»£ç 
            df: ä»·å€¼åˆ†æDataFrameï¼ˆæ¥è‡ªbond_zh_cov_value_analysisï¼‰
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        import logging
        logger = logging.getLogger("webapi")
        
        if df is None or df.empty:
            return 0
        
        from tradingagents.utils.instrument_validator import normalize_bond_code
        norm = normalize_bond_code(code)
        code_std = norm.get("code_std") or code
        
        # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨è½¬æ¢ä¸ºfloat
        def safe_float(value):
            """å®‰å…¨è½¬æ¢ä¸ºfloatï¼Œå¤„ç†NaNå’ŒNone"""
            if value is None or (isinstance(value, float) and pd.isna(value)):
                return None
            try:
                return float(value)
            except (ValueError, TypeError):
                return None
        
        ops = []
        for _, r in df.iterrows():
            date = str(r.get("æ—¥æœŸ") or "").strip()
            if not date:
                continue
            
            doc = {
                "code": code_std,
                "date": date,
                "close_price": safe_float(r.get("æ”¶ç›˜ä»·")),
                "pure_debt_value": safe_float(r.get("çº¯å€ºä»·å€¼")),
                "convert_value": safe_float(r.get("è½¬è‚¡ä»·å€¼")),
                "pure_debt_premium_rate": safe_float(r.get("çº¯å€ºæº¢ä»·ç‡")),
                "convert_premium_rate": safe_float(r.get("è½¬è‚¡æº¢ä»·ç‡")),
                "source": "akshare",
            }
            
            # ç§»é™¤Noneå€¼ï¼Œä½†ä¿ç•™0å€¼
            doc = {k: v for k, v in doc.items() if v is not None and v != ""}
            
            ops.append(UpdateOne(
                {"code": code_std, "date": date},
                {"$set": doc},
                upsert=True
            ))
        
        if not ops:
            return 0
        
        try:
            res = await self.col_cb_valuation.bulk_write(ops, ordered=False)
            saved = (res.upserted_count or 0) + (res.modified_count or 0) + (res.matched_count or 0)
            logger.info(f"ğŸ’¾ [å¯è½¬å€ºä»·å€¼] {code_std} ä¿å­˜ {saved} æ¡æ•°æ® (æ–°å¢={res.upserted_count}, æ›´æ–°={res.modified_count}, åŒ¹é…={res.matched_count})")
            return saved
        except Exception as e:
            logger.error(f"âŒ [å¯è½¬å€ºä»·å€¼] ä¿å­˜å¤±è´¥: {e}", exc_info=True)
            return 0

    async def save_spot_deals(self, df: pd.DataFrame) -> int:
        """ä¿å­˜ç°åˆ¸å¸‚åœºæˆäº¤è¡Œæƒ…
        
        Args:
            df: æˆäº¤è¡Œæƒ…DataFrameï¼ˆæ¥è‡ªbond_spot_dealï¼‰
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        import logging
        logger = logging.getLogger("webapi")
        
        if df is None or df.empty:
            return 0
        
        # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨è½¬æ¢ä¸ºfloat
        def safe_float(value):
            """å®‰å…¨è½¬æ¢ä¸ºfloatï¼Œå¤„ç†NaNå’ŒNone"""
            if value is None or (isinstance(value, float) and pd.isna(value)):
                return None
            try:
                return float(value)
            except (ValueError, TypeError):
                return None
        
        ops = []
        today = datetime.now().strftime("%Y-%m-%d")
        timestamp = datetime.now().isoformat()
        
        logger.info(f"[ç°åˆ¸æˆäº¤] å¼€å§‹å¤„ç† {len(df)} æ¡æ•°æ®")
        logger.debug(f"[ç°åˆ¸æˆäº¤] DataFrameåˆ—: {df.columns.tolist()}")
        
        skipped = 0
        for idx, r in df.iterrows():
            name = str(r.get("å€ºåˆ¸ç®€ç§°") or "").strip()
            if not name:
                skipped += 1
                logger.debug(f"[ç°åˆ¸æˆäº¤] è¡Œ{idx}: å€ºåˆ¸ç®€ç§°ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            doc = {
                "bond_name": name,
                "deal_price": safe_float(r.get("æˆäº¤å‡€ä»·")),
                "latest_yield": safe_float(r.get("æœ€æ–°æ”¶ç›Šç‡")),
                "change": safe_float(r.get("æ¶¨è·Œ")),
                "weighted_yield": safe_float(r.get("åŠ æƒæ”¶ç›Šç‡")),
                "volume": safe_float(r.get("äº¤æ˜“é‡")),
                "date": today,
                "timestamp": timestamp,
                "source": "akshare",
            }
            
            # ç§»é™¤Noneå€¼ï¼Œä½†ä¿ç•™0å€¼
            doc = {k: v for k, v in doc.items() if v is not None and v != ""}
            
            # ä½¿ç”¨bond_nameå’Œdateä½œä¸ºå”¯ä¸€é”®ï¼ˆåŒä¸€å¤©åŒä¸€å€ºåˆ¸åªä¿ç•™æœ€æ–°æ•°æ®ï¼‰
            ops.append(UpdateOne(
                {"bond_name": name, "date": today},
                {"$set": doc},
                upsert=True
            ))
        
        if not ops:
            return 0
        
        try:
            res = await self.col_spot_deals.bulk_write(ops, ordered=False)
            upserted = res.upserted_count or 0
            modified = res.modified_count or 0
            matched = res.matched_count or 0
            saved = upserted + modified + matched
            logger.info(f"[ç°åˆ¸æˆäº¤] ä¿å­˜å®Œæˆ: æ–°å¢={upserted}, æ›´æ–°={modified}, åŒ¹é…={matched}, æ€»è®¡={saved}, è·³è¿‡={skipped}")
            return saved
        except Exception as e:
            # å¤„ç†BulkWriteError - éƒ¨åˆ†æˆåŠŸä¹Ÿå¯ä»¥æå–ç»“æœ
            from pymongo.errors import BulkWriteError
            if isinstance(e, BulkWriteError):
                result = e.details
                upserted = result.get('nUpserted', 0)
                modified = result.get('nModified', 0)
                matched = result.get('nMatched', 0)
                saved = upserted + modified + matched
                
                write_errors = result.get('writeErrors', [])
                logger.warning(f"[ç°åˆ¸æˆäº¤] æ‰¹é‡å†™å…¥éƒ¨åˆ†æˆåŠŸ: æ–°å¢={upserted}, æ›´æ–°={modified}, åŒ¹é…={matched}, æ€»è®¡={saved}, é”™è¯¯æ•°={len(write_errors)}")
                
                if write_errors:
                    for i, err in enumerate(write_errors[:3]):
                        logger.warning(f"[ç°åˆ¸æˆäº¤] é”™è¯¯ç¤ºä¾‹ {i+1}: {err.get('errmsg', 'Unknown error')}")
                
                if saved > 0:
                    return saved
            
            logger.error(f"[ç°åˆ¸æˆäº¤] ä¿å­˜å¤±è´¥: {e}", exc_info=True)
            return 0

    async def query_cov_comparison(
        self,
        q: Optional[str] = None,
        keyword: Optional[str] = None,  # æ·»åŠ keywordå‚æ•°ä½œä¸ºqçš„åˆ«å
        sort_by: Optional[str] = None,
        sort_dir: str = "asc",
        page: int = 1,
        page_size: int = 50,
        min_premium: Optional[float] = None,
        max_premium: Optional[float] = None,
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢å¯è½¬å€ºæ¯”ä»·è¡¨
        
        Args:
            q: æœç´¢å…³é”®è¯ï¼ˆä»£ç æˆ–åç§°ï¼‰
            keyword: æœç´¢å…³é”®è¯åˆ«åï¼ˆä¸qç­‰æ•ˆï¼‰
            sort_by: æ’åºå­—æ®µ
            sort_dir: æ’åºæ–¹å‘ï¼ˆasc/descï¼‰
            page: é¡µç 
            page_size: æ¯é¡µæ•°é‡
            min_premium: æœ€å°è½¬è‚¡æº¢ä»·ç‡
            max_premium: æœ€å¤§è½¬è‚¡æº¢ä»·ç‡
            
        Returns:
            æŸ¥è¯¢ç»“æœå­—å…¸
        """
        import logging
        logger = logging.getLogger("webapi")
        
        # keywordæ˜¯qçš„åˆ«å
        if keyword and not q:
            q = keyword
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        filt: Dict[str, Any] = {}
        
        # å…³é”®è¯æœç´¢
        if q:
            q_regex = {"$regex": q, "$options": "i"}
            filt["$or"] = [{"code": q_regex}, {"name": q_regex}]
        
        # æº¢ä»·ç‡èŒƒå›´è¿‡æ»¤ï¼ˆåœ¨æ•°æ®åº“å±‚è¿‡æ»¤ï¼Œæå‡æ€§èƒ½ï¼‰
        if min_premium is not None or max_premium is not None:
            premium_filter = {}
            if min_premium is not None:
                premium_filter["$gte"] = min_premium
            if max_premium is not None:
                premium_filter["$lte"] = max_premium
            if premium_filter:
                filt["convert_premium_rate"] = premium_filter
        
        logger.debug(f"ğŸ” [å¯è½¬å€ºæŸ¥è¯¢] è¿‡æ»¤æ¡ä»¶: {filt}")
        
        # è®¡æ•°
        total = await self.col_cb_comparison.count_documents(filt)
        if total == 0:
            return {"total": 0, "items": []}
        
        # åˆ†é¡µå’Œæ’åº
        skip = max(0, (page - 1) * page_size)
        field = (sort_by or "code").lower()
        direc = 1 if str(sort_dir).lower() != "desc" else -1
        
        cursor = self.col_cb_comparison.find(filt).sort([(field, direc)]).skip(skip).limit(page_size)
        items = []
        async for doc in cursor:
            if "_id" in doc:
                doc.pop("_id", None)
            items.append(doc)
        
        logger.debug(f"ğŸ“Š [å¯è½¬å€ºæŸ¥è¯¢] è¿”å› {len(items)}/{total} æ¡æ•°æ®")
        
        return {"total": total, "items": items}

    async def query_cov_value_analysis(
        self,
        code: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        limit: int = 100,
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢å¯è½¬å€ºä»·å€¼åˆ†æå†å²æ•°æ®"""
        from tradingagents.utils.instrument_validator import normalize_bond_code
        norm = normalize_bond_code(code)
        code_std = norm.get("code_std") or code
        
        filt = {"code": code_std}
        if start_date:
            filt["date"] = {"$gte": start_date}
        if end_date:
            if "date" in filt:
                filt["date"]["$lte"] = end_date
            else:
                filt["date"] = {"$lte": end_date}
        
        cursor = self.col_cb_valuation.find(filt).sort("date", -1).limit(limit)
        items = []
        async for doc in cursor:
            if "_id" in doc:
                doc.pop("_id", None)
            items.append(doc)
        
        return {"total": len(items), "items": items}
    
    async def query_spot_deals(
        self,
        limit: int = 100,
        skip: int = 0
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢ç°åˆ¸æˆäº¤è¡Œæƒ…
        
        Args:
            limit: è¿”å›æ•°é‡é™åˆ¶
            skip: è·³è¿‡æ•°é‡
            
        Returns:
            æŸ¥è¯¢ç»“æœå­—å…¸ {"total": int, "items": list}
        """
        import logging
        logger = logging.getLogger("webapi")
        
        try:
            total = await self.col_spot_deals.count_documents({})
            
            if total == 0:
                return {"total": 0, "items": []}
            
            cursor = self.col_spot_deals.find({}).skip(skip).limit(limit)
            items = []
            async for doc in cursor:
                if "_id" in doc:
                    doc.pop("_id", None)
                items.append(doc)
            
            logger.debug(f"ğŸ“Š [ç°åˆ¸æˆäº¤] æŸ¥è¯¢æˆåŠŸ: {len(items)}/{total}")
            return {"total": total, "items": items}
        except Exception as e:
            logger.error(f"âŒ [ç°åˆ¸æˆäº¤] æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
            return {"total": 0, "items": []}
    
    async def query_spot_quotes(
        self,
        category: Optional[str] = None,
        bond_name: Optional[str] = None,
        limit: int = 100,
        skip: int = 0
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢ç°åˆ¸åšå¸‚æŠ¥ä»·
        
        Args:
            category: åˆ†ç±»è¿‡æ»¤
            bond_name: å€ºåˆ¸åç§°å…³é”®è¯
            limit: è¿”å›æ•°é‡é™åˆ¶
            skip: è·³è¿‡æ•°é‡
            
        Returns:
            æŸ¥è¯¢ç»“æœå­—å…¸ {"total": int, "items": list}
        """
        import logging
        logger = logging.getLogger("webapi")
        
        try:
            filt: Dict[str, Any] = {}
            
            if category:
                filt["category"] = category
            
            if bond_name:
                filt["å€ºåˆ¸ç®€ç§°"] = {"$regex": bond_name, "$options": "i"}
            
            total = await self.col_spot.count_documents(filt)
            
            if total == 0:
                return {"total": 0, "items": []}
            
            cursor = self.col_spot.find(filt).skip(skip).limit(limit)
            items = []
            async for doc in cursor:
                if "_id" in doc:
                    doc.pop("_id", None)
                items.append(doc)
            
            logger.debug(f"ğŸ“Š [ç°åˆ¸æŠ¥ä»·] æŸ¥è¯¢æˆåŠŸ: {len(items)}/{total}")
            return {"total": total, "items": items}
        except Exception as e:
            logger.error(f"âŒ [ç°åˆ¸æŠ¥ä»·] æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
            return {"total": 0, "items": []}
    
    async def query_historical_data(
        self,
        code: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        limit: int = 100
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢å€ºåˆ¸å†å²è¡Œæƒ…æ•°æ®
        
        Args:
            code: å€ºåˆ¸ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            limit: è¿”å›æ•°é‡é™åˆ¶
            
        Returns:
            æŸ¥è¯¢ç»“æœå­—å…¸ {"total": int, "items": list}
        """
        import logging
        logger = logging.getLogger("webapi")
        
        try:
            from tradingagents.utils.instrument_validator import normalize_bond_code
            norm = normalize_bond_code(code)
            code_std = norm.get("code_std") or code
            
            filt: Dict[str, Any] = {"code": code_std}
            
            if start_date or end_date:
                date_filter = {}
                if start_date:
                    date_filter["$gte"] = start_date
                if end_date:
                    date_filter["$lte"] = end_date
                filt["date"] = date_filter
            
            total = await self.col_daily.count_documents(filt)
            
            if total == 0:
                return {"total": 0, "items": []}
            
            cursor = self.col_daily.find(filt).sort("date", -1).limit(limit)
            items = []
            async for doc in cursor:
                if "_id" in doc:
                    doc.pop("_id", None)
                items.append(doc)
            
            logger.debug(f"ğŸ“Š [æ—¥çº¿æ•°æ®] æŸ¥è¯¢æˆåŠŸ: {len(items)}/{total}")
            return {"total": total, "items": items}
        except Exception as e:
            logger.error(f"âŒ [æ—¥çº¿æ•°æ®] æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
            return {"total": 0, "items": []}
    
    async def save_historical_data(self, df: pd.DataFrame, code: str) -> int:
        """ä¿å­˜å€ºåˆ¸å†å²æ•°æ® (save_bond_dailyçš„åˆ«å)
        
        Args:
            df: æ•°æ®DataFrame
            code: å€ºåˆ¸ä»£ç 
            
        Returns:
            ä¿å­˜æ•°é‡
        """
        return await self.save_bond_daily(code, df)

"""
åŸºé‡‘æ•°æ®æœåŠ¡
è´Ÿè´£ä»akshareè·å–åŸºé‡‘æ•°æ®å¹¶å­˜å‚¨åˆ°MongoDB
"""
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import pandas as pd
from motor.motor_asyncio import AsyncIOMotorDatabase
from pymongo import UpdateOne
import asyncio

logger = logging.getLogger("webapi")


class FundDataService:
    """åŸºé‡‘æ•°æ®æœåŠ¡ç±»"""
    
    def __init__(self, db: AsyncIOMotorDatabase):
        self.db = db
        self.col_fund_name_em = db.get_collection("fund_name_em")
        self.col_fund_basic_info = db.get_collection("fund_basic_info")
    
    async def save_fund_name_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ä¿å­˜åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®åˆ°MongoDB
        
        Args:
            df: åŒ…å«åŸºé‡‘åŸºæœ¬ä¿¡æ¯çš„DataFrame
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘æ•°æ®...")
            
            # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹500æ¡
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ“¦ å°†åˆ† {total_batches} æ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} æ¡")
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"ğŸ“ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ï¼Œè®°å½•èŒƒå›´: {start_idx + 1}-{end_idx}")
                
                # æ„å»ºæ‰¹é‡æ“ä½œ
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_name_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'endpoint': 'fund_name_em'},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # æ‰§è¡Œæ‰¹é‡å†™å…¥
                if ops:
                    result = await self.col_fund_name_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"âœ… ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹å†™å…¥å®Œæˆ: "
                        f"æ–°å¢={result.upserted_count}, æ›´æ–°={result.matched_count}, "
                        f"æœ¬æ‰¹ä¿å­˜={batch_saved}, ç´¯è®¡={total_saved}/{total_count}"
                    )
                    
                    # è°ƒç”¨è¿›åº¦å›è°ƒï¼ˆå¦‚æœæä¾›ï¼‰
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_name_em_data(self) -> int:
        """
        æ¸…ç©ºåŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®
        
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_name_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_name_em_stats(self) -> Dict[str, Any]:
        """
        è·å–åŸºé‡‘åŸºæœ¬ä¿¡æ¯ç»Ÿè®¡
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_name_em.count_documents({})
            
            # æŒ‰åŸºé‡‘ç±»å‹ç»Ÿè®¡
            pipeline = [
                {
                    '$group': {
                        '_id': '$åŸºé‡‘ç±»å‹',
                        'count': {'$sum': 1}
                    }
                },
                {
                    '$sort': {'count': -1}
                }
            ]
            
            type_stats = []
            async for doc in self.col_fund_name_em.aggregate(pipeline):
                type_stats.append({
                    'type': doc['_id'],
                    'count': doc['count']
                })
            
            return {
                'total_count': total_count,
                'type_stats': type_stats
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘åŸºæœ¬ä¿¡æ¯ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_basic_info_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ä¿å­˜åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®åˆ°fund_basic_infoé›†åˆ
        ä½¿ç”¨ç›¸åŒçš„fund_name_emæ•°æ®æº
        
        Args:
            df: åŒ…å«åŸºé‡‘åŸºæœ¬ä¿¡æ¯çš„DataFrame
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘æ•°æ®åˆ°fund_basic_infoé›†åˆ...")
            
            # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹500æ¡
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ“¦ å°†åˆ† {total_batches} æ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} æ¡")
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"ğŸ“ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ï¼Œè®°å½•èŒƒå›´: {start_idx + 1}-{end_idx}")
                
                # æ„å»ºæ‰¹é‡æ“ä½œ
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_name_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'endpoint': 'fund_name_em'},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # æ‰§è¡Œæ‰¹é‡å†™å…¥
                if ops:
                    result = await self.col_fund_basic_info.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"âœ… ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹å†™å…¥fund_basic_infoå®Œæˆ: "
                        f"æ–°å¢={result.upserted_count}, æ›´æ–°={result.matched_count}, "
                        f"æœ¬æ‰¹ä¿å­˜={batch_saved}, ç´¯è®¡={total_saved}/{total_count}"
                    )
                    
                    # è°ƒç”¨è¿›åº¦å›è°ƒï¼ˆå¦‚æœæä¾›ï¼‰
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ®åˆ°fund_basic_info ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥fund_basic_infoå®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®åˆ°fund_basic_infoå¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_basic_info_data(self) -> int:
        """
        æ¸…ç©ºfund_basic_infoåŸºé‡‘æ•°æ®
        
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_basic_info.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©ºfund_basic_info {deleted_count} æ¡æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºfund_basic_infoæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_basic_info_stats(self) -> Dict[str, Any]:
        """
        è·å–fund_basic_infoé›†åˆç»Ÿè®¡
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_basic_info.count_documents({})
            
            # æŒ‰åŸºé‡‘ç±»å‹ç»Ÿè®¡
            pipeline = [
                {
                    '$group': {
                        '_id': '$åŸºé‡‘ç±»å‹',
                        'count': {'$sum': 1}
                    }
                },
                {
                    '$sort': {'count': -1}
                }
            ]
            
            type_stats = []
            async for doc in self.col_fund_basic_info.aggregate(pipeline):
                type_stats.append({
                    'type': doc['_id'],
                    'count': doc['count']
                })
            
            return {
                'total_count': total_count,
                'type_stats': type_stats
            }
        except Exception as e:
            logger.error(f"è·å–fund_basic_infoç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

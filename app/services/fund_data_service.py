"""
åŸºé‡‘æ•°æ®æœåŠ¡
è´Ÿè´£ä»akshareè·å–åŸºé‡‘æ•°æ®å¹¶å­˜å‚¨åˆ°MongoDB
"""
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, date
import pandas as pd
from motor.motor_asyncio import AsyncIOMotorDatabase
from pymongo import UpdateOne
import asyncio
import io

logger = logging.getLogger("webapi")


class FundDataService:
    """åŸºé‡‘æ•°æ®æœåŠ¡ç±»"""
    
    def __init__(self, db: AsyncIOMotorDatabase):
        self.db = db
        self.col_fund_name_em = db.get_collection("fund_name_em")
        self.col_fund_basic_info = db.get_collection("fund_basic_info")
        self.col_fund_info_index = db.get_collection("fund_info_index_em")
        self.col_fund_purchase_status = db.get_collection("fund_purchase_status")
        self.col_fund_etf_spot = db.get_collection("fund_etf_spot_em")
        self.col_fund_etf_spot_ths = db.get_collection("fund_etf_spot_ths")
        self.col_fund_lof_spot = db.get_collection("fund_lof_spot_em")
        self.col_fund_spot_sina = db.get_collection("fund_spot_sina")
        self.col_fund_etf_hist_min_em = db.get_collection("fund_etf_hist_min_em")
        self.col_fund_lof_hist_min_em = db.get_collection("fund_lof_hist_min_em")
        self.col_fund_etf_hist_em = db.get_collection("fund_etf_hist_em")
        self.col_fund_lof_hist_em = db.get_collection("fund_lof_hist_em")
        self.col_fund_hist_sina = db.get_collection("fund_hist_sina")
        self.col_fund_open_fund_daily_em = db.get_collection("fund_open_fund_daily_em")
        self.col_fund_open_fund_info_em = db.get_collection("fund_open_fund_info_em")
        self.col_fund_money_fund_daily_em = db.get_collection("fund_money_fund_daily_em")
        self.col_fund_money_fund_info_em = db.get_collection("fund_money_fund_info_em")
        self.col_fund_financial_fund_daily_em = db.get_collection("fund_financial_fund_daily_em")
        self.col_fund_financial_fund_info_em = db.get_collection("fund_financial_fund_info_em")
        self.col_fund_graded_fund_daily_em = db.get_collection("fund_graded_fund_daily_em")
        self.col_fund_graded_fund_info_em = db.get_collection("fund_graded_fund_info_em")
        self.col_fund_etf_fund_daily_em = db.get_collection("fund_etf_fund_daily_em")
        self.col_fund_hk_hist_em = db.get_collection("fund_hk_hist_em")
        self.col_fund_etf_fund_info_em = db.get_collection("fund_etf_fund_info_em")
        self.col_fund_etf_dividend_sina = db.get_collection("fund_etf_dividend_sina")
        self.col_fund_fh_em = db.get_collection("fund_fh_em")
        self.col_fund_cf_em = db.get_collection("fund_cf_em")
        self.col_fund_fh_rank_em = db.get_collection("fund_fh_rank_em")
        self.col_fund_open_fund_rank_em = db.get_collection("fund_open_fund_rank_em")
        self.col_fund_exchange_rank_em = db.get_collection("fund_exchange_rank_em")
        self.col_fund_money_rank_em = db.get_collection("fund_money_rank_em")
        self.col_fund_lcx_rank_em = db.get_collection("fund_lcx_rank_em")
        self.col_fund_hk_rank_em = db.get_collection("fund_hk_rank_em")
        self.col_fund_individual_achievement_xq = db.get_collection("fund_individual_achievement_xq")
        self.col_fund_value_estimation_em = db.get_collection("fund_value_estimation_em")
        self.col_fund_individual_analysis_xq = db.get_collection("fund_individual_analysis_xq")
        self.col_fund_individual_profit_probability_xq = db.get_collection("fund_individual_profit_probability_xq")
        self.col_fund_individual_detail_hold_xq = db.get_collection("fund_individual_detail_hold_xq")
        self.col_fund_overview_em = db.get_collection("fund_overview_em")
        self.col_fund_fee_em = db.get_collection("fund_fee_em")
        self.col_fund_individual_detail_info_xq = db.get_collection("fund_individual_detail_info_xq")
        self.col_fund_portfolio_hold_em = db.get_collection("fund_portfolio_hold_em")
        self.col_fund_portfolio_bond_hold_em = db.get_collection("fund_portfolio_bond_hold_em")
        self.col_fund_portfolio_change_em = db.get_collection("fund_portfolio_change_em")
        self.col_fund_rating_all_em = db.get_collection("fund_rating_all_em")
        self.col_fund_rating_sh_em = db.get_collection("fund_rating_sh_em")
        self.col_fund_rating_zs_em = db.get_collection("fund_rating_zs_em")
        self.col_fund_rating_ja_em = db.get_collection("fund_rating_ja_em")
        self.col_fund_manager_em = db.get_collection("fund_manager_em")
        self.col_fund_new_found_em = db.get_collection("fund_new_found_em")
        self.col_fund_scale_open_sina = db.get_collection("fund_scale_open_sina")
        self.col_fund_scale_close_sina = db.get_collection("fund_scale_close_sina")
        self.col_fund_scale_structured_sina = db.get_collection("fund_scale_structured_sina")
        self.col_fund_aum_em = db.get_collection("fund_aum_em")
        self.col_fund_aum_trend_em = db.get_collection("fund_aum_trend_em")
        self.col_fund_aum_hist_em = db.get_collection("fund_aum_hist_em")
        self.col_reits_realtime_em = db.get_collection("reits_realtime_em")
        self.col_reits_hist_em = db.get_collection("reits_hist_em")
        self.col_fund_report_stock_cninfo = db.get_collection("fund_report_stock_cninfo")
        self.col_fund_report_industry_allocation_cninfo = db.get_collection("fund_report_industry_allocation_cninfo")
        self.col_fund_report_asset_allocation_cninfo = db.get_collection("fund_report_asset_allocation_cninfo")
        self.col_fund_scale_change_em = db.get_collection("fund_scale_change_em")
        self.col_fund_hold_structure_em = db.get_collection("fund_hold_structure_em")
        self.col_fund_stock_position_lg = db.get_collection("fund_stock_position_lg")
        self.col_fund_balance_position_lg = db.get_collection("fund_balance_position_lg")
        self.col_fund_linghuo_position_lg = db.get_collection("fund_linghuo_position_lg")
        self.col_fund_announcement_dividend_em = db.get_collection("fund_announcement_dividend_em")
        self.col_fund_announcement_report_em = db.get_collection("fund_announcement_report_em")
        self.col_fund_announcement_personnel_em = db.get_collection("fund_announcement_personnel_em")
    
    async def save_fund_name_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ä¿å­˜åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®åˆ°MongoDB
        
        Args:
            df: åŒ…å«åŸºé‡‘åŸºæœ¬ä¿¡æ¯çš„DataFrame
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            # æ¸…ç†æ— æ•ˆçš„æµ®ç‚¹æ•°å€¼ï¼ˆNaN, Infinityç­‰ï¼‰ï¼Œé˜²æ­¢JSONåºåˆ—åŒ–é”™è¯¯
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘æ•°æ®...")
            
            # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹500æ¡
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ“¦ å°†åˆ† {total_batches} æ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} æ¡")
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"ğŸ“ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ï¼Œè®°å½•èŒƒå›´: {start_idx + 1}-{end_idx}")
                
                # æ„å»ºæ‰¹é‡æ“ä½œ
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # æ¸…ç†NaN/Infinityå€¼ï¼ˆto_dict()å¯èƒ½ä¼šé‡æ–°å¼•å…¥NaNï¼‰
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        # è½¬æ¢ datetime.date å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        # è½¬æ¢ datetime.datetime å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_name_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'endpoint': 'fund_name_em'},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # æ‰§è¡Œæ‰¹é‡å†™å…¥
                if ops:
                    result = await self.col_fund_name_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"âœ… ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹å†™å…¥å®Œæˆ: "
                        f"æ–°å¢={result.upserted_count}, æ›´æ–°={result.matched_count}, "
                        f"æœ¬æ‰¹ä¿å­˜={batch_saved}, ç´¯è®¡={total_saved}/{total_count}"
                    )
                    
                    # è°ƒç”¨è¿›åº¦å›è°ƒï¼ˆå¦‚æœæä¾›ï¼‰
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_name_em_data(self) -> int:
        """
        æ¸…ç©ºåŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®
        
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_name_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_name_em_stats(self) -> Dict[str, Any]:
        """
        è·å–åŸºé‡‘åŸºæœ¬ä¿¡æ¯ç»Ÿè®¡
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_name_em.count_documents({})
            
            # æŒ‰åŸºé‡‘ç±»å‹ç»Ÿè®¡
            pipeline = [
                {
                    '$group': {
                        '_id': '$åŸºé‡‘ç±»å‹',
                        'count': {'$sum': 1}
                    }
                },
                {
                    '$sort': {'count': -1}
                }
            ]
            
            type_stats = []
            async for doc in self.col_fund_name_em.aggregate(pipeline):
                type_stats.append({
                    'type': doc['_id'],
                    'count': doc['count']
                })
            
            return {
                'total_count': total_count,
                'type_stats': type_stats
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘åŸºæœ¬ä¿¡æ¯ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_basic_info_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ä¿å­˜åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®åˆ°fund_basic_infoé›†åˆ
        ä½¿ç”¨ fund_individual_basic_info_xq æ•°æ®æº
        
        Args:
            df: åŒ…å«åŸºé‡‘åŸºæœ¬ä¿¡æ¯çš„DataFrame
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            # æ¸…ç†æ— æ•ˆçš„æµ®ç‚¹æ•°å€¼ï¼ˆNaN, Infinityç­‰ï¼‰ï¼Œé˜²æ­¢JSONåºåˆ—åŒ–é”™è¯¯
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘æ•°æ®åˆ°fund_basic_infoé›†åˆ...")
            
            # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹500æ¡
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ“¦ å°†åˆ† {total_batches} æ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} æ¡")
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"ğŸ“ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ï¼Œè®°å½•èŒƒå›´: {start_idx + 1}-{end_idx}")
                
                # æ„å»ºæ‰¹é‡æ“ä½œ
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # æ¸…ç†NaN/Infinityå€¼ï¼ˆto_dict()å¯èƒ½ä¼šé‡æ–°å¼•å…¥NaNï¼‰
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        # è½¬æ¢ datetime.date å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        # è½¬æ¢ datetime.datetime å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_individual_basic_info_xq'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    ops.append(
                        UpdateOne(
                            {'code': fund_code},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # æ‰§è¡Œæ‰¹é‡å†™å…¥
                if ops:
                    result = await self.col_fund_basic_info.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"âœ… ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹å†™å…¥fund_basic_infoå®Œæˆ: "
                        f"æ–°å¢={result.upserted_count}, æ›´æ–°={result.matched_count}, "
                        f"æœ¬æ‰¹ä¿å­˜={batch_saved}, ç´¯è®¡={total_saved}/{total_count}"
                    )
                    
                    # è°ƒç”¨è¿›åº¦å›è°ƒï¼ˆå¦‚æœæä¾›ï¼‰
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ®åˆ°fund_basic_info ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥fund_basic_infoå®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®åˆ°fund_basic_infoå¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_basic_info_data(self) -> int:
        """
        æ¸…ç©ºfund_basic_infoåŸºé‡‘æ•°æ®
        
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_basic_info.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©ºfund_basic_info {deleted_count} æ¡æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºfund_basic_infoæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_basic_info_stats(self) -> Dict[str, Any]:
        """
        è·å–fund_basic_infoé›†åˆç»Ÿè®¡
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_basic_info.count_documents({})
            
            # æŒ‰åŸºé‡‘ç±»å‹ç»Ÿè®¡
            pipeline = [
                {
                    '$group': {
                        '_id': '$åŸºé‡‘ç±»å‹',
                        'count': {'$sum': 1}
                    }
                },
                {
                    '$sort': {'count': -1}
                }
            ]
            
            type_stats = []
            async for doc in self.col_fund_basic_info.aggregate(pipeline):
                type_stats.append({
                    'type': doc['_id'],
                    'count': doc['count']
                })
            
            return {
                'total_count': total_count,
                'type_stats': type_stats
            }
        except Exception as e:
            logger.error(f"è·å–fund_basic_infoç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_info_index_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜æŒ‡æ•°å‹åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®åˆ° fund_info_index_em é›†åˆã€‚

        ä½¿ç”¨ akshare fund_info_index_em æ¥å£æ•°æ®ï¼Œ
        ä»¥ åŸºé‡‘ä»£ç  + æ—¥æœŸ ä½œä¸ºå”¯ä¸€æ ‡è¯†ã€‚
        """
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰æŒ‡æ•°å‹åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®éœ€è¦ä¿å­˜")
            return 0

        try:
            # æ¸…ç†æ— æ•ˆçš„æµ®ç‚¹æ•°å€¼ï¼ˆNaN, Infinityç­‰ï¼‰ï¼Œé˜²æ­¢JSONåºåˆ—åŒ–é”™è¯¯
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)  # æ›¿æ¢æ— ç©·å¤§ä¸ºNone
            df = df.where(pd.notna(df), None)  # æ›¿æ¢NaNä¸ºNone
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡æŒ‡æ•°å‹åŸºé‡‘æ•°æ®åˆ° fund_info_index_em é›†åˆ...")

            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size

            logger.info(f"ğŸ“¦ å°†åˆ† {total_batches} æ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} æ¡")

            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]

                logger.info(f"ğŸ“ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ï¼Œè®°å½•èŒƒå›´: {start_idx + 1}-{end_idx}")

                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # å†æ¬¡æ¸…ç†NaN/Infinityå€¼ï¼ˆto_dict()å¯èƒ½ä¼šé‡æ–°å¼•å…¥NaNï¼‰
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        # è½¬æ¢ datetime.date å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        # è½¬æ¢ datetime.datetime å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')

                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', '')).strip()
                    date_str = str(doc.get('æ—¥æœŸ', '')).strip()
                    tracking_target = str(doc.get('è·Ÿè¸ªæ ‡çš„', '')).strip()
                    
                    if not fund_code or not date_str or not tracking_target:
                        continue

                    # å…ƒæ•°æ®
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_info_index_em'
                    doc['updated_at'] = datetime.now().isoformat()

                    # ä½¿ç”¨ æ—¥æœŸ + åŸºé‡‘ä»£ç  + è·Ÿè¸ªæ ‡çš„ ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    ops.append(
                        UpdateOne(
                            {
                                'æ—¥æœŸ': date_str,
                                'code': fund_code,
                                'è·Ÿè¸ªæ ‡çš„': tracking_target
                            },
                            {'$set': doc},
                            upsert=True
                        )
                    )

                if ops:
                    result = await self.col_fund_info_index.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved

                    logger.info(
                        f"âœ… ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹å†™å…¥ fund_info_index_em å®Œæˆ: "
                        f"æ–°å¢={result.upserted_count}, æ›´æ–°={result.matched_count}, "
                        f"æœ¬æ‰¹ä¿å­˜={batch_saved}, ç´¯è®¡={total_saved}/{total_count}"
                    )

                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ®åˆ° fund_info_index_em ({progress}%)"
                        )

            logger.info(
                f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥ fund_info_index_em å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡æŒ‡æ•°å‹åŸºé‡‘æ•°æ®"
            )
            return total_saved
        except Exception as e:
            logger.error(f"ä¿å­˜æŒ‡æ•°å‹åŸºé‡‘åŸºæœ¬ä¿¡æ¯æ•°æ®åˆ° fund_info_index_em å¤±è´¥: {e}", exc_info=True)
            raise

    async def clear_fund_info_index_data(self) -> int:
        """æ¸…ç©º fund_info_index_em æŒ‡æ•°å‹åŸºé‡‘æ•°æ®"""
        try:
            result = await self.col_fund_info_index.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º fund_info_index_em {deleted_count} æ¡æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©º fund_info_index_em æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_fund_info_index_stats(self) -> Dict[str, Any]:
        """è·å– fund_info_index_em é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            total_count = await self.col_fund_info_index.count_documents({})

            # æŒ‰è·Ÿè¸ªæ ‡çš„ç»Ÿè®¡
            pipeline_type = [
                {
                    '$group': {
                        '_id': '$è·Ÿè¸ªæ ‡çš„',
                        'count': {'$sum': 1}
                    }
                },
                {
                    '$sort': {'count': -1}
                }
            ]

            type_stats: List[Dict[str, Any]] = []
            async for doc in self.col_fund_info_index.aggregate(pipeline_type):
                type_stats.append({
                    'type': doc['_id'],
                    'count': doc['count']
                })

            # è®¡ç®—æœ€æ—©å’Œæœ€æ™šæ—¥æœŸ
            earliest_date = None
            latest_date = None
            pipeline_date = [
                {
                    '$group': {
                        '_id': None,
                        'earliest': {'$min': '$æ—¥æœŸ'},
                        'latest': {'$max': '$æ—¥æœŸ'}
                    }
                }
            ]

            async for doc in self.col_fund_info_index.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')

            return {
                'total_count': total_count,
                'type_stats': type_stats,
                'earliest_date': earliest_date,
                'latest_date': latest_date
            }
        except Exception as e:
            logger.error(f"è·å– fund_info_index_em ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def import_data_from_file(self, collection_name: str, content: bytes, filename: str) -> Dict[str, Any]:
        """ä»æ–‡ä»¶å¯¼å…¥æ•°æ®"""
        try:
            if filename.endswith('.csv'):
                df = pd.read_csv(io.BytesIO(content))
            else:
                df = pd.read_excel(io.BytesIO(content))
            
            if df.empty:
                return {"imported_count": 0, "message": "æ–‡ä»¶ä¸ºç©º"}
                
            count = 0
            if collection_name == 'fund_name_em':
                count = await self.save_fund_name_em_data(df)
            elif collection_name == 'fund_basic_info':
                count = await self.save_fund_basic_info_data(df)
            elif collection_name == 'fund_info_index_em':
                count = await self.save_fund_info_index_data(df)
            elif collection_name == 'fund_purchase_status':
                count = await self.save_fund_purchase_status_data(df)
            elif collection_name == 'fund_etf_spot_em':
                count = await self.save_fund_etf_spot_data(df)
            elif collection_name == 'fund_etf_spot_ths':
                count = await self.save_fund_etf_spot_ths_data(df)
            elif collection_name == 'fund_lof_spot_em':
                count = await self.save_fund_lof_spot_data(df)
            elif collection_name == 'fund_spot_sina':
                count = await self.save_fund_spot_sina_data(df)
            elif collection_name == 'fund_etf_hist_min_em':
                count = await self.save_fund_etf_hist_min_data(df)
            elif collection_name == 'fund_etf_hist_em':
                count = await self.save_fund_etf_hist_data(df)
            elif collection_name == 'fund_lof_hist_em':
                count = await self.save_fund_lof_hist_data(df)
            elif collection_name == 'fund_hist_sina':
                count = await self.save_fund_hist_sina_data(df)
            elif collection_name == 'fund_open_fund_daily_em':
                count = await self.save_fund_open_fund_daily_data(df)
            elif collection_name == 'fund_open_fund_info_em':
                # æ–‡ä»¶å¯¼å…¥éœ€è¦æŒ‡å®š fund_code å’Œ indicator
                logger.warning("å¼€æ”¾å¼åŸºé‡‘å†å²è¡Œæƒ…æ–‡ä»¶å¯¼å…¥éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œè¯·ä½¿ç”¨ API åˆ·æ–°")
                return {"imported_count": 0, "message": "è¯¥é›†åˆéœ€è¦é€šè¿‡ API åˆ·æ–°"}
            elif collection_name == 'fund_money_fund_daily_em':
                count = await self.save_fund_money_fund_daily_data(df)
            elif collection_name == 'fund_money_fund_info_em':
                logger.warning("è´§å¸å‹åŸºé‡‘å†å²è¡Œæƒ…æ–‡ä»¶å¯¼å…¥éœ€è¦æŒ‡å®šåŸºé‡‘ä»£ç ")
                return {"imported_count": 0, "message": "è¯¥é›†åˆéœ€è¦é€šè¿‡ API åˆ·æ–°"}
            elif collection_name == 'fund_financial_fund_daily_em':
                count = await self.save_fund_financial_fund_daily_data(df)
            elif collection_name == 'fund_financial_fund_info_em':
                logger.warning("ç†è´¢å‹åŸºé‡‘å†å²è¡Œæƒæ–‡ä»¶å¯¼å…¥éœ€è¦æŒ‡å®šåŸºé‡‘ä»£ç ")
                return {"imported_count": 0, "message": "è¯¥é›†åˆéœ€è¦é€šè¿‡ API åˆ·æ–°"}
            elif collection_name == 'fund_graded_fund_daily_em':
                count = await self.save_fund_graded_fund_daily_data(df)
            elif collection_name == 'fund_graded_fund_info_em':
                logger.warning("åˆ†çº§åŸºé‡‘å†å²æ•°æ®æ–‡ä»¶å¯¼å…¥éœ€è¦æŒ‡å®šåŸºé‡‘ä»£ç ")
                return {"imported_count": 0, "message": "è¯¥é›†åˆéœ€è¦é€šè¿‡ API åˆ·æ–°"}
            elif collection_name == 'fund_etf_fund_daily_em':
                count = await self.save_fund_etf_fund_daily_data(df)
            elif collection_name == 'fund_hk_hist_em':
                count = await self.save_fund_hk_hist_em_data(df)
            elif collection_name == 'fund_cf_em':
                count = await self.save_fund_cf_em_data(df)
            elif collection_name == 'fund_fh_rank_em':
                count = await self.save_fund_fh_rank_em_data(df)
            elif collection_name == 'fund_open_fund_rank_em':
                count = await self.save_fund_open_fund_rank_em_data(df)
            elif collection_name == 'fund_exchange_rank_em':
                count = await self.save_fund_exchange_rank_em_data(df)
            elif collection_name == 'fund_money_rank_em':
                count = await self.save_fund_money_rank_em_data(df)
            elif collection_name == 'fund_lcx_rank_em':
                count = await self.save_fund_lcx_rank_em_data(df)
            elif collection_name == 'fund_hk_rank_em':
                count = await self.save_fund_hk_rank_em_data(df)
            elif collection_name == 'fund_individual_achievement_xq':
                count = await self.save_fund_individual_achievement_xq_data(df)
            elif collection_name == 'fund_value_estimation_em':
                count = await self.save_fund_value_estimation_em_data(df)
            elif collection_name == 'fund_individual_analysis_xq':
                count = await self.save_fund_individual_analysis_xq_data(df)
            elif collection_name == 'fund_individual_profit_probability_xq':
                count = await self.save_fund_individual_profit_probability_xq_data(df)
            elif collection_name == 'fund_individual_detail_hold_xq':
                count = await self.save_fund_individual_detail_hold_xq_data(df)
            elif collection_name == 'fund_overview_em':
                count = await self.save_fund_overview_em_data(df)
            elif collection_name == 'fund_fee_em':
                count = await self.save_fund_fee_em_data(df)
            elif collection_name == 'fund_individual_detail_info_xq':
                count = await self.save_fund_individual_detail_info_xq_data(df)
            elif collection_name == 'fund_portfolio_hold_em':
                count = await self.save_fund_portfolio_hold_em_data(df)
            elif collection_name == 'fund_portfolio_bond_hold_em':
                count = await self.save_fund_portfolio_bond_hold_em_data(df)
            elif collection_name == 'fund_portfolio_industry_allocation_em':
                count = await self.save_fund_portfolio_industry_allocation_em_data(df)
            elif collection_name == 'fund_portfolio_change_em':
                count = await self.save_fund_portfolio_change_em_data(df)
            elif collection_name == 'fund_rating_all_em':
                count = await self.save_fund_rating_all_em_data(df)
            elif collection_name == 'fund_rating_sh_em':
                count = await self.save_fund_rating_sh_em_data(df)
            elif collection_name == 'fund_rating_zs_em':
                count = await self.save_fund_rating_zs_em_data(df)
            elif collection_name == 'fund_rating_ja_em':
                count = await self.save_fund_rating_ja_em_data(df)
            elif collection_name == 'fund_manager_em':
                count = await self.save_fund_manager_em_data(df)
            elif collection_name == 'fund_new_found_em':
                count = await self.save_fund_new_found_em_data(df)
            elif collection_name == 'fund_scale_open_sina':
                count = await self.save_fund_scale_open_sina_data(df)
            elif collection_name == 'fund_scale_close_sina':
                count = await self.save_fund_scale_close_sina_data(df)
            elif collection_name == 'fund_scale_structured_sina':
                count = await self.save_fund_scale_structured_sina_data(df)
            elif collection_name == 'fund_aum_em':
                count = await self.save_fund_aum_em_data(df)
            elif collection_name == 'fund_aum_trend_em':
                count = await self.save_fund_aum_trend_em_data(df)
            elif collection_name == 'fund_aum_hist_em':
                count = await self.save_fund_aum_hist_em_data(df)
            elif collection_name == 'reits_realtime_em':
                count = await self.save_reits_realtime_em_data(df)
            elif collection_name == 'reits_hist_em':
                count = await self.save_reits_hist_em_data(df)
            elif collection_name == 'fund_report_stock_cninfo':
                count = await self.save_fund_report_stock_cninfo_data(df)
            elif collection_name == 'fund_report_industry_allocation_cninfo':
                count = await self.save_fund_report_industry_allocation_cninfo_data(df)
            elif collection_name == 'fund_report_asset_allocation_cninfo':
                count = await self.save_fund_report_asset_allocation_cninfo_data(df)
            elif collection_name == 'fund_scale_change_em':
                count = await self.save_fund_scale_change_em_data(df)
            elif collection_name == 'fund_hold_structure_em':
                count = await self.save_fund_hold_structure_em_data(df)
            elif collection_name == 'fund_stock_position_lg':
                count = await self.save_fund_stock_position_lg_data(df)
            elif collection_name == 'fund_balance_position_lg':
                count = await self.save_fund_balance_position_lg_data(df)
            elif collection_name == 'fund_linghuo_position_lg':
                count = await self.save_fund_linghuo_position_lg_data(df)
            elif collection_name == 'fund_announcement_dividend_em':
                count = await self.save_fund_announcement_dividend_em_data(df)
            elif collection_name == 'fund_announcement_report_em':
                count = await self.save_fund_announcement_report_em_data(df)
            elif collection_name == 'fund_announcement_personnel_em':
                count = await self.save_fund_announcement_personnel_em_data(df)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶å¯¼å…¥é›†åˆ: {collection_name}")
                
            return {"imported_count": count, "message": f"æˆåŠŸå¯¼å…¥ {count} æ¡æ•°æ®"}
        except Exception as e:
            logger.error(f"å¯¼å…¥æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
            raise

    async def sync_data_from_remote(self, collection_name: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """ä»è¿œç¨‹æ•°æ®åº“åŒæ­¥æ•°æ®"""
        from motor.motor_asyncio import AsyncIOMotorClient
        try:
            host = config.get('host')
            port = int(config.get('port', 27017))
            username = config.get('username')
            password = config.get('password')
            auth_source = config.get('authSource', 'admin')
            remote_db_name = config.get('database', 'tradingagents') 
            remote_col_name = config.get('collection', collection_name)
            batch_size = int(config.get('batch_size', 1000))
            
            if username and password:
                uri = f"mongodb://{username}:{password}@{host}:{port}/{auth_source}"
            else:
                uri = f"mongodb://{host}:{port}"
            
            if "mongodb://" in host:
                client = AsyncIOMotorClient(host)
            else:
                client = AsyncIOMotorClient(uri)
                
            try:
                if "mongodb://" in host and "/" in host.split("://")[1]:
                     remote_db = client.get_default_database()
                else:
                     remote_db = client[remote_db_name]
            except Exception:
                remote_db = client[remote_db_name]

            remote_col = remote_db[remote_col_name]
            
            cursor = remote_col.find({})
            
            batch = []
            total_synced = 0
            
            async for doc in cursor:
                if '_id' in doc:
                    del doc['_id']
                batch.append(doc)
                
                if len(batch) >= batch_size:
                    df = pd.DataFrame(batch)
                    if collection_name == 'fund_name_em':
                        await self.save_fund_name_em_data(df)
                    elif collection_name == 'fund_basic_info':
                        await self.save_fund_basic_info_data(df)
                    elif collection_name == 'fund_info_index_em':
                        await self.save_fund_info_index_data(df)
                    elif collection_name == 'fund_purchase_status':
                        await self.save_fund_purchase_status_data(df)
                    elif collection_name == 'fund_etf_spot_em':
                        await self.save_fund_etf_spot_data(df)
                    elif collection_name == 'fund_etf_spot_ths':
                        await self.save_fund_etf_spot_ths_data(df)
                    elif collection_name == 'fund_lof_spot_em':
                        await self.save_fund_lof_spot_data(df)
                    elif collection_name == 'fund_spot_sina':
                        await self.save_fund_spot_sina_data(df)
                    elif collection_name == 'fund_etf_hist_min_em':
                        await self.save_fund_etf_hist_min_data(df)
                    elif collection_name == 'fund_etf_hist_em':
                        await self.save_fund_etf_hist_data(df)
                    elif collection_name == 'fund_lof_hist_em':
                        await self.save_fund_lof_hist_data(df)
                    elif collection_name == 'fund_hist_sina':
                        await self.save_fund_hist_sina_data(df)
                    elif collection_name == 'fund_open_fund_daily_em':
                        await self.save_fund_open_fund_daily_data(df)
                    elif collection_name == 'fund_open_fund_info_em':
                        logger.warning("å¼€æ”¾å¼åŸºé‡‘å†å²è¡Œæƒ…éœ€è¦ç‰¹æ®Šå¤„ç†")
                    elif collection_name == 'fund_money_fund_daily_em':
                        await self.save_fund_money_fund_daily_data(df)
                    elif collection_name == 'fund_money_fund_info_em':
                        logger.warning("è´§å¸å‹åŸºé‡‘å†å²è¡Œæƒ…éœ€è¦ç‰¹æ®Šå¤„ç†")
                    elif collection_name == 'fund_financial_fund_daily_em':
                        await self.save_fund_financial_fund_daily_data(df)
                    elif collection_name == 'fund_financial_fund_info_em':
                        logger.warning("ç†è´¢å‹åŸºé‡‘å†å²è¡Œæƒ…éœ€è¦ç‰¹æ®Šå¤„ç†")
                    elif collection_name == 'fund_etf_fund_daily_em':
                        await self.save_fund_etf_fund_daily_data(df)
                    elif collection_name == 'fund_etf_fund_info_em':
                        logger.warning("åœºå†…äº¤æ˜“åŸºé‡‘å†å²è¡Œæƒ…éœ€è¦ç‰¹æ®Šå¤„ç†")
                    elif collection_name == 'fund_hk_hist_em':
                        await self.save_fund_hk_hist_em_data(df)
                    elif collection_name == 'fund_cf_em':
                        await self.save_fund_cf_em_data(df)
                    elif collection_name == 'fund_fh_rank_em':
                        await self.save_fund_fh_rank_em_data(df)
                    elif collection_name == 'fund_open_fund_rank_em':
                        await self.save_fund_open_fund_rank_em_data(df)
                    elif collection_name == 'fund_exchange_rank_em':
                        await self.save_fund_exchange_rank_em_data(df)
                    elif collection_name == 'fund_money_rank_em':
                        await self.save_fund_money_rank_em_data(df)
                    elif collection_name == 'fund_lcx_rank_em':
                        await self.save_fund_lcx_rank_em_data(df)
                    elif collection_name == 'fund_hk_rank_em':
                        await self.save_fund_hk_rank_em_data(df)
                    elif collection_name == 'fund_individual_achievement_xq':
                        await self.save_fund_individual_achievement_xq_data(df)
                    elif collection_name == 'fund_value_estimation_em':
                        await self.save_fund_value_estimation_em_data(df)
                    elif collection_name == 'fund_individual_analysis_xq':
                        await self.save_fund_individual_analysis_xq_data(df)
                    elif collection_name == 'fund_individual_profit_probability_xq':
                        await self.save_fund_individual_profit_probability_xq_data(df)
                    elif collection_name == 'fund_individual_detail_hold_xq':
                        await self.save_fund_individual_detail_hold_xq_data(df)
                    elif collection_name == 'fund_overview_em':
                        await self.save_fund_overview_em_data(df)
                    elif collection_name == 'fund_fee_em':
                        await self.save_fund_fee_em_data(df)
                    elif collection_name == 'fund_individual_detail_info_xq':
                        await self.save_fund_individual_detail_info_xq_data(df)
                    elif collection_name == 'fund_portfolio_hold_em':
                        await self.save_fund_portfolio_hold_em_data(df)
                    elif collection_name == 'fund_portfolio_bond_hold_em':
                        await self.save_fund_portfolio_bond_hold_em_data(df)
                    elif collection_name == 'fund_portfolio_industry_allocation_em':
                        await self.save_fund_portfolio_industry_allocation_em_data(df)
                    elif collection_name == 'fund_portfolio_change_em':
                        await self.save_fund_portfolio_change_em_data(df)
                    elif collection_name == 'fund_rating_all_em':
                        await self.save_fund_rating_all_em_data(df)
                    elif collection_name == 'fund_rating_sh_em':
                        await self.save_fund_rating_sh_em_data(df)
                    elif collection_name == 'fund_rating_zs_em':
                        await self.save_fund_rating_zs_em_data(df)
                    elif collection_name == 'fund_rating_ja_em':
                        await self.save_fund_rating_ja_em_data(df)
                    elif collection_name == 'fund_manager_em':
                        await self.save_fund_manager_em_data(df)
                    elif collection_name == 'fund_new_found_em':
                        await self.save_fund_new_found_em_data(df)
                    elif collection_name == 'fund_scale_open_sina':
                        await self.save_fund_scale_open_sina_data(df)
                    elif collection_name == 'fund_scale_close_sina':
                        await self.save_fund_scale_close_sina_data(df)
                    elif collection_name == 'fund_scale_structured_sina':
                        await self.save_fund_scale_structured_sina_data(df)
                    elif collection_name == 'fund_aum_em':
                        await self.save_fund_aum_em_data(df)
                    elif collection_name == 'fund_aum_trend_em':
                        await self.save_fund_aum_trend_em_data(df)
                    elif collection_name == 'fund_aum_hist_em':
                        await self.save_fund_aum_hist_em_data(df)
                    elif collection_name == 'reits_realtime_em':
                        await self.save_reits_realtime_em_data(df)
                    elif collection_name == 'reits_hist_em':
                        await self.save_reits_hist_em_data(df)
                    elif collection_name == 'fund_report_stock_cninfo':
                        await self.save_fund_report_stock_cninfo_data(df)
                    elif collection_name == 'fund_report_industry_allocation_cninfo':
                        await self.save_fund_report_industry_allocation_cninfo_data(df)
                    elif collection_name == 'fund_report_asset_allocation_cninfo':
                        await self.save_fund_report_asset_allocation_cninfo_data(df)
                    elif collection_name == 'fund_scale_change_em':
                        await self.save_fund_scale_change_em_data(df)
                    elif collection_name == 'fund_hold_structure_em':
                        await self.save_fund_hold_structure_em_data(df)
                    elif collection_name == 'fund_stock_position_lg':
                        await self.save_fund_stock_position_lg_data(df)
                    elif collection_name == 'fund_balance_position_lg':
                        await self.save_fund_balance_position_lg_data(df)
                    elif collection_name == 'fund_linghuo_position_lg':
                        await self.save_fund_linghuo_position_lg_data(df)
                    elif collection_name == 'fund_announcement_dividend_em':
                        await self.save_fund_announcement_dividend_em_data(df)
                    elif collection_name == 'fund_announcement_report_em':
                        await self.save_fund_announcement_report_em_data(df)
                    elif collection_name == 'fund_announcement_personnel_em':
                        await self.save_fund_announcement_personnel_em_data(df)
                    total_synced += len(batch)
                    batch = []
            
            if batch:
                df = pd.DataFrame(batch)
                if collection_name == 'fund_name_em':
                    await self.save_fund_name_em_data(df)
                elif collection_name == 'fund_basic_info':
                    await self.save_fund_basic_info_data(df)
                elif collection_name == 'fund_info_index_em':
                    await self.save_fund_info_index_data(df)
                elif collection_name == 'fund_purchase_status':
                    await self.save_fund_purchase_status_data(df)
                elif collection_name == 'fund_etf_spot_em':
                    await self.save_fund_etf_spot_data(df)
                elif collection_name == 'fund_etf_spot_ths':
                    await self.save_fund_etf_spot_ths_data(df)
                elif collection_name == 'fund_lof_spot_em':
                    await self.save_fund_lof_spot_data(df)
                elif collection_name == 'fund_spot_sina':
                    await self.save_fund_spot_sina_data(df)
                elif collection_name == 'fund_etf_hist_min_em':
                    await self.save_fund_etf_hist_min_data(df)
                elif collection_name == 'fund_etf_hist_em':
                    await self.save_fund_etf_hist_data(df)
                elif collection_name == 'fund_lof_hist_em':
                    await self.save_fund_lof_hist_data(df)
                elif collection_name == 'fund_hist_sina':
                    await self.save_fund_hist_sina_data(df)
                elif collection_name == 'fund_open_fund_daily_em':
                    await self.save_fund_open_fund_daily_data(df)
                elif collection_name == 'fund_open_fund_info_em':
                    logger.warning("å¼€æ”¾å¼åŸºé‡‘å†å²è¡Œæƒ…éœ€è¦ç‰¹æ®Šå¤„ç†")
                elif collection_name == 'fund_money_fund_daily_em':
                    await self.save_fund_money_fund_daily_data(df)
                elif collection_name == 'fund_money_fund_info_em':
                    logger.warning("è´§å¸å‹åŸºé‡‘å†å²è¡Œæƒ…éœ€è¦ç‰¹æ®Šå¤„ç†")
                elif collection_name == 'fund_financial_fund_daily_em':
                    await self.save_fund_financial_fund_daily_data(df)
                elif collection_name == 'fund_financial_fund_info_em':
                    logger.warning("ç†è´¢å‹åŸºé‡‘å†å²è¡Œæƒ…éœ€è¦ç‰¹æ®Šå¤„ç†")
                elif collection_name == 'fund_etf_fund_daily_em':
                    await self.save_fund_etf_fund_daily_data(df)
                elif collection_name == 'fund_etf_fund_info_em':
                    logger.warning("åœºå†…äº¤æ˜“åŸºé‡‘å†å²è¡Œæƒ…éœ€è¦ç‰¹æ®Šå¤„ç†")
                elif collection_name == 'fund_hk_hist_em':
                    await self.save_fund_hk_hist_em_data(df)
                elif collection_name == 'fund_cf_em':
                    await self.save_fund_cf_em_data(df)
                elif collection_name == 'fund_fh_rank_em':
                    await self.save_fund_fh_rank_em_data(df)
                elif collection_name == 'fund_open_fund_rank_em':
                    await self.save_fund_open_fund_rank_em_data(df)
                elif collection_name == 'fund_exchange_rank_em':
                    await self.save_fund_exchange_rank_em_data(df)
                elif collection_name == 'fund_money_rank_em':
                    await self.save_fund_money_rank_em_data(df)
                elif collection_name == 'fund_lcx_rank_em':
                    await self.save_fund_lcx_rank_em_data(df)
                elif collection_name == 'fund_hk_rank_em':
                    await self.save_fund_hk_rank_em_data(df)
                elif collection_name == 'fund_individual_achievement_xq':
                    await self.save_fund_individual_achievement_xq_data(df)
                elif collection_name == 'fund_value_estimation_em':
                    await self.save_fund_value_estimation_em_data(df)
                elif collection_name == 'fund_individual_analysis_xq':
                    await self.save_fund_individual_analysis_xq_data(df)
                elif collection_name == 'fund_individual_profit_probability_xq':
                    await self.save_fund_individual_profit_probability_xq_data(df)
                elif collection_name == 'fund_individual_detail_hold_xq':
                    await self.save_fund_individual_detail_hold_xq_data(df)
                elif collection_name == 'fund_overview_em':
                    await self.save_fund_overview_em_data(df)
                elif collection_name == 'fund_fee_em':
                    await self.save_fund_fee_em_data(df)
                elif collection_name == 'fund_individual_detail_info_xq':
                    await self.save_fund_individual_detail_info_xq_data(df)
                elif collection_name == 'fund_portfolio_hold_em':
                    await self.save_fund_portfolio_hold_em_data(df)
                elif collection_name == 'fund_portfolio_bond_hold_em':
                    await self.save_fund_portfolio_bond_hold_em_data(df)
                elif collection_name == 'fund_portfolio_industry_allocation_em':
                    await self.save_fund_portfolio_industry_allocation_em_data(df)
                elif collection_name == 'fund_portfolio_change_em':
                    await self.save_fund_portfolio_change_em_data(df)
                elif collection_name == 'fund_rating_all_em':
                    await self.save_fund_rating_all_em_data(df)
                elif collection_name == 'fund_rating_sh_em':
                    await self.save_fund_rating_sh_em_data(df)
                elif collection_name == 'fund_rating_zs_em':
                    await self.save_fund_rating_zs_em_data(df)
                elif collection_name == 'fund_rating_ja_em':
                    await self.save_fund_rating_ja_em_data(df)
                elif collection_name == 'fund_manager_em':
                    await self.save_fund_manager_em_data(df)
                elif collection_name == 'fund_new_found_em':
                    await self.save_fund_new_found_em_data(df)
                elif collection_name == 'fund_scale_open_sina':
                    await self.save_fund_scale_open_sina_data(df)
                elif collection_name == 'fund_scale_close_sina':
                    await self.save_fund_scale_close_sina_data(df)
                elif collection_name == 'fund_scale_structured_sina':
                    await self.save_fund_scale_structured_sina_data(df)
                elif collection_name == 'fund_aum_em':
                    await self.save_fund_aum_em_data(df)
                elif collection_name == 'fund_aum_trend_em':
                    await self.save_fund_aum_trend_em_data(df)
                elif collection_name == 'fund_aum_hist_em':
                    await self.save_fund_aum_hist_em_data(df)
                elif collection_name == 'reits_realtime_em':
                    await self.save_reits_realtime_em_data(df)
                elif collection_name == 'reits_hist_em':
                    await self.save_reits_hist_em_data(df)
                elif collection_name == 'fund_report_stock_cninfo':
                    await self.save_fund_report_stock_cninfo_data(df)
                elif collection_name == 'fund_report_industry_allocation_cninfo':
                    await self.save_fund_report_industry_allocation_cninfo_data(df)
                elif collection_name == 'fund_report_asset_allocation_cninfo':
                    await self.save_fund_report_asset_allocation_cninfo_data(df)
                elif collection_name == 'fund_scale_change_em':
                    await self.save_fund_scale_change_em_data(df)
                elif collection_name == 'fund_hold_structure_em':
                    await self.save_fund_hold_structure_em_data(df)
                elif collection_name == 'fund_stock_position_lg':
                    await self.save_fund_stock_position_lg_data(df)
                elif collection_name == 'fund_balance_position_lg':
                    await self.save_fund_balance_position_lg_data(df)
                elif collection_name == 'fund_linghuo_position_lg':
                    await self.save_fund_linghuo_position_lg_data(df)
                elif collection_name == 'fund_announcement_dividend_em':
                    await self.save_fund_announcement_dividend_em_data(df)
                elif collection_name == 'fund_announcement_report_em':
                    await self.save_fund_announcement_report_em_data(df)
                elif collection_name == 'fund_announcement_personnel_em':
                    await self.save_fund_announcement_personnel_em_data(df)
                total_synced += len(batch)
                
            client.close()
            
            return {
                "synced_count": total_synced, 
                "remote_total": total_synced,
                "message": f"æˆåŠŸåŒæ­¥ {total_synced} æ¡æ•°æ®"
            }
            
        except Exception as e:
            logger.error(f"è¿œç¨‹åŒæ­¥å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_purchase_status_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ä¿å­˜åŸºé‡‘ç”³è´­çŠ¶æ€æ•°æ®åˆ°fund_purchase_statusé›†åˆ
        
        Args:
            df: åŒ…å«åŸºé‡‘ç”³è´­çŠ¶æ€çš„DataFrame
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘ç”³è´­çŠ¶æ€æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            # æ¸…ç†æ— æ•ˆçš„æµ®ç‚¹æ•°å€¼
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘ç”³è´­çŠ¶æ€æ•°æ®...")
            
            # åˆ†æ‰¹å¤„ç†
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ“¦ å°†åˆ† {total_batches} æ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} æ¡")
            
            # è·å–å½“å‰æ—¥æœŸä½œä¸ºæ•°æ®æ—¥æœŸ
            current_date = datetime.now().strftime('%Y-%m-%d')
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"ğŸ“ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ï¼Œè®°å½•èŒƒå›´: {start_idx + 1}-{end_idx}")
                
                # æ„å»ºæ‰¹é‡æ“ä½œ
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # æ¸…ç†NaN/Infinityå€¼
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        # è½¬æ¢ datetime.date å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        # è½¬æ¢ datetime.datetime å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # è·å–åŸºé‡‘ä»£ç å’ŒæŠ¥å‘Šæ—¶é—´
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    report_time = str(doc.get('æœ€æ–°å‡€å€¼/ä¸‡ä»½æ”¶ç›Š-æŠ¥å‘Šæ—¶é—´', current_date))
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    doc['code'] = fund_code
                    doc['date'] = report_time
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_purchase_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç å’Œæ—¥æœŸä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼ˆå¦‚éœ€æ±‚æ‰€è¿°ï¼‰
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'date': report_time},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # æ‰§è¡Œæ‰¹é‡å†™å…¥
                if ops:
                    result = await self.col_fund_purchase_status.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"âœ… ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹å†™å…¥å®Œæˆ: "
                        f"æ–°å¢={result.upserted_count}, æ›´æ–°={result.matched_count}, "
                        f"æœ¬æ‰¹ä¿å­˜={batch_saved}, ç´¯è®¡={total_saved}/{total_count}"
                    )
                    
                    # è°ƒç”¨è¿›åº¦å›è°ƒ
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡åŸºé‡‘ç”³è´­çŠ¶æ€æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘ç”³è´­çŠ¶æ€æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘ç”³è´­çŠ¶æ€æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_purchase_status_data(self) -> int:
        """
        æ¸…ç©ºåŸºé‡‘ç”³è´­çŠ¶æ€æ•°æ®
        
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_purchase_status.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘ç”³è´­çŠ¶æ€æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘ç”³è´­çŠ¶æ€æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_purchase_status_stats(self) -> Dict[str, Any]:
        """
        è·å–åŸºé‡‘ç”³è´­çŠ¶æ€ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_purchase_status.count_documents({})
            
            # æŒ‰åŸºé‡‘ç±»å‹ç»Ÿè®¡
            pipeline_type = [
                {
                    '$group': {
                        '_id': '$åŸºé‡‘ç±»å‹',
                        'count': {'$sum': 1}
                    }
                },
                {
                    '$sort': {'count': -1}
                }
            ]
            
            type_stats: List[Dict[str, Any]] = []
            async for doc in self.col_fund_purchase_status.aggregate(pipeline_type):
                type_stats.append({
                    'type': doc['_id'],
                    'count': doc['count']
                })
            
            # æŒ‰ç”³è´­çŠ¶æ€ç»Ÿè®¡
            pipeline_purchase = [
                {
                    '$group': {
                        '_id': '$ç”³è´­çŠ¶æ€',
                        'count': {'$sum': 1}
                    }
                },
                {
                    '$sort': {'count': -1}
                }
            ]
            
            purchase_status_stats: List[Dict[str, Any]] = []
            async for doc in self.col_fund_purchase_status.aggregate(pipeline_purchase):
                purchase_status_stats.append({
                    'status': doc['_id'],
                    'count': doc['count']
                })
            
            # æŒ‰èµå›çŠ¶æ€ç»Ÿè®¡
            pipeline_redeem = [
                {
                    '$group': {
                        '_id': '$èµå›çŠ¶æ€',
                        'count': {'$sum': 1}
                    }
                },
                {
                    '$sort': {'count': -1}
                }
            ]
            
            redeem_status_stats: List[Dict[str, Any]] = []
            async for doc in self.col_fund_purchase_status.aggregate(pipeline_redeem):
                redeem_status_stats.append({
                    'status': doc['_id'],
                    'count': doc['count']
                })
            
            # è®¡ç®—æœ€æ—©å’Œæœ€æ™šæ—¥æœŸ
            earliest_date = None
            latest_date = None
            pipeline_date = [
                {
                    '$group': {
                        '_id': None,
                        'earliest': {'$min': '$date'},
                        'latest': {'$max': '$date'}
                    }
                }
            ]
            
            async for doc in self.col_fund_purchase_status.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            return {
                'total_count': total_count,
                'type_stats': type_stats,
                'purchase_status_stats': purchase_status_stats,
                'redeem_status_stats': redeem_status_stats,
                'earliest_date': earliest_date,
                'latest_date': latest_date
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘ç”³è´­çŠ¶æ€ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_etf_spot_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ä¿å­˜ETFåŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®åˆ°fund_etf_spot_emé›†åˆ
        
        Args:
            df: åŒ…å«ETFåŸºé‡‘å®æ—¶è¡Œæƒ…çš„DataFrame
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰ETFåŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            # æ¸…ç†æ— æ•ˆçš„æµ®ç‚¹æ•°å€¼
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡ETFåŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®...")
            
            # åˆ†æ‰¹å¤„ç†
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ“¦ å°†åˆ† {total_batches} æ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} æ¡")
            
            # è·å–å½“å‰æ—¥æœŸä½œä¸ºæ•°æ®æ—¥æœŸï¼ˆå¦‚æœæ•°æ®ä¸­æ²¡æœ‰æ—¥æœŸå­—æ®µï¼‰
            current_date = datetime.now().strftime('%Y-%m-%d')
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"ğŸ“ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ï¼Œè®°å½•èŒƒå›´: {start_idx + 1}-{end_idx}")
                
                # æ„å»ºæ‰¹é‡æ“ä½œ
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # æ¸…ç†NaN/Infinityå€¼å’Œè½¬æ¢æ—¥æœŸç±»å‹
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        # è½¬æ¢ datetime.date å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        # è½¬æ¢ datetime.datetime å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d %H:%M:%S')
                    
                    # è·å–åŸºé‡‘ä»£ç å’Œæ•°æ®æ—¥æœŸ
                    fund_code = str(doc.get('ä»£ç ', ''))
                    data_date = str(doc.get('æ•°æ®æ—¥æœŸ', current_date))
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    doc['code'] = fund_code
                    doc['date'] = data_date
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_etf_spot_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç å’Œæ—¥æœŸä½œä¸ºå”¯ä¸€æ ‡è¯†
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'date': data_date},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # æ‰§è¡Œæ‰¹é‡å†™å…¥
                if ops:
                    result = await self.col_fund_etf_spot.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"âœ… ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹å†™å…¥å®Œæˆ: "
                        f"æ–°å¢={result.upserted_count}, æ›´æ–°={result.matched_count}, "
                        f"æœ¬æ‰¹ä¿å­˜={batch_saved}, ç´¯è®¡={total_saved}/{total_count}"
                    )
                    
                    # è°ƒç”¨è¿›åº¦å›è°ƒ
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡ETFåŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜ETFåŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_etf_spot_data(self) -> int:
        """
        æ¸…ç©ºETFåŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®
        
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_etf_spot.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ğŸ—‘ï¸  å·²æ¸…ç©º {deleted_count} æ¡ETFåŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºETFåŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_etf_spot_stats(self) -> Dict[str, Any]:
        """
        è·å–ETFåŸºé‡‘å®æ—¶è¡Œæƒ…ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_etf_spot.count_documents({})
            
            # ç»Ÿè®¡æ¶¨è·Œæ•°é‡
            rise_count = await self.col_fund_etf_spot.count_documents({'æ¶¨è·Œå¹…': {'$gt': 0}})
            fall_count = await self.col_fund_etf_spot.count_documents({'æ¶¨è·Œå¹…': {'$lt': 0}})
            flat_count = total_count - rise_count - fall_count
            
            # ç»Ÿè®¡æˆäº¤é¢TOP10
            pipeline_volume = [
                {
                    '$sort': {'æˆäº¤é¢': -1}
                },
                {
                    '$limit': 10
                },
                {
                    '$project': {
                        'name': '$åç§°',
                        'code': '$ä»£ç ',
                        'volume': '$æˆäº¤é¢',
                        'price': '$æœ€æ–°ä»·',
                        'change_pct': '$æ¶¨è·Œå¹…'
                    }
                }
            ]
            
            top_volume: List[Dict[str, Any]] = []
            async for doc in self.col_fund_etf_spot.aggregate(pipeline_volume):
                top_volume.append({
                    'name': doc.get('name'),
                    'code': doc.get('code'),
                    'volume': doc.get('volume'),
                    'price': doc.get('price'),
                    'change_pct': doc.get('change_pct')
                })
            
            # ç»Ÿè®¡æ¶¨è·Œå¹…TOP10
            pipeline_rise = [
                {
                    '$sort': {'æ¶¨è·Œå¹…': -1}
                },
                {
                    '$limit': 10
                },
                {
                    '$project': {
                        'name': '$åç§°',
                        'code': '$ä»£ç ',
                        'change_pct': '$æ¶¨è·Œå¹…',
                        'price': '$æœ€æ–°ä»·',
                        'volume': '$æˆäº¤é¢'
                    }
                }
            ]
            
            top_gainers: List[Dict[str, Any]] = []
            async for doc in self.col_fund_etf_spot.aggregate(pipeline_rise):
                top_gainers.append({
                    'name': doc.get('name'),
                    'code': doc.get('code'),
                    'change_pct': doc.get('change_pct'),
                    'price': doc.get('price'),
                    'volume': doc.get('volume')
                })
            
            # è®¡ç®—æœ€æ–°æ—¥æœŸ
            pipeline_date = [
                {
                    '$group': {
                        '_id': None,
                        'latest': {'$max': '$date'}
                    }
                }
            ]
            
            latest_date = None
            async for doc in self.col_fund_etf_spot.aggregate(pipeline_date):
                latest_date = doc.get('latest')
            
            # ç»Ÿè®¡åŸºé‡‘ç±»å‹åˆ†å¸ƒï¼ˆåŸºäºåç§°å…³é”®è¯åˆ†ç±»ï¼‰
            type_keywords = {
                'è¡Œä¸šETF': ['èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'åŒ»è¯', 'æ¶ˆè´¹', 'é‡‘è', 'åœ°äº§', 'èƒ½æº', 'åŒ–å·¥', 'å†›å·¥', 'æ±½è½¦', 'é€šä¿¡', 'ä¼ åª’', 'ç”µå­', 'è®¡ç®—æœº', 'æœºæ¢°', 'ç”µæ°”', 'å»ºç­‘', 'é’¢é“', 'æœ‰è‰²', 'ç…¤ç‚­', 'çŸ³æ²¹', 'é“¶è¡Œ', 'è¯åˆ¸', 'ä¿é™©'],
                'å®½åŸºETF': ['æ²ªæ·±300', 'ä¸­è¯500', 'åˆ›ä¸šæ¿', 'ç§‘åˆ›50', 'ä¸Šè¯50', 'ä¸­è¯1000', 'çº¢åˆ©', 'ä»·å€¼', 'æˆé•¿', 'è´¨é‡', 'ä½æ³¢'],
                'ä¸»é¢˜ETF': ['æ–°èƒ½æº', 'ç§‘æŠ€', 'ç¢³ä¸­å’Œ', 'æ•°å­—ç»æµ', 'å¤§æ•°æ®', 'äººå·¥æ™ºèƒ½', '5G', 'ç‰©è”ç½‘', 'äº‘è®¡ç®—', 'æ™ºèƒ½', 'åˆ›æ–°', 'è½¬å‹'],
                'è¡Œä¸šæŒ‡æ•°ETF': ['è¯åˆ¸å…¬å¸', 'éé“¶é‡‘è', 'æˆ¿åœ°äº§', 'å›½é˜²å†›å·¥', 'é£Ÿå“é¥®æ–™', 'å®¶ç”¨ç”µå™¨', 'çººç»‡æœè£…', 'å†œæ—ç‰§æ¸”'],
                'æ¸¯è‚¡ETF': ['æ¸¯è‚¡', 'æ’ç”Ÿ', 'é¦™æ¸¯', 'Hè‚¡', 'HKEX'],
                'å€ºåˆ¸ETF': ['å€º', 'å›½å€º', 'åœ°æ–¹å€º', 'ä¼ä¸šå€º', 'å¯è½¬å€º', 'ä¿¡ç”¨å€º'],
                'å•†å“ETF': ['é»„é‡‘', 'ç™½é“¶', 'åŸæ²¹', 'å•†å“', 'æœ‰è‰²é‡‘å±', 'è´µé‡‘å±'],
                'è·¨å¢ƒETF': ['ç¾è‚¡', 'çº³æ–¯è¾¾å…‹', 'æ ‡æ™®', 'å¾·å›½', 'æ³•å›½', 'æ—¥æœ¬', 'å°åº¦', 'è¶Šå—', 'å…¨çƒ'],
            }
            
            type_counts: Dict[str, int] = {}
            
            # è·å–æ‰€æœ‰åŸºé‡‘åç§°å¹¶åˆ†ç±»
            async for doc in self.col_fund_etf_spot.find({}, {'åç§°': 1}):
                name = doc.get('åç§°', '')
                classified = False
                
                # æŒ‰å…³é”®è¯åŒ¹é…ç±»å‹
                for fund_type, keywords in type_keywords.items():
                    if any(keyword in name for keyword in keywords):
                        type_counts[fund_type] = type_counts.get(fund_type, 0) + 1
                        classified = True
                        break
                
                # æœªåŒ¹é…çš„å½’ä¸ºå…¶ä»–ç±»å‹
                if not classified:
                    type_counts['å…¶ä»–ETF'] = type_counts.get('å…¶ä»–ETF', 0) + 1
            
            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            type_stats = [
                {'type': fund_type, 'count': count}
                for fund_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True)
            ]
            
            return {
                'total_count': total_count,
                'rise_count': rise_count,
                'fall_count': fall_count,
                'flat_count': flat_count,
                'top_volume': top_volume,
                'top_gainers': top_gainers,
                'latest_date': latest_date,
                'type_stats': type_stats
            }
        except Exception as e:
            logger.error(f"è·å–ETFåŸºé‡‘å®æ—¶è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_etf_spot_ths_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ä¿å­˜åŒèŠ±é¡ºETFå®æ—¶è¡Œæƒ…æ•°æ®åˆ°MongoDB
        
        Args:
            df: åŒ…å«ETFå®æ—¶è¡Œæƒ…çš„DataFrame
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°(current, total, percentage, message)
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("DataFrameä¸ºç©ºï¼Œæ— æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            # æ¸…ç†æ•°æ®ï¼šæ›¿æ¢æ— æ•ˆå€¼
            df = df.replace([float('inf'), float('-inf')], None)
            df = df.where(pd.notna(df), None)
            
            # å‡†å¤‡æ‰¹é‡æ“ä½œ
            ops = []
            total_count = len(df)
            batch_size = 500
            
            for idx, row in df.iterrows():
                # è·å–åŸºé‡‘ä»£ç å’ŒæŸ¥è¯¢æ—¥æœŸä½œä¸ºå”¯ä¸€æ ‡è¯†
                fund_code = str(row['åŸºé‡‘ä»£ç ']).strip()
                query_date = str(row['æŸ¥è¯¢æ—¥æœŸ']).strip() if pd.notna(row.get('æŸ¥è¯¢æ—¥æœŸ')) else ''
                
                if not fund_code or not query_date:
                    continue
                
                # æ„å»ºæ–‡æ¡£
                doc = {
                    'åºå·': int(row['åºå·']) if pd.notna(row.get('åºå·')) else None,
                    'åŸºé‡‘ä»£ç ': fund_code,
                    'åŸºé‡‘åç§°': str(row['åŸºé‡‘åç§°']).strip() if pd.notna(row.get('åŸºé‡‘åç§°')) else '',
                    'å½“å‰-å•ä½å‡€å€¼': float(row['å½“å‰-å•ä½å‡€å€¼']) if pd.notna(row.get('å½“å‰-å•ä½å‡€å€¼')) else None,
                    'å½“å‰-ç´¯è®¡å‡€å€¼': float(row['å½“å‰-ç´¯è®¡å‡€å€¼']) if pd.notna(row.get('å½“å‰-ç´¯è®¡å‡€å€¼')) else None,
                    'å‰ä¸€æ—¥-å•ä½å‡€å€¼': float(row['å‰ä¸€æ—¥-å•ä½å‡€å€¼']) if pd.notna(row.get('å‰ä¸€æ—¥-å•ä½å‡€å€¼')) else None,
                    'å‰ä¸€æ—¥-ç´¯è®¡å‡€å€¼': float(row['å‰ä¸€æ—¥-ç´¯è®¡å‡€å€¼']) if pd.notna(row.get('å‰ä¸€æ—¥-ç´¯è®¡å‡€å€¼')) else None,
                    'å¢é•¿å€¼': float(row['å¢é•¿å€¼']) if pd.notna(row.get('å¢é•¿å€¼')) else None,
                    'å¢é•¿ç‡': float(row['å¢é•¿ç‡']) if pd.notna(row.get('å¢é•¿ç‡')) else None,
                    'èµå›çŠ¶æ€': str(row['èµå›çŠ¶æ€']).strip() if pd.notna(row.get('èµå›çŠ¶æ€')) else '',
                    'ç”³è´­çŠ¶æ€': str(row['ç”³è´­çŠ¶æ€']).strip() if pd.notna(row.get('ç”³è´­çŠ¶æ€')) else '',
                    'æœ€æ–°-äº¤æ˜“æ—¥': str(row['æœ€æ–°-äº¤æ˜“æ—¥']).strip() if pd.notna(row.get('æœ€æ–°-äº¤æ˜“æ—¥')) else '',
                    'æœ€æ–°-å•ä½å‡€å€¼': float(row['æœ€æ–°-å•ä½å‡€å€¼']) if pd.notna(row.get('æœ€æ–°-å•ä½å‡€å€¼')) else None,
                    'æœ€æ–°-ç´¯è®¡å‡€å€¼': float(row['æœ€æ–°-ç´¯è®¡å‡€å€¼']) if pd.notna(row.get('æœ€æ–°-ç´¯è®¡å‡€å€¼')) else None,
                    'åŸºé‡‘ç±»å‹': str(row['åŸºé‡‘ç±»å‹']).strip() if pd.notna(row.get('åŸºé‡‘ç±»å‹')) else '',
                    'æŸ¥è¯¢æ—¥æœŸ': query_date,
                    'code': fund_code,
                    'date': query_date,
                    'source': 'akshare',
                    'endpoint': 'fund_etf_spot_ths',
                    'updated_at': datetime.now()
                }
                
                # å¤„ç†æ—¥æœŸç±»å‹å­—æ®µ
                for field in ['æŸ¥è¯¢æ—¥æœŸ', 'æœ€æ–°-äº¤æ˜“æ—¥', 'date']:
                    if field in doc and doc[field] and isinstance(doc[field], (date, datetime)):
                        doc[field] = doc[field].isoformat() if hasattr(doc[field], 'isoformat') else str(doc[field])
                
                # æ·»åŠ åˆ°æ‰¹é‡æ“ä½œ
                ops.append(
                    UpdateOne(
                        {'code': fund_code, 'date': query_date},
                        {'$set': doc},
                        upsert=True
                    )
                )
                
                # æ‰¹é‡æ‰§è¡Œ
                if len(ops) >= batch_size:
                    result = await self.col_fund_etf_spot_ths.bulk_write(ops, ordered=False)
                    
                    if progress_callback:
                        current = idx + 1
                        percentage = int((current / total_count) * 100)
                        progress_callback(current, total_count, percentage, f"å·²ä¿å­˜ {current}/{total_count} æ¡æ•°æ®")
                    
                    ops = []
            
            # æ‰§è¡Œå‰©ä½™æ“ä½œ
            saved_count = 0
            if ops:
                result = await self.col_fund_etf_spot_ths.bulk_write(ops, ordered=False)
                saved_count = result.upserted_count + result.modified_count
            
            if progress_callback:
                progress_callback(total_count, total_count, 100, f"å®Œæˆï¼å…±ä¿å­˜ {total_count} æ¡æ•°æ®")
            
            logger.info(f"æˆåŠŸä¿å­˜ {total_count} æ¡åŒèŠ±é¡ºETFå®æ—¶è¡Œæƒ…æ•°æ®")
            return total_count
            
        except Exception as e:
            logger.error(f"ä¿å­˜åŒèŠ±é¡ºETFå®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_etf_spot_ths_data(self) -> int:
        """
        æ¸…ç©ºåŒèŠ±é¡ºETFå®æ—¶è¡Œæƒ…æ•°æ®
        
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_etf_spot_ths.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŒèŠ±é¡ºETFå®æ—¶è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŒèŠ±é¡ºETFå®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_etf_spot_ths_stats(self) -> Dict[str, Any]:
        """
        è·å–åŒèŠ±é¡ºETFå®æ—¶è¡Œæƒ…ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            # æ€»è®°å½•æ•°
            total_count = await self.col_fund_etf_spot_ths.count_documents({})
            
            # æ¶¨è·Œç»Ÿè®¡
            rise_count = await self.col_fund_etf_spot_ths.count_documents({'å¢é•¿ç‡': {'$gt': 0}})
            fall_count = await self.col_fund_etf_spot_ths.count_documents({'å¢é•¿ç‡': {'$lt': 0}})
            flat_count = await self.col_fund_etf_spot_ths.count_documents({'å¢é•¿ç‡': 0})
            
            # åŸºé‡‘ç±»å‹åˆ†å¸ƒ
            type_pipeline = [
                {'$group': {'_id': '$åŸºé‡‘ç±»å‹', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            type_stats = []
            async for doc in self.col_fund_etf_spot_ths.aggregate(type_pipeline):
                if doc['_id']:
                    type_stats.append({
                        'type': doc['_id'],
                        'count': doc['count']
                    })
            
            # æ¶¨å¹…TOP10
            top_gainers = []
            cursor = self.col_fund_etf_spot_ths.find(
                {'å¢é•¿ç‡': {'$ne': None, '$gt': 0}},
                {'åŸºé‡‘ä»£ç ': 1, 'åŸºé‡‘åç§°': 1, 'å¢é•¿ç‡': 1, '_id': 0}
            ).sort('å¢é•¿ç‡', -1).limit(10)
            
            async for doc in cursor:
                top_gainers.append({
                    'code': doc.get('åŸºé‡‘ä»£ç '),
                    'name': doc.get('åŸºé‡‘åç§°'),
                    'rate': doc.get('å¢é•¿ç‡')
                })
            
            # è·Œå¹…TOP10
            top_losers = []
            cursor = self.col_fund_etf_spot_ths.find(
                {'å¢é•¿ç‡': {'$ne': None, '$lt': 0}},
                {'åŸºé‡‘ä»£ç ': 1, 'åŸºé‡‘åç§°': 1, 'å¢é•¿ç‡': 1, '_id': 0}
            ).sort('å¢é•¿ç‡', 1).limit(10)
            
            async for doc in cursor:
                top_losers.append({
                    'code': doc.get('åŸºé‡‘ä»£ç '),
                    'name': doc.get('åŸºé‡‘åç§°'),
                    'rate': doc.get('å¢é•¿ç‡')
                })
            
            # ç”³èµçŠ¶æ€ç»Ÿè®¡
            purchase_pipeline = [
                {'$group': {'_id': '$ç”³è´­çŠ¶æ€', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            purchase_stats = []
            async for doc in self.col_fund_etf_spot_ths.aggregate(purchase_pipeline):
                if doc['_id']:
                    purchase_stats.append({
                        'status': doc['_id'],
                        'count': doc['count']
                    })
            
            redeem_pipeline = [
                {'$group': {'_id': '$èµå›çŠ¶æ€', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            redeem_stats = []
            async for doc in self.col_fund_etf_spot_ths.aggregate(redeem_pipeline):
                if doc['_id']:
                    redeem_stats.append({
                        'status': doc['_id'],
                        'count': doc['count']
                    })
            
            # æœ€æ–°æ—¥æœŸ
            latest_date = None
            cursor = self.col_fund_etf_spot_ths.find(
                {'æŸ¥è¯¢æ—¥æœŸ': {'$ne': None}},
                {'æŸ¥è¯¢æ—¥æœŸ': 1, '_id': 0}
            ).sort('æŸ¥è¯¢æ—¥æœŸ', -1).limit(1)
            
            async for doc in cursor:
                latest_date = doc.get('æŸ¥è¯¢æ—¥æœŸ')
            
            return {
                'total_count': total_count,
                'rise_count': rise_count,
                'fall_count': fall_count,
                'flat_count': flat_count,
                'type_stats': type_stats,
                'top_gainers': top_gainers,
                'top_losers': top_losers,
                'purchase_status_stats': purchase_stats,
                'redeem_status_stats': redeem_stats,
                'latest_date': latest_date
            }
        except Exception as e:
            logger.error(f"è·å–åŒèŠ±é¡ºETFå®æ—¶è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_lof_spot_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ä¿å­˜LOFåŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®åˆ°MongoDB
        
        Args:
            df: åŒ…å«LOFåŸºé‡‘å®æ—¶è¡Œæƒ…çš„DataFrame
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°(current, total, percentage, message)
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("DataFrameä¸ºç©ºï¼Œæ— æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            # æ¸…ç†æ•°æ®ï¼šæ›¿æ¢æ— æ•ˆå€¼
            df = df.replace([float('inf'), float('-inf')], None)
            df = df.where(pd.notna(df), None)
            
            # å‡†å¤‡æ‰¹é‡æ“ä½œ
            ops = []
            total_count = len(df)
            batch_size = 500
            
            # æ·»åŠ æ•°æ®æ—¥æœŸ
            data_date = datetime.now().strftime('%Y-%m-%d')
            
            for idx, row in df.iterrows():
                # è·å–åŸºé‡‘ä»£ç ä½œä¸ºå”¯ä¸€æ ‡è¯†
                fund_code = str(row['ä»£ç ']).strip()
                
                if not fund_code:
                    continue
                
                # æ„å»ºæ–‡æ¡£
                doc = {
                    'ä»£ç ': fund_code,
                    'åç§°': str(row['åç§°']).strip() if pd.notna(row.get('åç§°')) else '',
                    'æœ€æ–°ä»·': float(row['æœ€æ–°ä»·']) if pd.notna(row.get('æœ€æ–°ä»·')) else None,
                    'æ¶¨è·Œé¢': float(row['æ¶¨è·Œé¢']) if pd.notna(row.get('æ¶¨è·Œé¢')) else None,
                    'æ¶¨è·Œå¹…': float(row['æ¶¨è·Œå¹…']) if pd.notna(row.get('æ¶¨è·Œå¹…')) else None,
                    'æˆäº¤é‡': float(row['æˆäº¤é‡']) if pd.notna(row.get('æˆäº¤é‡')) else None,
                    'æˆäº¤é¢': float(row['æˆäº¤é¢']) if pd.notna(row.get('æˆäº¤é¢')) else None,
                    'å¼€ç›˜ä»·': float(row['å¼€ç›˜ä»·']) if pd.notna(row.get('å¼€ç›˜ä»·')) else None,
                    'æœ€é«˜ä»·': float(row['æœ€é«˜ä»·']) if pd.notna(row.get('æœ€é«˜ä»·')) else None,
                    'æœ€ä½ä»·': float(row['æœ€ä½ä»·']) if pd.notna(row.get('æœ€ä½ä»·')) else None,
                    'æ˜¨æ”¶': float(row['æ˜¨æ”¶']) if pd.notna(row.get('æ˜¨æ”¶')) else None,
                    'æ¢æ‰‹ç‡': float(row['æ¢æ‰‹ç‡']) if pd.notna(row.get('æ¢æ‰‹ç‡')) else None,
                    'æµé€šå¸‚å€¼': int(row['æµé€šå¸‚å€¼']) if pd.notna(row.get('æµé€šå¸‚å€¼')) else None,
                    'æ€»å¸‚å€¼': int(row['æ€»å¸‚å€¼']) if pd.notna(row.get('æ€»å¸‚å€¼')) else None,
                    'æ•°æ®æ—¥æœŸ': data_date,
                    'code': fund_code,
                    'date': data_date,
                    'source': 'akshare',
                    'endpoint': 'fund_lof_spot_em',
                    'updated_at': datetime.now()
                }
                
                # æ·»åŠ åˆ°æ‰¹é‡æ“ä½œ
                ops.append(
                    UpdateOne(
                        {'code': fund_code, 'date': data_date},
                        {'$set': doc},
                        upsert=True
                    )
                )
                
                # æ‰¹é‡æ‰§è¡Œ
                if len(ops) >= batch_size:
                    result = await self.col_fund_lof_spot.bulk_write(ops, ordered=False)
                    
                    if progress_callback:
                        current = idx + 1
                        percentage = int((current / total_count) * 100)
                        progress_callback(current, total_count, percentage, f"å·²ä¿å­˜ {current}/{total_count} æ¡æ•°æ®")
                    
                    ops = []
            
            # æ‰§è¡Œå‰©ä½™æ“ä½œ
            saved_count = 0
            if ops:
                result = await self.col_fund_lof_spot.bulk_write(ops, ordered=False)
                saved_count = result.upserted_count + result.modified_count
            
            if progress_callback:
                progress_callback(total_count, total_count, 100, f"å®Œæˆï¼å…±ä¿å­˜ {total_count} æ¡æ•°æ®")
            
            logger.info(f"æˆåŠŸä¿å­˜ {total_count} æ¡LOFåŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®")
            return total_count
            
        except Exception as e:
            logger.error(f"ä¿å­˜LOFåŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_lof_spot_data(self) -> int:
        """
        æ¸…ç©ºLOFåŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®
        
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_lof_spot.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡LOFåŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºLOFåŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_lof_spot_stats(self) -> Dict[str, Any]:
        """
        è·å–LOFåŸºé‡‘å®æ—¶è¡Œæƒ…ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            # æ€»è®°å½•æ•°
            total_count = await self.col_fund_lof_spot.count_documents({})
            
            # æ¶¨è·Œç»Ÿè®¡
            rise_count = await self.col_fund_lof_spot.count_documents({'æ¶¨è·Œå¹…': {'$gt': 0}})
            fall_count = await self.col_fund_lof_spot.count_documents({'æ¶¨è·Œå¹…': {'$lt': 0}})
            flat_count = await self.col_fund_lof_spot.count_documents({'æ¶¨è·Œå¹…': 0})
            
            # æˆäº¤é¢TOP10
            top_volume = []
            cursor = self.col_fund_lof_spot.find(
                {'æˆäº¤é¢': {'$ne': None}},
                {'ä»£ç ': 1, 'åç§°': 1, 'æˆäº¤é¢': 1, '_id': 0}
            ).sort('æˆäº¤é¢', -1).limit(10)
            
            async for doc in cursor:
                top_volume.append({
                    'code': doc.get('ä»£ç '),
                    'name': doc.get('åç§°'),
                    'amount': doc.get('æˆäº¤é¢')
                })
            
            # æ¶¨å¹…TOP10
            top_gainers = []
            cursor = self.col_fund_lof_spot.find(
                {'æ¶¨è·Œå¹…': {'$ne': None, '$gt': 0}},
                {'ä»£ç ': 1, 'åç§°': 1, 'æ¶¨è·Œå¹…': 1, '_id': 0}
            ).sort('æ¶¨è·Œå¹…', -1).limit(10)
            
            async for doc in cursor:
                top_gainers.append({
                    'code': doc.get('ä»£ç '),
                    'name': doc.get('åç§°'),
                    'rate': doc.get('æ¶¨è·Œå¹…')
                })
            
            # è·Œå¹…TOP10
            top_losers = []
            cursor = self.col_fund_lof_spot.find(
                {'æ¶¨è·Œå¹…': {'$ne': None, '$lt': 0}},
                {'ä»£ç ': 1, 'åç§°': 1, 'æ¶¨è·Œå¹…': 1, '_id': 0}
            ).sort('æ¶¨è·Œå¹…', 1).limit(10)
            
            async for doc in cursor:
                top_losers.append({
                    'code': doc.get('ä»£ç '),
                    'name': doc.get('åç§°'),
                    'rate': doc.get('æ¶¨è·Œå¹…')
                })
            
            # å¸‚å€¼åˆ†å¸ƒç»Ÿè®¡ï¼ˆæŒ‰å¸‚å€¼èŒƒå›´åˆ†ç»„ï¼‰
            market_cap_ranges = [
                {'name': '10äº¿ä»¥ä¸‹', 'min': 0, 'max': 1000000000},
                {'name': '10-50äº¿', 'min': 1000000000, 'max': 5000000000},
                {'name': '50-100äº¿', 'min': 5000000000, 'max': 10000000000},
                {'name': '100äº¿ä»¥ä¸Š', 'min': 10000000000, 'max': float('inf')}
            ]
            
            market_cap_stats = []
            for range_item in market_cap_ranges:
                count = await self.col_fund_lof_spot.count_documents({
                    'æ€»å¸‚å€¼': {
                        '$gte': range_item['min'],
                        '$lt': range_item['max'] if range_item['max'] != float('inf') else 999999999999
                    }
                })
                if count > 0:
                    market_cap_stats.append({
                        'range': range_item['name'],
                        'count': count
                    })
            
            # æœ€æ–°æ—¥æœŸ
            latest_date = None
            cursor = self.col_fund_lof_spot.find(
                {'æ•°æ®æ—¥æœŸ': {'$ne': None}},
                {'æ•°æ®æ—¥æœŸ': 1, '_id': 0}
            ).sort('æ•°æ®æ—¥æœŸ', -1).limit(1)
            
            async for doc in cursor:
                latest_date = doc.get('æ•°æ®æ—¥æœŸ')
            
            return {
                'total_count': total_count,
                'rise_count': rise_count,
                'fall_count': fall_count,
                'flat_count': flat_count,
                'top_volume': top_volume,
                'top_gainers': top_gainers,
                'top_losers': top_losers,
                'market_cap_stats': market_cap_stats,
                'latest_date': latest_date
            }
        except Exception as e:
            logger.error(f"è·å–LOFåŸºé‡‘å®æ—¶è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_spot_sina_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘å®æ—¶è¡Œæƒ…-æ–°æµªæ•°æ®åˆ° MongoDBã€‚

        ä½¿ç”¨ `code + date` ä½œä¸ºå”¯ä¸€é”®è¿›è¡Œ upsertï¼Œç»“æ„ä¸ LOF å®æ—¶è¡Œæƒ…ä¿æŒä¸€è‡´ï¼Œ
        å¹¶é€šè¿‡ `UpdateOne` æ„é€ åˆæ³•çš„ bulk_write è¯·æ±‚ã€‚

        Args:
            df: åŒ…å«åŸºé‡‘å®æ—¶è¡Œæƒ…çš„ DataFrame
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total, percentage, message)

        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("åŸºé‡‘å®æ—¶è¡Œæƒ…-æ–°æµªæ•°æ®ä¸ºç©º")
            return 0

        try:
            logger.info(f"å¼€å§‹ä¿å­˜åŸºé‡‘å®æ—¶è¡Œæƒ…-æ–°æµªæ•°æ®ï¼Œå…± {len(df)} æ¡")

            # æ‹·è´ä¸€ä»½ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            df = df.copy()

            # æ·»åŠ å…ƒæ•°æ®
            current_date = datetime.now().strftime("%Y-%m-%d")
            df["æ•°æ®æ—¥æœŸ"] = current_date
            # éƒ¨åˆ†åœºæ™¯ä¸‹å¯èƒ½æ²¡æœ‰ code åˆ—ï¼Œè¿™é‡Œæ˜¾å¼ä» ä»£ç  è¡ç”Ÿ
            if "code" not in df.columns:
                df["code"] = df["ä»£ç "].astype(str)
            df["date"] = current_date
            df["source"] = "akshare"
            df["endpoint"] = "fund_etf_category_sina"
            df["updated_at"] = datetime.now()

            # æ¸…ç†æ•°æ®ï¼šå°† NaN å’Œ Infinity æ›¿æ¢ä¸º Noneï¼Œé¿å… JSON åºåˆ—åŒ–é—®é¢˜
            df = df.replace([float("inf"), float("-inf")], None)
            df = df.where(pd.notnull(df), None)

            batch_size = 500
            total_count = len(df)
            total_saved = 0

            for batch_start in range(0, total_count, batch_size):
                batch_end = min(batch_start + batch_size, total_count)
                batch_df = df.iloc[batch_start:batch_end]

                ops = []
                for _, row in batch_df.iterrows():
                    record = row.to_dict()

                    code = str(record.get("code") or record.get("ä»£ç ") or "").strip()
                    if not code:
                        continue

                    date_value = record.get("date") or current_date
                    record["code"] = code
                    record["date"] = date_value

                    ops.append(
                        UpdateOne(
                            {"code": code, "date": date_value},
                            {"$set": record},
                            upsert=True,
                        )
                    )

                if ops:
                    result = await self.col_fund_spot_sina.bulk_write(ops, ordered=False)
                    saved_count = (result.upserted_count or 0) + (result.modified_count or 0)
                    total_saved += saved_count

                # æ›´æ–°è¿›åº¦
                if progress_callback:
                    current = batch_end
                    percentage = int((current / total_count) * 100)
                    progress_callback(current, total_count, percentage, f"å·²ä¿å­˜ {current}/{total_count} æ¡æ•°æ®")

            logger.info(f"åŸºé‡‘å®æ—¶è¡Œæƒ…-æ–°æµªæ•°æ®ä¿å­˜å®Œæˆï¼Œå…±ä¿å­˜ {total_saved} æ¡")
            return total_saved

        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘å®æ—¶è¡Œæƒ…-æ–°æµªæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_spot_sina_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘å®æ—¶è¡Œæƒ…-æ–°æµªæ•°æ®"""
        try:
            result = await self.col_fund_spot_sina.delete_many({})
            logger.info(f"æ¸…ç©ºåŸºé‡‘å®æ—¶è¡Œæƒ…-æ–°æµªæ•°æ®æˆåŠŸï¼Œåˆ é™¤ {result.deleted_count} æ¡")
            return result.deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘å®æ—¶è¡Œæƒ…-æ–°æµªæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_spot_sina_stats(self) -> dict:
        """
        è·å–åŸºé‡‘å®æ—¶è¡Œæƒ…-æ–°æµªç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            # æ€»æ•°
            total_count = await self.col_fund_spot_sina.count_documents({})
            
            # æ¶¨è·Œç»Ÿè®¡
            rise_count = await self.col_fund_spot_sina.count_documents({
                "æ¶¨è·Œå¹…": {"$gt": 0}
            })
            fall_count = await self.col_fund_spot_sina.count_documents({
                "æ¶¨è·Œå¹…": {"$lt": 0}
            })
            flat_count = await self.col_fund_spot_sina.count_documents({
                "æ¶¨è·Œå¹…": 0
            })
            
            # åŸºé‡‘ç±»å‹åˆ†å¸ƒç»Ÿè®¡
            type_pipeline = [
                {
                    "$group": {
                        "_id": "$åŸºé‡‘ç±»å‹",
                        "count": {"$sum": 1}
                    }
                },
                {
                    "$project": {
                        "_id": 0,
                        "type": "$_id",
                        "count": 1
                    }
                },
                {"$sort": {"count": -1}}
            ]
            type_stats = await self.col_fund_spot_sina.aggregate(type_pipeline).to_list(None)
            
            # æˆäº¤é¢TOP10
            volume_pipeline = [
                {"$match": {"æˆäº¤é¢": {"$ne": None}}},
                {"$sort": {"æˆäº¤é¢": -1}},
                {"$limit": 10},
                {
                    "$project": {
                        "_id": 0,
                        "code": "$ä»£ç ",
                        "name": "$åç§°",
                        "amount": "$æˆäº¤é¢",
                        "type": "$åŸºé‡‘ç±»å‹"
                    }
                }
            ]
            top_volume = await self.col_fund_spot_sina.aggregate(volume_pipeline).to_list(None)
            
            # æ¶¨å¹…TOP10
            gainers_pipeline = [
                {"$match": {"æ¶¨è·Œå¹…": {"$ne": None, "$gt": 0}}},
                {"$sort": {"æ¶¨è·Œå¹…": -1}},
                {"$limit": 10},
                {
                    "$project": {
                        "_id": 0,
                        "code": "$ä»£ç ",
                        "name": "$åç§°",
                        "rate": "$æ¶¨è·Œå¹…",
                        "type": "$åŸºé‡‘ç±»å‹"
                    }
                }
            ]
            top_gainers = await self.col_fund_spot_sina.aggregate(gainers_pipeline).to_list(None)
            
            # è·Œå¹…TOP10
            losers_pipeline = [
                {"$match": {"æ¶¨è·Œå¹…": {"$ne": None, "$lt": 0}}},
                {"$sort": {"æ¶¨è·Œå¹…": 1}},
                {"$limit": 10},
                {
                    "$project": {
                        "_id": 0,
                        "code": "$ä»£ç ",
                        "name": "$åç§°",
                        "rate": "$æ¶¨è·Œå¹…",
                        "type": "$åŸºé‡‘ç±»å‹"
                    }
                }
            ]
            top_losers = await self.col_fund_spot_sina.aggregate(losers_pipeline).to_list(None)
            
            # æœ€æ–°æ•°æ®æ—¥æœŸ
            latest_doc = await self.col_fund_spot_sina.find_one(
                {},
                sort=[("updated_at", -1)]
            )
            latest_date = latest_doc.get("æ•°æ®æ—¥æœŸ") if latest_doc else None
            
            return {
                'total_count': total_count,
                'rise_count': rise_count,
                'fall_count': fall_count,
                'flat_count': flat_count,
                'type_stats': type_stats,
                'top_volume': top_volume,
                'top_gainers': top_gainers,
                'top_losers': top_losers,
                'latest_date': latest_date
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘å®æ—¶è¡Œæƒ…-æ–°æµªç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                'total_count': 0,
                'rise_count': 0,
                'fall_count': 0,
                'flat_count': 0,
                'latest_date': None
            }

    async def save_fund_etf_hist_min_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜ ETF åŸºé‡‘åˆ†æ—¶è¡Œæƒ…æ•°æ®åˆ° fund_etf_hist_min_em é›†åˆã€‚

        ä½¿ç”¨ `code + time + period + adjust` ä½œä¸ºå”¯ä¸€é”®è¿›è¡Œ upsertã€‚

        Args:
            df: åŒ…å« ETF åˆ†æ—¶è¡Œæƒ…æ•°æ®çš„ DataFrameï¼Œè‡³å°‘éœ€åŒ…å« `ä»£ç ` å’Œ `æ—¶é—´` åˆ—ã€‚
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total, percentage, message)

        Returns:
            å®é™…å†™å…¥(æ–°å¢+æ›´æ–°)çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("ETF åˆ†æ—¶è¡Œæƒ…æ•°æ®ä¸ºç©ºï¼Œæ— éœ€ä¿å­˜")
            return 0

        try:
            # æ‹·è´ä¸€ä»½ï¼Œé¿å…ä¿®æ”¹å¤–éƒ¨ DataFrame
            df = df.copy()

            # ç»Ÿä¸€æ¸…ç†æ— æ•ˆæ•°å€¼
            df = df.replace([float("inf"), float("-inf")], None)
            df = df.where(pd.notnull(df), None)

            total_count = len(df)
            batch_size = 1000  # è¾¾åˆ°1000æ¡ä¿å­˜ä¸€æ¬¡ï¼Œé€€å‡ºæ—¶ä¸è¶³1000æ¡ä¹Ÿä¿å­˜
            total_saved = 0

            for batch_start in range(0, total_count, batch_size):
                batch_end = min(batch_start + batch_size, total_count)
                batch_df = df.iloc[batch_start:batch_end]

                ops: List[UpdateOne] = []

                for _, row in batch_df.iterrows():
                    record = row.to_dict()

                    # ä»£ç 
                    code = str(record.get("ä»£ç ") or record.get("code") or "").strip()
                    if not code:
                        continue

                    # æ—¶é—´ -> ç»Ÿä¸€ä¸ºå­—ç¬¦ä¸²ï¼Œå¹¶æ´¾ç”Ÿ date
                    time_val = record.get("æ—¶é—´") or record.get("time") or record.get("datetime")
                    if time_val is None or (isinstance(time_val, float) and pd.isna(time_val)):
                        continue

                    if isinstance(time_val, pd.Timestamp):
                        time_str = time_val.strftime("%Y-%m-%d %H:%M:%S")
                    else:
                        time_str = str(time_val).strip()

                    if not time_str:
                        continue

                    date_str = time_str[:10]

                    # å‘¨æœŸä¸å¤æƒæ–¹å¼ï¼ˆé»˜è®¤ 5 åˆ†é’Ÿã€åå¤æƒï¼‰
                    period = str(record.get("period") or "5")
                    adjust = str(record.get("adjust") or "hfq")

                    # å†™å›è§„èŒƒåŒ–å­—æ®µ
                    record["ä»£ç "] = code
                    record["æ—¶é—´"] = time_str
                    record["code"] = code
                    record["time"] = time_str
                    record["date"] = date_str
                    record["period"] = period
                    record["adjust"] = adjust
                    record["source"] = record.get("source") or "akshare"
                    record["endpoint"] = record.get("endpoint") or "fund_etf_hist_min_em"
                    record["updated_at"] = datetime.now()

                    # æ„é€  upsert æ“ä½œ
                    ops.append(
                        UpdateOne(
                            {"code": code, "time": time_str, "period": period, "adjust": adjust},
                            {"$set": record},
                            upsert=True,
                        )
                    )

                if ops:
                    result = await self.col_fund_etf_hist_min_em.bulk_write(ops, ordered=False)
                    saved_count = (result.upserted_count or 0) + (result.modified_count or 0)
                    total_saved += saved_count

                # è¿›åº¦å›è°ƒ
                if progress_callback:
                    current = batch_end
                    percentage = int((current / total_count) * 100)
                    progress_callback(
                        current,
                        total_count,
                        percentage,
                        f"å·²ä¿å­˜ {current}/{total_count} æ¡ ETF åˆ†æ—¶è¡Œæƒ…æ•°æ®",
                    )

            logger.info(f"æˆåŠŸä¿å­˜ {total_saved}/{total_count} æ¡ ETF åˆ†æ—¶è¡Œæƒ…æ•°æ®")
            return total_saved

        except Exception as e:
            logger.error(f"ä¿å­˜ ETF åˆ†æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def clear_fund_etf_hist_min_data(self) -> int:
        """æ¸…ç©º ETF åˆ†æ—¶è¡Œæƒ…æ•°æ®é›†åˆã€‚"""
        try:
            result = await self.col_fund_etf_hist_min_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡ ETF åˆ†æ—¶è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©º ETF åˆ†æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_fund_etf_hist_min_stats(self) -> Dict[str, Any]:
        """è·å– ETF åˆ†æ—¶è¡Œæƒ…ç»Ÿè®¡ä¿¡æ¯ã€‚

        å½“å‰æä¾›ï¼š
        - total_count: æ€»è®°å½•æ•°
        - code_stats: æŒ‰ä»£ç åˆ†ç»„çš„è®°å½•æ•°
        - earliest_time / latest_time: æœ€æ—©å’Œæœ€æ™šçš„æ—¶é—´æˆ³
        """
        try:
            total_count = await self.col_fund_etf_hist_min_em.count_documents({})

            # æŒ‰ä»£ç åˆ†ç»„ç»Ÿè®¡æ•°é‡
            code_stats: List[Dict[str, Any]] = []
            pipeline_codes = [
                {"$group": {"_id": "$code", "count": {"$sum": 1}}},
                {"$sort": {"count": -1}},
            ]
            async for doc in self.col_fund_etf_hist_min_em.aggregate(pipeline_codes):
                if doc.get("_id"):
                    code_stats.append({"code": doc["_id"], "count": doc["count"]})

            # è®¡ç®—æ—¶é—´èŒƒå›´
            earliest_time = None
            latest_time = None
            pipeline_time = [
                {
                    "$group": {
                        "_id": None,
                        "earliest": {"$min": "$time"},
                        "latest": {"$max": "$time"},
                    }
                }
            ]

            async for doc in self.col_fund_etf_hist_min_em.aggregate(pipeline_time):
                earliest_time = doc.get("earliest")
                latest_time = doc.get("latest")

            return {
                "total_count": total_count,
                "code_stats": code_stats,
                "earliest_time": earliest_time,
                "latest_time": latest_time,
            }
        except Exception as e:
            logger.error(f"è·å– ETF åˆ†æ—¶è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "code_stats": [],
                "earliest_time": None,
                "latest_time": None,
            }

    async def save_fund_lof_hist_min_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜ LOF åŸºé‡‘åˆ†æ—¶è¡Œæƒ…æ•°æ®åˆ° fund_lof_hist_min_em é›†åˆã€‚

        ä½¿ç”¨ `code + time + period + adjust` ä½œä¸ºå”¯ä¸€é”®è¿›è¡Œ upsertã€‚

        Args:
            df: åŒ…å« LOF åˆ†æ—¶è¡Œæƒ…æ•°æ®çš„ DataFrameï¼Œè‡³å°‘éœ€åŒ…å« `ä»£ç ` å’Œ `æ—¶é—´` åˆ—ã€‚
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total, percentage, message)

        Returns:
            å®é™…å†™å…¥(æ–°å¢+æ›´æ–°)çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("LOF åˆ†æ—¶è¡Œæƒ…æ•°æ®ä¸ºç©ºï¼Œæ— éœ€ä¿å­˜")
            return 0

        try:
            # æ‹·è´ä¸€ä»½ï¼Œé¿å…ä¿®æ”¹å¤–éƒ¨ DataFrame
            df = df.copy()

            # ç»Ÿä¸€æ¸…ç†æ— æ•ˆæ•°å€¼
            df = df.replace([float("inf"), float("-inf")], None)
            df = df.where(pd.notnull(df), None)

            total_count = len(df)
            batch_size = 1000  # è¾¾åˆ°1000æ¡ä¿å­˜ä¸€æ¬¡ï¼Œé€€å‡ºæ—¶ä¸è¶³1000æ¡ä¹Ÿä¿å­˜
            total_saved = 0

            for batch_start in range(0, total_count, batch_size):
                batch_end = min(batch_start + batch_size, total_count)
                batch_df = df.iloc[batch_start:batch_end]

                ops: List[UpdateOne] = []

                for _, row in batch_df.iterrows():
                    record = row.to_dict()

                    # ä»£ç 
                    code = str(record.get("ä»£ç ") or record.get("code") or "").strip()
                    if not code:
                        continue

                    # æ—¶é—´ -> ç»Ÿä¸€ä¸ºå­—ç¬¦ä¸²ï¼Œå¹¶æ´¾ç”Ÿ date
                    time_val = record.get("æ—¶é—´") or record.get("time") or record.get("datetime")
                    if time_val is None or (isinstance(time_val, float) and pd.isna(time_val)):
                        continue

                    if isinstance(time_val, pd.Timestamp):
                        time_str = time_val.strftime("%Y-%m-%d %H:%M:%S")
                    else:
                        time_str = str(time_val).strip()

                    if not time_str:
                        continue

                    date_str = time_str[:10]

                    # å‘¨æœŸä¸å¤æƒæ–¹å¼ï¼ˆé»˜è®¤ 5 åˆ†é’Ÿã€åå¤æƒï¼‰
                    period = str(record.get("period") or "5")
                    adjust = str(record.get("adjust") or "hfq")

                    # å†™å›è§„èŒƒåŒ–å­—æ®µ
                    record["ä»£ç "] = code
                    record["æ—¶é—´"] = time_str
                    record["code"] = code
                    record["time"] = time_str
                    record["date"] = date_str
                    record["period"] = period
                    record["adjust"] = adjust
                    record["source"] = record.get("source") or "akshare"
                    record["endpoint"] = record.get("endpoint") or "fund_lof_hist_min_em"
                    record["updated_at"] = datetime.now()

                    # æ„é€  upsert æ“ä½œ
                    ops.append(
                        UpdateOne(
                            {"code": code, "time": time_str, "period": period, "adjust": adjust},
                            {"$set": record},
                            upsert=True,
                        )
                    )

                if ops:
                    result = await self.col_fund_lof_hist_min_em.bulk_write(ops, ordered=False)
                    saved_count = (result.upserted_count or 0) + (result.modified_count or 0)
                    total_saved += saved_count

                # è¿›åº¦å›è°ƒ
                if progress_callback:
                    current = batch_end
                    percentage = int((current / total_count) * 100)
                    progress_callback(
                        current,
                        total_count,
                        percentage,
                        f"å·²ä¿å­˜ {current}/{total_count} æ¡ LOF åˆ†æ—¶è¡Œæƒ…æ•°æ®",
                    )

            logger.info(f"æˆåŠŸä¿å­˜ {total_saved}/{total_count} æ¡ LOF åˆ†æ—¶è¡Œæƒ…æ•°æ®")
            return total_saved

        except Exception as e:
            logger.error(f"ä¿å­˜ LOF åˆ†æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def clear_fund_lof_hist_min_data(self) -> int:
        """æ¸…ç©º LOF åˆ†æ—¶è¡Œæƒ…æ•°æ®é›†åˆã€‚"""
        try:
            result = await self.col_fund_lof_hist_min_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡ LOF åˆ†æ—¶è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©º LOF åˆ†æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_fund_lof_hist_min_stats(self) -> Dict[str, Any]:
        """è·å– LOF åˆ†æ—¶è¡Œæƒ…ç»Ÿè®¡ä¿¡æ¯ã€‚

        å½“å‰æä¾›ï¼š
        - total_count: æ€»è®°å½•æ•°
        - code_stats: æŒ‰ä»£ç åˆ†ç»„çš„è®°å½•æ•°
        - earliest_time / latest_time: æœ€æ—©å’Œæœ€æ™šçš„æ—¶é—´æˆ³
        """
        try:
            total_count = await self.col_fund_lof_hist_min_em.count_documents({})

            # æŒ‰ä»£ç åˆ†ç»„ç»Ÿè®¡æ•°é‡
            code_stats: List[Dict[str, Any]] = []
            pipeline_codes = [
                {"$group": {"_id": "$code", "count": {"$sum": 1}}},
                {"$sort": {"count": -1}},
            ]
            async for doc in self.col_fund_lof_hist_min_em.aggregate(pipeline_codes):
                if doc.get("_id"):
                    code_stats.append({"code": doc["_id"], "count": doc["count"]})

            # è®¡ç®—æ—¶é—´èŒƒå›´
            earliest_time = None
            latest_time = None
            pipeline_time = [
                {
                    "$group": {
                        "_id": None,
                        "earliest": {"$min": "$time"},
                        "latest": {"$max": "$time"},
                    }
                }
            ]

            async for doc in self.col_fund_lof_hist_min_em.aggregate(pipeline_time):
                earliest_time = doc.get("earliest")
                latest_time = doc.get("latest")

            return {
                "total_count": total_count,
                "code_stats": code_stats,
                "earliest_time": earliest_time,
                "latest_time": latest_time,
            }
        except Exception as e:
            logger.error(f"è·å– LOF åˆ†æ—¶è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "code_stats": [],
                "earliest_time": None,
                "latest_time": None,
            }

    async def save_fund_etf_hist_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜ ETF åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®åˆ° fund_etf_hist_em é›†åˆã€‚

        ä½¿ç”¨ `code + date + period + adjust` ä½œä¸ºå”¯ä¸€é”®è¿›è¡Œ upsertã€‚

        Args:
            df: åŒ…å« ETF å†å²è¡Œæƒ…æ•°æ®çš„ DataFrameï¼Œè‡³å°‘éœ€åŒ…å« `ä»£ç ` å’Œ `æ—¥æœŸ` åˆ—ã€‚
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total, percentage, message)

        Returns:
            å®é™…å†™å…¥(æ–°å¢+æ›´æ–°)çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("ETF å†å²è¡Œæƒ…æ•°æ®ä¸ºç©ºï¼Œæ— éœ€ä¿å­˜")
            return 0

        try:
            # æ‹·è´ä¸€ä»½ï¼Œé¿å…ä¿®æ”¹å¤–éƒ¨ DataFrame
            df = df.copy()

            # ç»Ÿä¸€æ¸…ç†æ— æ•ˆæ•°å€¼
            df = df.replace([float("inf"), float("-inf")], None)
            df = df.where(pd.notnull(df), None)

            total_count = len(df)
            batch_size = 500
            total_saved = 0

            for batch_start in range(0, total_count, batch_size):
                batch_end = min(batch_start + batch_size, total_count)
                batch_df = df.iloc[batch_start:batch_end]

                ops: List[UpdateOne] = []

                for _, row in batch_df.iterrows():
                    record = row.to_dict()

                    # ä»£ç 
                    code = str(record.get("ä»£ç ") or record.get("code") or "").strip()
                    if not code:
                        continue

                    # æ—¥æœŸ -> ç»Ÿä¸€ä¸ºå­—ç¬¦ä¸²
                    date_val = record.get("æ—¥æœŸ") or record.get("date")
                    if date_val is None or (isinstance(date_val, float) and pd.isna(date_val)):
                        continue

                    if isinstance(date_val, pd.Timestamp):
                        date_str = date_val.strftime("%Y-%m-%d")
                    else:
                        date_str = str(date_val).strip()[:10]

                    if not date_str:
                        continue

                    # å‘¨æœŸä¸å¤æƒæ–¹å¼ï¼ˆé»˜è®¤ dailyã€åå¤æƒï¼‰
                    period = str(record.get("period") or "daily")
                    adjust = str(record.get("adjust") or "hfq")

                    # å†™å›è§„èŒƒåŒ–å­—æ®µ
                    record["ä»£ç "] = code
                    record["æ—¥æœŸ"] = date_str
                    record["code"] = code
                    record["date"] = date_str
                    record["period"] = period
                    record["adjust"] = adjust
                    record["source"] = record.get("source") or "akshare"
                    record["endpoint"] = record.get("endpoint") or "fund_etf_hist_em"
                    record["updated_at"] = datetime.now()

                    # æ„é€  upsert æ“ä½œ
                    ops.append(
                        UpdateOne(
                            {"code": code, "date": date_str, "period": period, "adjust": adjust},
                            {"$set": record},
                            upsert=True,
                        )
                    )

                if ops:
                    result = await self.col_fund_etf_hist_em.bulk_write(ops, ordered=False)
                    saved_count = (result.upserted_count or 0) + (result.modified_count or 0)
                    total_saved += saved_count

                # è¿›åº¦å›è°ƒ
                if progress_callback:
                    current = batch_end
                    percentage = int((current / total_count) * 100)
                    progress_callback(
                        current,
                        total_count,
                        percentage,
                        f"å·²ä¿å­˜ {current}/{total_count} æ¡ ETF å†å²è¡Œæƒ…æ•°æ®",
                    )

            logger.info(f"æˆåŠŸä¿å­˜ {total_saved}/{total_count} æ¡ ETF å†å²è¡Œæƒ…æ•°æ®")
            return total_saved

        except Exception as e:
            logger.error(f"ä¿å­˜ ETF å†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def clear_fund_etf_hist_data(self) -> int:
        """æ¸…ç©º ETF å†å²è¡Œæƒ…æ•°æ®é›†åˆã€‚"""
        try:
            result = await self.col_fund_etf_hist_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡ ETF å†å²è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©º ETF å†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_fund_etf_hist_stats(self) -> Dict[str, Any]:
        """è·å– ETF å†å²è¡Œæƒ…ç»Ÿè®¡ä¿¡æ¯ã€‚

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«ï¼š
            - total_count: æ€»è®°å½•æ•°
            - code_stats: å„ä»£ç çš„ç»Ÿè®¡ [{code, count}, ...]
            - earliest_date: æœ€æ—©æ—¥æœŸ
            - latest_date: æœ€æ–°æ—¥æœŸ
        """
        try:
            # æ€»è®°å½•æ•°
            total_count = await self.col_fund_etf_hist_em.count_documents({})

            # å„ä»£ç ç»Ÿè®¡
            code_pipeline = [
                {"$group": {"_id": "$code", "count": {"$sum": 1}}},
                {"$project": {"code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 100},
            ]
            code_stats = await self.col_fund_etf_hist_em.aggregate(code_pipeline).to_list(100)

            # æœ€æ—©å’Œæœ€æ–°æ—¥æœŸ
            earliest_date = None
            latest_date = None

            if total_count > 0:
                earliest_doc = (
                    await self.col_fund_etf_hist_em.find({}, {"date": 1})
                    .sort("date", 1)
                    .limit(1)
                    .to_list(1)
                )
                if earliest_doc:
                    earliest_date = earliest_doc[0].get("date")

                latest_doc = (
                    await self.col_fund_etf_hist_em.find({}, {"date": 1})
                    .sort("date", -1)
                    .limit(1)
                    .to_list(1)
                )
                if latest_doc:
                    latest_date = latest_doc[0].get("date")

            result = {
                "total_count": total_count,
                "code_stats": code_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            logger.debug(f"ETF å†å²è¡Œæƒ…ç»Ÿè®¡: {result}")
            return result

        except Exception as e:
            logger.error(f"è·å– ETF å†å²è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_lof_hist_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜ LOF åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®åˆ° fund_lof_hist_em é›†åˆã€‚

        ä½¿ç”¨ `code + date + period + adjust` ä½œä¸ºå”¯ä¸€é”®è¿›è¡Œ upsertã€‚

        Args:
            df: åŒ…å« LOF å†å²è¡Œæƒ…æ•°æ®çš„ DataFrameï¼Œè‡³å°‘éœ€åŒ…å« `ä»£ç ` å’Œ `æ—¥æœŸ` åˆ—ã€‚
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total, percentage, message)

        Returns:
            å®é™…å†™å…¥(æ–°å¢+æ›´æ–°)çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("LOF å†å²è¡Œæƒ…æ•°æ®ä¸ºç©ºï¼Œæ— éœ€ä¿å­˜")
            return 0

        try:
            # æ‹·è´ä¸€ä»½ï¼Œé¿å…ä¿®æ”¹å¤–éƒ¨ DataFrame
            df = df.copy()

            # ç»Ÿä¸€æ¸…ç†æ— æ•ˆæ•°å€¼
            df = df.replace([float("inf"), float("-inf")], None)
            df = df.where(pd.notnull(df), None)

            total_count = len(df)
            batch_size = 500
            total_saved = 0

            for batch_start in range(0, total_count, batch_size):
                batch_end = min(batch_start + batch_size, total_count)
                batch_df = df.iloc[batch_start:batch_end]

                ops: List[UpdateOne] = []

                for _, row in batch_df.iterrows():
                    record = row.to_dict()

                    # ä»£ç 
                    code = str(record.get("ä»£ç ") or record.get("code") or "").strip()
                    if not code:
                        continue

                    # æ—¥æœŸ -> ç»Ÿä¸€ä¸ºå­—ç¬¦ä¸²
                    date_val = record.get("æ—¥æœŸ") or record.get("date")
                    if date_val is None or (isinstance(date_val, float) and pd.isna(date_val)):
                        continue

                    if isinstance(date_val, pd.Timestamp):
                        date_str = date_val.strftime("%Y-%m-%d")
                    else:
                        date_str = str(date_val).strip()[:10]

                    if not date_str:
                        continue

                    # å‘¨æœŸä¸å¤æƒæ–¹å¼ï¼ˆé»˜è®¤ dailyã€åå¤æƒï¼‰
                    period = str(record.get("period") or "daily")
                    adjust = str(record.get("adjust") or "hfq")

                    # å†™å›è§„èŒƒåŒ–å­—æ®µ
                    record["ä»£ç "] = code
                    record["æ—¥æœŸ"] = date_str
                    record["code"] = code
                    record["date"] = date_str
                    record["period"] = period
                    record["adjust"] = adjust
                    record["source"] = record.get("source") or "akshare"
                    record["endpoint"] = record.get("endpoint") or "fund_lof_hist_em"
                    record["updated_at"] = datetime.now()

                    # æ„é€  upsert æ“ä½œ
                    ops.append(
                        UpdateOne(
                            {"code": code, "date": date_str, "period": period, "adjust": adjust},
                            {"$set": record},
                            upsert=True,
                        )
                    )

                if ops:
                    result = await self.col_fund_lof_hist_em.bulk_write(ops, ordered=False)
                    saved_count = (result.upserted_count or 0) + (result.modified_count or 0)
                    total_saved += saved_count

                # è¿›åº¦å›è°ƒ
                if progress_callback:
                    current = batch_end
                    percentage = int((current / total_count) * 100)
                    progress_callback(
                        current,
                        total_count,
                        percentage,
                        f"å·²ä¿å­˜ {current}/{total_count} æ¡ LOF å†å²è¡Œæƒ…æ•°æ®",
                    )

            logger.info(f"æˆåŠŸä¿å­˜ {total_saved}/{total_count} æ¡ LOF å†å²è¡Œæƒ…æ•°æ®")
            return total_saved

        except Exception as e:
            logger.error(f"ä¿å­˜ LOF å†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def clear_fund_lof_hist_data(self) -> int:
        """æ¸…ç©º LOF å†å²è¡Œæƒ…æ•°æ®é›†åˆã€‚"""
        try:
            result = await self.col_fund_lof_hist_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡ LOF å†å²è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©º LOF å†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_fund_lof_hist_stats(self) -> Dict[str, Any]:
        """è·å– LOF å†å²è¡Œæƒ…ç»Ÿè®¡ä¿¡æ¯ã€‚

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«ï¼š
            - total_count: æ€»è®°å½•æ•°
            - code_stats: å„ä»£ç çš„ç»Ÿè®¡ [{code, count}, ...]
            - earliest_date: æœ€æ—©æ—¥æœŸ
            - latest_date: æœ€æ–°æ—¥æœŸ
        """
        try:
            # æ€»è®°å½•æ•°
            total_count = await self.col_fund_lof_hist_em.count_documents({})

            # å„ä»£ç ç»Ÿè®¡
            code_pipeline = [
                {"$group": {"_id": "$code", "count": {"$sum": 1}}},
                {"$project": {"code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 100},
            ]
            code_stats = await self.col_fund_lof_hist_em.aggregate(code_pipeline).to_list(100)

            # æœ€æ—©å’Œæœ€æ–°æ—¥æœŸ
            earliest_date = None
            latest_date = None

            if total_count > 0:
                earliest_doc = (
                    await self.col_fund_lof_hist_em.find({}, {"date": 1})
                    .sort("date", 1)
                    .limit(1)
                    .to_list(1)
                )
                if earliest_doc:
                    earliest_date = earliest_doc[0].get("date")

                latest_doc = (
                    await self.col_fund_lof_hist_em.find({}, {"date": 1})
                    .sort("date", -1)
                    .limit(1)
                    .to_list(1)
                )
                if latest_doc:
                    latest_date = latest_doc[0].get("date")

            result = {
                "total_count": total_count,
                "code_stats": code_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            logger.debug(f"LOF å†å²è¡Œæƒ…ç»Ÿè®¡: {result}")
            return result

        except Exception as e:
            logger.error(f"è·å– LOF å†å²è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_hist_sina_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜æ–°æµªåŸºé‡‘å†å²è¡Œæƒ…æ•°æ®

        Args:
            df: åŒ…å«å†å²è¡Œæƒ…æ•°æ®çš„ DataFrameï¼Œå¿…é¡»åŒ…å« date, open, high, low, close, volume å’Œä»£ç å­—æ®µ
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("æ–°æµªåŸºé‡‘å†å²è¡Œæƒ…æ•°æ®ä¸ºç©ºï¼Œæ— éœ€ä¿å­˜")
            return 0

        try:
            df = df.copy()

            # å­—æ®µæ˜ å°„å’Œè§„èŒƒåŒ–
            field_mapping = {
                "date": "date",
                "æ—¥æœŸ": "date",
                "open": "open",
                "å¼€ç›˜": "open",
                "high": "high",
                "æœ€é«˜": "high",
                "low": "low",
                "æœ€ä½": "low",
                "close": "close",
                "æ”¶ç›˜": "close",
                "volume": "volume",
                "æˆäº¤é‡": "volume",
                "ä»£ç ": "code",
                "code": "code",
            }

            # é‡å‘½ååˆ—
            df = df.rename(columns=field_mapping)

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = ["date", "open", "high", "low", "close", "volume", "code"]
            missing = [f for f in required_fields if f not in df.columns]
            if missing:
                logger.error(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing}")
                return 0

            # æ•°æ®æ¸…ç†ï¼šå¤„ç†æ— æ•ˆæ•°å€¼ï¼ˆinfã€NaNï¼‰
            numeric_fields = ["open", "high", "low", "close", "volume"]
            for field in numeric_fields:
                if field in df.columns:
                    df[field] = pd.to_numeric(df[field], errors="coerce")
                    df[field] = df[field].replace([float("inf"), float("-inf")], None)

            # åˆ é™¤å…³é”®å­—æ®µä¸ºç©ºçš„è¡Œ
            df = df.dropna(subset=["date", "code"])

            # æ—¥æœŸæ ¼å¼è½¬æ¢
            if df["date"].dtype == "object":
                try:
                    df["date"] = pd.to_datetime(df["date"]).dt.strftime("%Y-%m-%d")
                except Exception as e:
                    logger.warning(f"æ—¥æœŸæ ¼å¼è½¬æ¢å¤±è´¥: {e}")

            ops = []
            total = len(df)

            for idx, row in df.iterrows():
                code = str(row.get("code", "")).strip()
                date_str = str(row.get("date", "")).strip()

                if not code or not date_str:
                    continue

                record = {
                    "code": code,
                    "date": date_str,
                    "open": float(row["open"]) if pd.notna(row.get("open")) else None,
                    "high": float(row["high"]) if pd.notna(row.get("high")) else None,
                    "low": float(row["low"]) if pd.notna(row.get("low")) else None,
                    "close": float(row["close"]) if pd.notna(row.get("close")) else None,
                    "volume": int(row["volume"]) if pd.notna(row.get("volume")) else None,
                }

                # å”¯ä¸€é”®ï¼šcode + date
                ops.append(
                    UpdateOne(
                        {"code": code, "date": date_str},
                        {"$set": record},
                        upsert=True,
                    )
                )

                # è¿›åº¦å›è°ƒ
                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            if not ops:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")
                return 0

            result = await self.col_fund_hist_sina.bulk_write(ops, ordered=False)
            saved_count = (result.upserted_count or 0) + (result.modified_count or 0)

            logger.info(f"æˆåŠŸä¿å­˜ {saved_count} æ¡æ–°æµªåŸºé‡‘å†å²è¡Œæƒ…æ•°æ®")
            return saved_count

        except Exception as e:
            logger.error(f"ä¿å­˜æ–°æµªåŸºé‡‘å†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def clear_fund_hist_sina_data(self) -> int:
        """æ¸…ç©ºæ–°æµªåŸºé‡‘å†å²è¡Œæƒ…æ•°æ®

        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_hist_sina.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡æ–°æµªåŸºé‡‘å†å²è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºæ–°æµªåŸºé‡‘å†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_fund_hist_sina_stats(self) -> Dict[str, Any]:
        """è·å–æ–°æµªåŸºé‡‘å†å²è¡Œæƒ…ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…æ‹¬æ€»æ•°ã€å„ä»£ç è®°å½•æ•°ã€æœ€æ—©/æœ€æ–°æ—¥æœŸ
        """
        try:
            total_count = await self.col_fund_hist_sina.count_documents({})

            # æŒ‰ä»£ç ç»Ÿè®¡è®°å½•æ•°ï¼ˆTop 100ï¼‰
            code_pipeline = [
                {"$group": {"_id": "$code", "count": {"$sum": 1}}},
                {"$project": {"code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 100},
            ]
            code_stats = await self.col_fund_hist_sina.aggregate(code_pipeline).to_list(100)

            # è·å–æœ€æ—©å’Œæœ€æ–°æ—¥æœŸ
            earliest_doc = (
                await self.col_fund_hist_sina.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_hist_sina.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            earliest_date = earliest_doc[0]["date"] if earliest_doc else None
            latest_date = latest_doc[0]["date"] if latest_doc else None

            result = {
                "total_count": total_count,
                "code_stats": code_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            logger.debug(f"æ–°æµªåŸºé‡‘å†å²è¡Œæƒ…ç»Ÿè®¡: {result}")
            return result

        except Exception as e:
            logger.error(f"è·å–æ–°æµªåŸºé‡‘å†å²è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_open_fund_daily_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜å¼€æ”¾å¼åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®

        Args:
            df: åŒ…å«å¼€æ”¾å¼åŸºé‡‘å®æ—¶è¡Œæƒ…çš„ DataFrame
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("å¼€æ”¾å¼åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®ä¸ºç©ºï¼Œæ— éœ€ä¿å­˜")
            return 0

        try:
            df = df.copy()

            # ä»åˆ—åä¸­æå–æ—¥æœŸï¼ˆåˆ—åæ ¼å¼å¦‚ "2024-01-01-å•ä½å‡€å€¼"ï¼‰
            date_str = None
            for col in df.columns:
                if "-å•ä½å‡€å€¼" in col:
                    # æå–æ—¥æœŸéƒ¨åˆ†
                    date_str = col.split("-å•ä½å‡€å€¼")[0]
                    break

            if not date_str:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
                from datetime import datetime

                date_str = datetime.now().strftime("%Y-%m-%d")
                logger.warning(f"æœªä»åˆ—åä¸­æ‰¾åˆ°æ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ: {date_str}")

            # å­—æ®µæ˜ å°„
            field_mapping = {
                "åŸºé‡‘ä»£ç ": "fund_code",
                "åŸºé‡‘ç®€ç§°": "fund_name",
                f"{date_str}-å•ä½å‡€å€¼": "unit_net_value",
                f"{date_str}-ç´¯è®¡å‡€å€¼": "cumulative_net_value",
                f"{date_str}-å‰äº¤æ˜“æ—¥-å•ä½å‡€å€¼": "prev_unit_net_value",
                f"{date_str}-å‰äº¤æ˜“æ—¥-ç´¯è®¡å‡€å€¼": "prev_cumulative_net_value",
                "æ—¥å¢é•¿å€¼": "daily_growth_value",
                "æ—¥å¢é•¿ç‡": "daily_growth_rate",
                "ç”³è´­çŠ¶æ€": "purchase_status",
                "èµå›çŠ¶æ€": "redemption_status",
                "æ‰‹ç»­è´¹": "fee",
            }

            # é‡å‘½ååˆ—
            df = df.rename(columns=field_mapping)

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = ["fund_code"]
            missing = [f for f in required_fields if f not in df.columns]
            if missing:
                logger.error(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing}")
                return 0

            # æ•°æ®æ¸…ç†ï¼šå¤„ç†æ— æ•ˆæ•°å€¼ï¼ˆinfã€NaNï¼‰
            numeric_fields = [
                "unit_net_value",
                "cumulative_net_value",
                "prev_unit_net_value",
                "prev_cumulative_net_value",
                "daily_growth_value",
                "daily_growth_rate",
            ]
            for field in numeric_fields:
                if field in df.columns:
                    df[field] = pd.to_numeric(df[field], errors="coerce")
                    df[field] = df[field].replace([float("inf"), float("-inf")], None)

            # åˆ é™¤å…³é”®å­—æ®µä¸ºç©ºçš„è¡Œ
            df = df.dropna(subset=["fund_code"])

            ops = []
            total = len(df)
            batch_size = 1000  # æ¯æ‰¹å¤„ç†1000æ¡
            total_saved = 0

            for idx, row in df.iterrows():
                fund_code = str(row.get("fund_code", "")).strip()

                if not fund_code:
                    continue

                record = {
                    "fund_code": fund_code,
                    "date": date_str,
                    "fund_name": str(row.get("fund_name", "")).strip() if pd.notna(row.get("fund_name")) else None,
                    "unit_net_value": float(row["unit_net_value"]) if pd.notna(row.get("unit_net_value")) else None,
                    "cumulative_net_value": float(row["cumulative_net_value"]) if pd.notna(row.get("cumulative_net_value")) else None,
                    "prev_unit_net_value": float(row["prev_unit_net_value"]) if pd.notna(row.get("prev_unit_net_value")) else None,
                    "prev_cumulative_net_value": float(row["prev_cumulative_net_value"]) if pd.notna(row.get("prev_cumulative_net_value")) else None,
                    "daily_growth_value": float(row["daily_growth_value"]) if pd.notna(row.get("daily_growth_value")) else None,
                    "daily_growth_rate": float(row["daily_growth_rate"]) if pd.notna(row.get("daily_growth_rate")) else None,
                    "purchase_status": str(row.get("purchase_status", "")).strip() if pd.notna(row.get("purchase_status")) else None,
                    "redemption_status": str(row.get("redemption_status", "")).strip() if pd.notna(row.get("redemption_status")) else None,
                    "fee": str(row.get("fee", "")).strip() if pd.notna(row.get("fee")) else None,
                }

                # å”¯ä¸€é”®ï¼šfund_code + date
                ops.append(
                    UpdateOne(
                        {"fund_code": fund_code, "date": date_str},
                        {"$set": record},
                        upsert=True,
                    )
                )

                # æ‰¹é‡ä¿å­˜ï¼šæ¯1000æ¡ä¿å­˜ä¸€æ¬¡
                if len(ops) >= batch_size:
                    result = await self.col_fund_open_fund_daily_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.modified_count or 0)
                    total_saved += batch_saved
                    logger.info(f"å·²ä¿å­˜ {len(ops)} æ¡æ•°æ®ï¼Œç´¯è®¡ä¿å­˜ {total_saved} æ¡")
                    ops = []  # æ¸…ç©ºå·²å¤„ç†çš„æ“ä½œ

                # è¿›åº¦å›è°ƒ
                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            # ä¿å­˜å‰©ä½™æ•°æ®
            if ops:
                result = await self.col_fund_open_fund_daily_em.bulk_write(ops, ordered=False)
                batch_saved = (result.upserted_count or 0) + (result.modified_count or 0)
                total_saved += batch_saved
                logger.info(f"å·²ä¿å­˜å‰©ä½™ {len(ops)} æ¡æ•°æ®")

            if total_saved == 0:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")
                return 0

            logger.info(f"æˆåŠŸä¿å­˜ {total_saved} æ¡å¼€æ”¾å¼åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®ï¼ˆæ—¥æœŸ: {date_str}ï¼‰")
            return total_saved

        except Exception as e:
            logger.error(f"ä¿å­˜å¼€æ”¾å¼åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def clear_fund_open_fund_daily_data(self) -> int:
        """æ¸…ç©ºå¼€æ”¾å¼åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®

        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_open_fund_daily_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡å¼€æ”¾å¼åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºå¼€æ”¾å¼åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_fund_open_fund_daily_stats(self) -> Dict[str, Any]:
        """è·å–å¼€æ”¾å¼åŸºé‡‘å®æ—¶è¡Œæƒ…ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…æ‹¬æ€»æ•°ã€æœ€æ—©/æœ€æ–°æ—¥æœŸ
        """
        try:
            total_count = await self.col_fund_open_fund_daily_em.count_documents({})

            # è·å–æœ€æ—©å’Œæœ€æ–°æ—¥æœŸ
            earliest_doc = (
                await self.col_fund_open_fund_daily_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_open_fund_daily_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            earliest_date = earliest_doc[0]["date"] if earliest_doc else None
            latest_date = latest_doc[0]["date"] if latest_doc else None

            # æŒ‰æ—¥æœŸç»Ÿè®¡è®°å½•æ•°
            date_pipeline = [
                {"$group": {"_id": "$date", "count": {"$sum": 1}}},
                {"$project": {"date": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"date": -1}},
                {"$limit": 30},
            ]
            date_stats = await self.col_fund_open_fund_daily_em.aggregate(date_pipeline).to_list(30)

            result = {
                "total_count": total_count,
                "date_stats": date_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            logger.debug(f"å¼€æ”¾å¼åŸºé‡‘å®æ—¶è¡Œæƒ…ç»Ÿè®¡: {result}")
            return result

        except Exception as e:
            logger.error(f"è·å–å¼€æ”¾å¼åŸºé‡‘å®æ—¶è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "date_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_open_fund_info_data(
        self, df: pd.DataFrame, fund_code: str, indicator: str, progress_callback=None
    ) -> int:
        """ä¿å­˜å¼€æ”¾å¼åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®ï¼ˆæ”¯æŒæ‰€æœ‰7ä¸ªæŒ‡æ ‡ï¼‰

        Args:
            df: åŒ…å«å†å²è¡Œæƒ…æ•°æ®çš„ DataFrame
            fund_code: åŸºé‡‘ä»£ç 
            indicator: æŒ‡æ ‡ç±»å‹ï¼ˆå•ä½å‡€å€¼èµ°åŠ¿ã€ç´¯è®¡å‡€å€¼èµ°åŠ¿ã€ç´¯è®¡æ”¶ç›Šç‡èµ°åŠ¿ç­‰ï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning(f"å¼€æ”¾å¼åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®ä¸ºç©ºï¼ˆ{fund_code}, {indicator}ï¼‰")
            return 0

        try:
            df = df.copy()

            # æ ¹æ®ä¸åŒçš„ indicator ç¡®å®šæ—¥æœŸå­—æ®µåç§°
            date_field_map = {
                "å•ä½å‡€å€¼èµ°åŠ¿": "å‡€å€¼æ—¥æœŸ",
                "ç´¯è®¡å‡€å€¼èµ°åŠ¿": "å‡€å€¼æ—¥æœŸ",
                "ç´¯è®¡æ”¶ç›Šç‡èµ°åŠ¿": "æ—¥æœŸ",
                "åŒç±»æ’åèµ°åŠ¿": "æŠ¥å‘Šæ—¥æœŸ",
                "åŒç±»æ’åç™¾åˆ†æ¯”": "æŠ¥å‘Šæ—¥æœŸ",
                "åˆ†çº¢é€é…è¯¦æƒ…": "æƒç›Šç™»è®°æ—¥",  # æˆ–é™¤æ¯æ—¥
                "æ‹†åˆ†è¯¦æƒ…": "æ‹†åˆ†æŠ˜ç®—æ—¥",
            }

            source_date_field = date_field_map.get(indicator)
            if not source_date_field or source_date_field not in df.columns:
                # å°è¯•å…¶ä»–å¯èƒ½çš„æ—¥æœŸå­—æ®µ
                possible_date_fields = ["å‡€å€¼æ—¥æœŸ", "æ—¥æœŸ", "æŠ¥å‘Šæ—¥æœŸ", "æƒç›Šç™»è®°æ—¥", "é™¤æ¯æ—¥", "æ‹†åˆ†æŠ˜ç®—æ—¥", "åˆ†çº¢å‘æ”¾æ—¥"]
                source_date_field = None
                for field in possible_date_fields:
                    if field in df.columns:
                        source_date_field = field
                        break

            if not source_date_field:
                logger.error(f"æ— æ³•æ‰¾åˆ°æ—¥æœŸå­—æ®µ: indicator={indicator}, columns={df.columns.tolist()}")
                return 0

            ops = []
            total = len(df)

            for idx, row in df.iterrows():
                date_value = str(row.get(source_date_field, "")).strip()
                if not date_value or date_value == "nan":
                    continue

                # æ„å»ºè®°å½• - åŠ¨æ€ä¿å­˜æ‰€æœ‰å­—æ®µ
                record = {
                    "fund_code": fund_code,
                    "indicator": indicator,
                    "date": date_value,
                }

                # ä¿å­˜æ‰€æœ‰å…¶ä»–å­—æ®µ
                for col in df.columns:
                    if col != source_date_field:  # æ—¥æœŸå­—æ®µå·²ç»ä¿å­˜ä¸º date
                        value = row.get(col)
                        if pd.notna(value):
                            # å°è¯•è½¬æ¢æ•°å€¼ç±»å‹
                            if isinstance(value, (int, float)):
                                record[col] = float(value) if not isinstance(value, int) else int(value)
                            else:
                                record[col] = str(value).strip()

                # å”¯ä¸€é”®ï¼šfund_code + indicator + date
                ops.append(
                    UpdateOne(
                        {"fund_code": fund_code, "indicator": indicator, "date": date_value},
                        {"$set": record},
                        upsert=True,
                    )
                )

                # è¿›åº¦å›è°ƒ
                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            if not ops:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")
                return 0

            result = await self.col_fund_open_fund_info_em.bulk_write(ops, ordered=False)
            saved_count = (result.upserted_count or 0) + (result.modified_count or 0)

            logger.info(
                f"æˆåŠŸä¿å­˜ {saved_count} æ¡å¼€æ”¾å¼åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®ï¼ˆ{fund_code}, {indicator}ï¼‰"
            )
            return saved_count

        except Exception as e:
            logger.error(f"ä¿å­˜å¼€æ”¾å¼åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_open_fund_info_merged_data(
        self, df_unit: pd.DataFrame, df_acc: pd.DataFrame, fund_code: str, progress_callback=None
    ) -> int:
        """åˆå¹¶å•ä½å‡€å€¼èµ°åŠ¿å’Œç´¯è®¡å‡€å€¼èµ°åŠ¿ï¼Œä¿å­˜åˆ°æ•°æ®åº“
        
        åªä¿ç•™5ä¸ªå­—æ®µï¼šæ—¥æœŸã€åŸºé‡‘ä»£ç ã€å•ä½å‡€å€¼ã€æ—¥å¢é•¿ç‡ã€ç´¯è®¡å‡€å€¼
        
        Args:
            df_unit: å•ä½å‡€å€¼èµ°åŠ¿DataFrameï¼ˆåŒ…å«ï¼šå‡€å€¼æ—¥æœŸã€å•ä½å‡€å€¼ã€æ—¥å¢é•¿ç‡ï¼‰
            df_acc: ç´¯è®¡å‡€å€¼èµ°åŠ¿DataFrameï¼ˆåŒ…å«ï¼šå‡€å€¼æ—¥æœŸã€ç´¯è®¡å‡€å€¼ï¼‰
            fund_code: åŸºé‡‘ä»£ç 
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df_unit is None or df_unit.empty or df_acc is None or df_acc.empty:
            logger.warning(f"å•ä½å‡€å€¼æˆ–ç´¯è®¡å‡€å€¼æ•°æ®ä¸ºç©ºï¼ˆ{fund_code}ï¼‰")
            return 0
            
        try:
            df_unit = df_unit.copy()
            df_acc = df_acc.copy()
            
            # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºåŸå§‹æ•°æ®ç»“æ„
            logger.info(f"å•ä½å‡€å€¼èµ°åŠ¿å­—æ®µ: {df_unit.columns.tolist()}, æ•°æ®é‡: {len(df_unit)}")
            logger.info(f"ç´¯è®¡å‡€å€¼èµ°åŠ¿å­—æ®µ: {df_acc.columns.tolist()}, æ•°æ®é‡: {len(df_acc)}")
            
            # ç¡®ä¿ä¸¤ä¸ªDataFrameéƒ½æœ‰æ—¥æœŸå­—æ®µ
            if "å‡€å€¼æ—¥æœŸ" not in df_unit.columns or "å‡€å€¼æ—¥æœŸ" not in df_acc.columns:
                logger.error(f"æ•°æ®ç¼ºå°‘å‡€å€¼æ—¥æœŸå­—æ®µ: df_unit columns={df_unit.columns.tolist()}, df_acc columns={df_acc.columns.tolist()}")
                return 0
            
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if "å•ä½å‡€å€¼" not in df_unit.columns:
                logger.error(f"å•ä½å‡€å€¼èµ°åŠ¿ç¼ºå°‘'å•ä½å‡€å€¼'å­—æ®µ")
                return 0
            if "æ—¥å¢é•¿ç‡" not in df_unit.columns:
                logger.error(f"å•ä½å‡€å€¼èµ°åŠ¿ç¼ºå°‘'æ—¥å¢é•¿ç‡'å­—æ®µ")
                return 0
            if "ç´¯è®¡å‡€å€¼" not in df_acc.columns:
                logger.error(f"ç´¯è®¡å‡€å€¼èµ°åŠ¿ç¼ºå°‘'ç´¯è®¡å‡€å€¼'å­—æ®µ")
                return 0
            
            # åªé€‰æ‹©éœ€è¦çš„å­—æ®µè¿›è¡Œåˆå¹¶
            df_unit_selected = df_unit[["å‡€å€¼æ—¥æœŸ", "å•ä½å‡€å€¼", "æ—¥å¢é•¿ç‡"]].copy()
            df_acc_selected = df_acc[["å‡€å€¼æ—¥æœŸ", "ç´¯è®¡å‡€å€¼"]].copy()
            
            # æŒ‰æ—¥æœŸï¼ˆåˆ—ï¼‰åˆå¹¶ä¸¤ä¸ªDataFrame
            merged_df = pd.merge(
                df_unit_selected,
                df_acc_selected,
                on="å‡€å€¼æ—¥æœŸ",
                how="inner"  # åªä¿ç•™ä¸¤ä¸ªDataFrameéƒ½æœ‰çš„æ—¥æœŸ
            )
            
            logger.info(f"åˆå¹¶åæ•°æ®é‡: {len(merged_df)}, å­—æ®µ: {merged_df.columns.tolist()}")
            
            if merged_df.empty:
                logger.warning(f"åˆå¹¶åæ•°æ®ä¸ºç©ºï¼ˆ{fund_code}ï¼‰")
                return 0
            
            # æ‰¹é‡ä¿å­˜
            ops = []
            total = len(merged_df)
            batch_size = 1000
            total_saved = 0
            
            for idx, row in merged_df.iterrows():
                date_value = str(row.get("å‡€å€¼æ—¥æœŸ", "")).strip()
                if not date_value or date_value == "nan":
                    continue
                
                # åªä¿ç•™5ä¸ªå­—æ®µï¼šæ—¥æœŸã€åŸºé‡‘ä»£ç ã€å•ä½å‡€å€¼ã€æ—¥å¢é•¿ç‡ã€ç´¯è®¡å‡€å€¼
                record = {
                    "åŸºé‡‘ä»£ç ": fund_code,
                    "æ—¥æœŸ": date_value,
                    "å•ä½å‡€å€¼": float(row["å•ä½å‡€å€¼"]) if pd.notna(row.get("å•ä½å‡€å€¼")) else None,
                    "æ—¥å¢é•¿ç‡": float(row["æ—¥å¢é•¿ç‡"]) if pd.notna(row.get("æ—¥å¢é•¿ç‡")) else None,
                    "ç´¯è®¡å‡€å€¼": float(row["ç´¯è®¡å‡€å€¼"]) if pd.notna(row.get("ç´¯è®¡å‡€å€¼")) else None,
                }
                
                # å”¯ä¸€é”®ï¼šåŸºé‡‘ä»£ç  + æ—¥æœŸ
                ops.append(
                    UpdateOne(
                        {"åŸºé‡‘ä»£ç ": fund_code, "æ—¥æœŸ": date_value},
                        {"$set": record},
                        upsert=True,
                    )
                )
                
                # æ‰¹é‡ä¿å­˜ï¼šæ¯1000æ¡ä¿å­˜ä¸€æ¬¡
                if len(ops) >= batch_size:
                    result = await self.col_fund_open_fund_info_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.modified_count or 0)
                    total_saved += batch_saved
                    logger.info(f"å·²ä¿å­˜ {len(ops)} æ¡æ•°æ®ï¼Œç´¯è®¡ä¿å­˜ {total_saved} æ¡ï¼ˆ{fund_code}ï¼‰")
                    ops = []
                
                # è¿›åº¦å›è°ƒ
                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)
            
            # ä¿å­˜å‰©ä½™æ•°æ®
            if ops:
                result = await self.col_fund_open_fund_info_em.bulk_write(ops, ordered=False)
                batch_saved = (result.upserted_count or 0) + (result.modified_count or 0)
                total_saved += batch_saved
                logger.info(f"å·²ä¿å­˜å‰©ä½™ {len(ops)} æ¡æ•°æ®ï¼ˆ{fund_code}ï¼‰")
            
            if total_saved == 0:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")
                return 0
            
            logger.info(f"æˆåŠŸä¿å­˜ {total_saved} æ¡å¼€æ”¾å¼åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®ï¼ˆ{fund_code}ï¼‰")
            return total_saved
            
        except Exception as e:
            logger.error(f"ä¿å­˜åˆå¹¶åçš„å¼€æ”¾å¼åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def clear_fund_open_fund_info_data(self) -> int:
        """æ¸…ç©ºå¼€æ”¾å¼åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®

        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_open_fund_info_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡å¼€æ”¾å¼åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºå¼€æ”¾å¼åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_fund_open_fund_info_stats(self) -> Dict[str, Any]:
        """è·å–å¼€æ”¾å¼åŸºé‡‘å†å²è¡Œæƒ…ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…æ‹¬æ€»æ•°ã€å„åŸºé‡‘ä»£ç è®°å½•æ•°ã€å„æŒ‡æ ‡è®°å½•æ•°
        """
        try:
            total_count = await self.col_fund_open_fund_info_em.count_documents({})

            # æŒ‰åŸºé‡‘ä»£ç ç»Ÿè®¡è®°å½•æ•°ï¼ˆTop 50ï¼‰
            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 50},
            ]
            fund_code_stats = await self.col_fund_open_fund_info_em.aggregate(
                fund_code_pipeline
            ).to_list(50)

            # æŒ‰æŒ‡æ ‡ç»Ÿè®¡è®°å½•æ•°
            indicator_pipeline = [
                {"$group": {"_id": "$indicator", "count": {"$sum": 1}}},
                {"$project": {"indicator": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
            ]
            indicator_stats = await self.col_fund_open_fund_info_em.aggregate(
                indicator_pipeline
            ).to_list(10)

            # è·å–æœ€æ—©å’Œæœ€æ–°æ—¥æœŸ
            earliest_doc = (
                await self.col_fund_open_fund_info_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_open_fund_info_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            earliest_date = earliest_doc[0]["date"] if earliest_doc else None
            latest_date = latest_doc[0]["date"] if latest_doc else None

            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "indicator_stats": indicator_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            logger.debug(f"å¼€æ”¾å¼åŸºé‡‘å†å²è¡Œæƒ…ç»Ÿè®¡: {result}")
            return result

        except Exception as e:
            logger.error(f"è·å–å¼€æ”¾å¼åŸºé‡‘å†å²è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "indicator_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_money_fund_daily_data(
        self, df: pd.DataFrame, progress_callback=None
    ) -> int:
        """ä¿å­˜è´§å¸å‹åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®

        Args:
            df: åŒ…å«è´§å¸å‹åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®çš„ DataFrame
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("è´§å¸å‹åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®ä¸ºç©º")
            return 0

        try:
            df = df.copy()

            # è·å–å½“å‰æ—¥æœŸä½œä¸ºæ•°æ®æ—¥æœŸ
            from datetime import datetime
            current_date = datetime.now().strftime("%Y-%m-%d")

            # æ¸…ç†å’Œè§„èŒƒåŒ–åˆ—å
            df.columns = df.columns.str.strip()

            # å­—æ®µæ˜ å°„ï¼šAKShareä¸­æ–‡å­—æ®µå -> æ•°æ®åº“ä¸­æ–‡å­—æ®µå
            field_map = {
                "å½“å‰äº¤æ˜“æ—¥-ä¸‡ä»½æ”¶ç›Š": "æ¯ä¸‡ä»½æ”¶ç›Š",
                "å½“å‰äº¤æ˜“æ—¥-7æ—¥å¹´åŒ–%": "7æ—¥å¹´åŒ–æ”¶ç›Šç‡",
                "å½“å‰äº¤æ˜“æ—¥-å•ä½å‡€å€¼": "å•ä½å‡€å€¼",
                "å‰ä¸€äº¤æ˜“æ—¥-ä¸‡ä»½æ”¶ç›Š": "å‰ä¸€æ—¥ä¸‡ä»½æ”¶ç›Š",
                "å‰ä¸€äº¤æ˜“æ—¥-7æ—¥å¹´åŒ–%": "å‰ä¸€æ—¥7æ—¥å¹´åŒ–",
                "å‰ä¸€äº¤æ˜“æ—¥-å•ä½å‡€å€¼": "å‰ä¸€æ—¥å‡€å€¼",
                "æ—¥æ¶¨å¹…": "æ—¥å¢é•¿",
                "æˆç«‹æ—¥æœŸ": "æˆç«‹æ—¥æœŸ",
                "åŸºé‡‘ç»ç†": "åŸºé‡‘ç»ç†",
                "æ‰‹ç»­è´¹": "æ‰‹ç»­è´¹",
                "å¯è´­å…¨éƒ¨": "ç”³è´­çŠ¶æ€",
            }

            ops = []
            total = len(df)

            for idx, row in df.iterrows():
                fund_code = str(row.get("åŸºé‡‘ä»£ç ", "")).strip()
                if not fund_code or fund_code == "nan":
                    continue

                # æ„å»ºè®°å½• - ä½¿ç”¨ä¸­æ–‡å­—æ®µå
                record = {
                    "åŸºé‡‘ä»£ç ": fund_code,
                    "åŸºé‡‘ç®€ç§°": str(row.get("åŸºé‡‘ç®€ç§°", "")).strip() if pd.notna(row.get("åŸºé‡‘ç®€ç§°")) else "",
                    "æ—¥æœŸ": current_date,
                }

                # æ˜ å°„å…¶ä»–å­—æ®µ
                for akshare_field, db_field in field_map.items():
                    value = row.get(akshare_field)
                    if pd.notna(value):
                        value_str = str(value).strip()
                        # è·³è¿‡ "---" ç­‰æ— æ•ˆå€¼
                        if value_str and value_str != "---" and value_str != "nan":
                            # å¤„ç†ç™¾åˆ†æ¯”å’Œæ•°å€¼å­—æ®µ
                            if "%" in akshare_field or "æ”¶ç›Š" in akshare_field or "å‡€å€¼" in akshare_field:
                                # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                                try:
                                    if isinstance(value, (int, float)):
                                        record[db_field] = float(value)
                                    else:
                                        # ç§»é™¤ç™¾åˆ†å·å¹¶è½¬æ¢
                                        clean_value = value_str.replace("%", "").strip()
                                        record[db_field] = float(clean_value) if clean_value else None
                                except:
                                    record[db_field] = value_str
                            else:
                                record[db_field] = value_str

                # å”¯ä¸€é”®ï¼šåŸºé‡‘ä»£ç  + æ—¥æœŸ
                ops.append(
                    UpdateOne(
                        {"åŸºé‡‘ä»£ç ": fund_code, "æ—¥æœŸ": current_date},
                        {"$set": record},
                        upsert=True,
                    )
                )

                # è¿›åº¦å›è°ƒ
                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            if not ops:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")
                return 0

            result = await self.col_fund_money_fund_daily_em.bulk_write(ops, ordered=False)
            saved_count = (result.upserted_count or 0) + (result.modified_count or 0)

            logger.info(f"æˆåŠŸä¿å­˜ {saved_count} æ¡è´§å¸å‹åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®")
            return saved_count

        except Exception as e:
            logger.error(f"ä¿å­˜è´§å¸å‹åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def clear_fund_money_fund_daily_data(self) -> int:
        """æ¸…ç©ºè´§å¸å‹åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®

        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_money_fund_daily_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡è´§å¸å‹åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºè´§å¸å‹åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_fund_money_fund_daily_stats(self) -> Dict[str, Any]:
        """è·å–è´§å¸å‹åŸºé‡‘å®æ—¶è¡Œæƒ…ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…æ‹¬æ€»æ•°ã€å„åŸºé‡‘ä»£ç è®°å½•æ•°ã€æœ€æ—©/æœ€æ–°æ—¥æœŸ
        """
        try:
            total_count = await self.col_fund_money_fund_daily_em.count_documents({})

            # æŒ‰åŸºé‡‘ä»£ç ç»Ÿè®¡è®°å½•æ•°ï¼ˆTop 50ï¼‰
            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 50},
            ]
            fund_code_stats = await self.col_fund_money_fund_daily_em.aggregate(
                fund_code_pipeline
            ).to_list(50)

            # æŒ‰æ—¥æœŸç»Ÿè®¡è®°å½•æ•°
            date_pipeline = [
                {"$group": {"_id": "$date", "count": {"$sum": 1}}},
                {"$project": {"date": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"date": -1}},
                {"$limit": 30},
            ]
            date_stats = await self.col_fund_money_fund_daily_em.aggregate(
                date_pipeline
            ).to_list(30)

            # è·å–æœ€æ—©å’Œæœ€æ–°æ—¥æœŸ
            earliest_doc = (
                await self.col_fund_money_fund_daily_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_money_fund_daily_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            earliest_date = earliest_doc[0]["date"] if earliest_doc else None
            latest_date = latest_doc[0]["date"] if latest_doc else None

            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "date_stats": date_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            logger.debug(f"è´§å¸å‹åŸºé‡‘å®æ—¶è¡Œæƒ…ç»Ÿè®¡: {result}")
            return result

        except Exception as e:
            logger.error(f"è·å–è´§å¸å‹åŸºé‡‘å®æ—¶è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "date_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_money_fund_info_data(
        self, df: pd.DataFrame, fund_code: str, progress_callback=None
    ) -> int:
        """ä¿å­˜è´§å¸å‹åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®
        
        åªä¿ç•™6ä¸ªå­—æ®µï¼šåŸºé‡‘ä»£ç ã€æ—¥æœŸã€æ¯ä¸‡ä»½æ”¶ç›Šã€7æ—¥å¹´åŒ–æ”¶ç›Šç‡ã€ç”³è´­çŠ¶æ€ã€èµå›çŠ¶æ€

        Args:
            df: åŒ…å«å†å²è¡Œæƒ…æ•°æ®çš„ DataFrameï¼ˆä»AKShareè·å–ï¼‰
            fund_code: åŸºé‡‘ä»£ç 
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning(f"è´§å¸å‹åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®ä¸ºç©ºï¼ˆ{fund_code}ï¼‰")
            return 0

        try:
            df = df.copy()
            df.columns = df.columns.str.strip()

            # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºåŸå§‹æ•°æ®ç»“æ„
            logger.info(f"è´§å¸å‹åŸºé‡‘å†å²è¡Œæƒ…å­—æ®µ: {df.columns.tolist()}, æ•°æ®é‡: {len(df)}")

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if "å‡€å€¼æ—¥æœŸ" not in df.columns:
                logger.error(f"è´§å¸å‹åŸºé‡‘å†å²è¡Œæƒ…ç¼ºå°‘'å‡€å€¼æ—¥æœŸ'å­—æ®µ")
                return 0

            ops = []
            total = len(df)
            batch_size = 1000
            total_saved = 0

            for idx, row in df.iterrows():
                date_value = str(row.get("å‡€å€¼æ—¥æœŸ", "")).strip()
                if not date_value or date_value == "nan":
                    continue

                # åªä¿ç•™6ä¸ªå­—æ®µï¼šåŸºé‡‘ä»£ç ã€æ—¥æœŸã€æ¯ä¸‡ä»½æ”¶ç›Šã€7æ—¥å¹´åŒ–æ”¶ç›Šç‡ã€ç”³è´­çŠ¶æ€ã€èµå›çŠ¶æ€
                record = {
                    "åŸºé‡‘ä»£ç ": fund_code,
                    "æ—¥æœŸ": date_value,
                    "æ¯ä¸‡ä»½æ”¶ç›Š": float(row["æ¯ä¸‡ä»½æ”¶ç›Š"]) if pd.notna(row.get("æ¯ä¸‡ä»½æ”¶ç›Š")) else None,
                    "7æ—¥å¹´åŒ–æ”¶ç›Šç‡": float(row["7æ—¥å¹´åŒ–æ”¶ç›Šç‡"]) if pd.notna(row.get("7æ—¥å¹´åŒ–æ”¶ç›Šç‡")) else None,
                    "ç”³è´­çŠ¶æ€": str(row["ç”³è´­çŠ¶æ€"]).strip() if pd.notna(row.get("ç”³è´­çŠ¶æ€")) else None,
                    "èµå›çŠ¶æ€": str(row["èµå›çŠ¶æ€"]).strip() if pd.notna(row.get("èµå›çŠ¶æ€")) else None,
                }

                # å”¯ä¸€é”®ï¼šåŸºé‡‘ä»£ç  + æ—¥æœŸ
                ops.append(
                    UpdateOne(
                        {"åŸºé‡‘ä»£ç ": fund_code, "æ—¥æœŸ": date_value},
                        {"$set": record},
                        upsert=True,
                    )
                )

                # æ‰¹é‡ä¿å­˜ï¼šæ¯1000æ¡ä¿å­˜ä¸€æ¬¡
                if len(ops) >= batch_size:
                    result = await self.col_fund_money_fund_info_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.modified_count or 0)
                    total_saved += batch_saved
                    logger.info(f"å·²ä¿å­˜ {len(ops)} æ¡æ•°æ®ï¼Œç´¯è®¡ä¿å­˜ {total_saved} æ¡ï¼ˆ{fund_code}ï¼‰")
                    ops = []

                # è¿›åº¦å›è°ƒ
                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            # ä¿å­˜å‰©ä½™æ•°æ®
            if ops:
                result = await self.col_fund_money_fund_info_em.bulk_write(ops, ordered=False)
                batch_saved = (result.upserted_count or 0) + (result.modified_count or 0)
                total_saved += batch_saved
                logger.info(f"å·²ä¿å­˜å‰©ä½™ {len(ops)} æ¡æ•°æ®ï¼ˆ{fund_code}ï¼‰")

            if total_saved == 0:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")
                return 0

            logger.info(f"æˆåŠŸä¿å­˜ {total_saved} æ¡è´§å¸å‹åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®ï¼ˆ{fund_code}ï¼‰")
            return total_saved

        except Exception as e:
            logger.error(f"ä¿å­˜è´§å¸å‹åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def clear_fund_money_fund_info_data(self) -> int:
        """æ¸…ç©ºè´§å¸å‹åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®

        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_money_fund_info_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡è´§å¸å‹åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºè´§å¸å‹åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_fund_money_fund_info_stats(self) -> Dict[str, Any]:
        """è·å–è´§å¸å‹åŸºé‡‘å†å²è¡Œæƒ…ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_money_fund_info_em.count_documents({})

            # æŒ‰åŸºé‡‘ä»£ç ç»Ÿè®¡è®°å½•æ•°ï¼ˆTop 50ï¼‰
            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 50},
            ]
            fund_code_stats = await self.col_fund_money_fund_info_em.aggregate(
                fund_code_pipeline
            ).to_list(50)

            # è·å–æœ€æ—©å’Œæœ€æ–°æ—¥æœŸ
            earliest_doc = (
                await self.col_fund_money_fund_info_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_money_fund_info_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            earliest_date = earliest_doc[0]["date"] if earliest_doc else None
            latest_date = latest_doc[0]["date"] if latest_doc else None

            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            return result

        except Exception as e:
            logger.error(f"è·å–è´§å¸å‹åŸºé‡‘å†å²è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_financial_fund_daily_data(
        self, df: pd.DataFrame, progress_callback=None
    ) -> int:
        """ä¿å­˜ç†è´¢å‹åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®

        Args:
            df: åŒ…å«ç†è´¢å‹åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®çš„ DataFrame
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("ç†è´¢å‹åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®ä¸ºç©º")
            return 0

        try:
            df = df.copy()
            from datetime import datetime
            current_date = datetime.now().strftime("%Y-%m-%d")
            df.columns = df.columns.str.strip()

            field_map = {
                "åºå·": "sequence",
                "åŸºé‡‘ä»£ç ": "fund_code",
                "åŸºé‡‘ç®€ç§°": "fund_name",
                "ä¸Šä¸€æœŸå¹´åŒ–æ”¶ç›Šç‡": "last_period_annual_yield",
                "å½“å‰äº¤æ˜“æ—¥-ä¸‡ä»½æ”¶ç›Š": "current_daily_profit_per_10k",
                "å½“å‰äº¤æ˜“æ—¥-7æ—¥å¹´å": "current_7day_annual_yield",
                "å‰ä¸€ä¸ªäº¤æ˜“æ—¥-ä¸‡ä»½æ”¶ç›Š": "prev_daily_profit_per_10k",
                "å‰ä¸€ä¸ªäº¤æ˜“æ—¥-7æ—¥å¹´å": "prev_7day_annual_yield",
                "å°é—­æœŸ": "closed_period",
                "ç”³è´­çŠ¶æ€": "purchase_status",
            }

            ops = []
            total = len(df)

            for idx, row in df.iterrows():
                fund_code = str(row.get("åŸºé‡‘ä»£ç ", "")).strip()
                if not fund_code or fund_code == "nan":
                    continue

                record = {"fund_code": fund_code, "date": current_date}

                for cn_field, en_field in field_map.items():
                    if cn_field == "åŸºé‡‘ä»£ç ":
                        continue
                    
                    value = row.get(cn_field)
                    if pd.notna(value):
                        value_str = str(value).strip()
                        if value_str and value_str != "---" and value_str != "nan":
                            try:
                                if isinstance(value, (int, float)):
                                    record[en_field] = float(value)
                                else:
                                    record[en_field] = value_str
                            except:
                                record[en_field] = value_str

                ops.append(
                    UpdateOne(
                        {"fund_code": fund_code, "date": current_date},
                        {"$set": record},
                        upsert=True,
                    )
                )

                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            if not ops:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")
                return 0

            result = await self.col_fund_financial_fund_daily_em.bulk_write(ops, ordered=False)
            saved_count = (result.upserted_count or 0) + (result.modified_count or 0)

            logger.info(f"æˆåŠŸä¿å­˜ {saved_count} æ¡ç†è´¢å‹åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®")
            return saved_count

        except Exception as e:
            logger.error(f"ä¿å­˜ç†è´¢å‹åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def clear_fund_financial_fund_daily_data(self) -> int:
        """æ¸…ç©ºç†è´¢å‹åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®

        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_financial_fund_daily_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡ç†è´¢å‹åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºç†è´¢å‹åŸºé‡‘å®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_fund_financial_fund_daily_stats(self) -> Dict[str, Any]:
        """è·å–ç†è´¢å‹åŸºé‡‘å®æ—¶è¡Œæƒ…ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…æ‹¬æ€»æ•°ã€å„åŸºé‡‘ä»£ç è®°å½•æ•°ã€æœ€æ—©/æœ€æ–°æ—¥æœŸ
        """
        try:
            total_count = await self.col_fund_financial_fund_daily_em.count_documents({})

            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 50},
            ]
            fund_code_stats = await self.col_fund_financial_fund_daily_em.aggregate(
                fund_code_pipeline
            ).to_list(50)

            date_pipeline = [
                {"$group": {"_id": "$date", "count": {"$sum": 1}}},
                {"$project": {"date": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"date": -1}},
                {"$limit": 30},
            ]
            date_stats = await self.col_fund_financial_fund_daily_em.aggregate(
                date_pipeline
            ).to_list(30)

            earliest_doc = (
                await self.col_fund_financial_fund_daily_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_financial_fund_daily_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            earliest_date = earliest_doc[0]["date"] if earliest_doc else None
            latest_date = latest_doc[0]["date"] if latest_doc else None

            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "date_stats": date_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            logger.debug(f"ç†è´¢å‹åŸºé‡‘å®æ—¶è¡Œæƒ…ç»Ÿè®¡: {result}")
            return result

        except Exception as e:
            logger.error(f"è·å–ç†è´¢å‹åŸºé‡‘å®æ—¶è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "date_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_financial_fund_info_data(
        self, df: pd.DataFrame, fund_code: str, progress_callback=None
    ) -> int:
        """ä¿å­˜ç†è´¢å‹åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®
        
        åªä¿ç•™8ä¸ªå­—æ®µï¼šåŸºé‡‘ä»£ç ã€æ—¥æœŸã€å•ä½å‡€å€¼ã€ç´¯è®¡å‡€å€¼ã€æ—¥å¢é•¿ç‡ã€ç”³è´­çŠ¶æ€ã€èµå›çŠ¶æ€ã€åˆ†çº¢é€é…

        Args:
            df: åŒ…å«å†å²è¡Œæƒ…æ•°æ®çš„ DataFrameï¼ˆä»AKShareè·å–ï¼‰
            fund_code: åŸºé‡‘ä»£ç 
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning(f"ç†è´¢å‹åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®ä¸ºç©ºï¼ˆ{fund_code}ï¼‰")
            return 0

        try:
            df = df.copy()
            df.columns = df.columns.str.strip()

            # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºåŸå§‹æ•°æ®ç»“æ„
            logger.info(f"ç†è´¢å‹åŸºé‡‘å†å²è¡Œæƒ…å­—æ®µ: {df.columns.tolist()}, æ•°æ®é‡: {len(df)}")

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if "å‡€å€¼æ—¥æœŸ" not in df.columns:
                logger.error(f"ç†è´¢å‹åŸºé‡‘å†å²è¡Œæƒ…ç¼ºå°‘'å‡€å€¼æ—¥æœŸ'å­—æ®µ")
                return 0

            ops = []
            total = len(df)
            batch_size = 1000
            total_saved = 0

            for idx, row in df.iterrows():
                date_value = str(row.get("å‡€å€¼æ—¥æœŸ", "")).strip()
                if not date_value or date_value == "nan":
                    continue

                # åªä¿ç•™8ä¸ªå­—æ®µï¼šåŸºé‡‘ä»£ç ã€æ—¥æœŸã€å•ä½å‡€å€¼ã€ç´¯è®¡å‡€å€¼ã€æ—¥å¢é•¿ç‡ã€ç”³è´­çŠ¶æ€ã€èµå›çŠ¶æ€ã€åˆ†çº¢é€é…
                record = {
                    "åŸºé‡‘ä»£ç ": fund_code,
                    "æ—¥æœŸ": date_value,
                    "å•ä½å‡€å€¼": float(row["å•ä½å‡€å€¼"]) if pd.notna(row.get("å•ä½å‡€å€¼")) else None,
                    "ç´¯è®¡å‡€å€¼": float(row["ç´¯è®¡å‡€å€¼"]) if pd.notna(row.get("ç´¯è®¡å‡€å€¼")) else None,
                    "æ—¥å¢é•¿ç‡": str(row["æ—¥å¢é•¿ç‡"]).strip() if pd.notna(row.get("æ—¥å¢é•¿ç‡")) else None,
                    "ç”³è´­çŠ¶æ€": str(row["ç”³è´­çŠ¶æ€"]).strip() if pd.notna(row.get("ç”³è´­çŠ¶æ€")) else None,
                    "èµå›çŠ¶æ€": str(row["èµå›çŠ¶æ€"]).strip() if pd.notna(row.get("èµå›çŠ¶æ€")) else None,
                    "åˆ†çº¢é€é…": str(row["åˆ†çº¢é€é…"]).strip() if pd.notna(row.get("åˆ†çº¢é€é…")) else None,
                }

                # å”¯ä¸€é”®ï¼šåŸºé‡‘ä»£ç  + æ—¥æœŸ
                ops.append(
                    UpdateOne(
                        {"åŸºé‡‘ä»£ç ": fund_code, "æ—¥æœŸ": date_value},
                        {"$set": record},
                        upsert=True,
                    )
                )

                # æ‰¹é‡ä¿å­˜ï¼šæ¯1000æ¡ä¿å­˜ä¸€æ¬¡
                if len(ops) >= batch_size:
                    result = await self.col_fund_financial_fund_info_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.modified_count or 0)
                    total_saved += batch_saved
                    logger.info(f"å·²ä¿å­˜ {len(ops)} æ¡æ•°æ®ï¼Œç´¯è®¡ä¿å­˜ {total_saved} æ¡ï¼ˆ{fund_code}ï¼‰")
                    ops = []

                # è¿›åº¦å›è°ƒ
                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            # ä¿å­˜å‰©ä½™æ•°æ®
            if ops:
                result = await self.col_fund_financial_fund_info_em.bulk_write(ops, ordered=False)
                batch_saved = (result.upserted_count or 0) + (result.modified_count or 0)
                total_saved += batch_saved
                logger.info(f"å·²ä¿å­˜å‰©ä½™ {len(ops)} æ¡æ•°æ®ï¼ˆ{fund_code}ï¼‰")

            if total_saved == 0:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")
                return 0

            logger.info(f"æˆåŠŸä¿å­˜ {total_saved} æ¡ç†è´¢å‹åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®ï¼ˆ{fund_code}ï¼‰")
            return total_saved

        except Exception as e:
            logger.error(f"ä¿å­˜ç†è´¢å‹åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def clear_fund_financial_fund_info_data(self) -> int:
        """æ¸…ç©ºç†è´¢å‹åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®

        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_financial_fund_info_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡ç†è´¢å‹åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºç†è´¢å‹åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_fund_financial_fund_info_stats(self) -> Dict[str, Any]:
        """è·å–ç†è´¢å‹åŸºé‡‘å†å²è¡Œæƒ…ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_financial_fund_info_em.count_documents({})

            # æŒ‰åŸºé‡‘ä»£ç ç»Ÿè®¡è®°å½•æ•°ï¼ˆTop 50ï¼‰
            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 50},
            ]
            fund_code_stats = await self.col_fund_financial_fund_info_em.aggregate(
                fund_code_pipeline
            ).to_list(50)

            # è·å–æœ€æ—©å’Œæœ€æ–°æ—¥æœŸ
            earliest_doc = (
                await self.col_fund_financial_fund_info_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_financial_fund_info_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            earliest_date = earliest_doc[0]["date"] if earliest_doc else None
            latest_date = latest_doc[0]["date"] if latest_doc else None

            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            return result

        except Exception as e:
            logger.error(f"è·å–ç†è´¢å‹åŸºé‡‘å†å²è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_graded_fund_daily_data(
        self, df: pd.DataFrame, progress_callback=None
    ) -> int:
        """ä¿å­˜åˆ†çº§åŸºé‡‘å®æ—¶æ•°æ®

        Args:
            df: åŒ…å«åˆ†çº§åŸºé‡‘å®æ—¶æ•°æ®çš„ DataFrame
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("åˆ†çº§åŸºé‡‘å®æ—¶æ•°æ®ä¸ºç©º")
            return 0

        try:
            df = df.copy()
            from datetime import datetime
            current_date = datetime.now().strftime("%Y-%m-%d")
            df.columns = df.columns.str.strip()

            field_map = {
                "åŸºé‡‘ä»£ç ": "fund_code",
                "åŸºé‡‘ç®€ç§°": "fund_name",
                "å•ä½å‡€å€¼": "unit_net_value",
                "ç´¯è®¡å‡€å€¼": "accumulative_net_value",
                "å‰äº¤æ˜“æ—¥-å•ä½å‡€å€¼": "prev_unit_net_value",
                "å‰äº¤æ˜“æ—¥-ç´¯è®¡å‡€å€¼": "prev_accumulative_net_value",
                "æ—¥å¢é•¿å€¼": "daily_growth_value",
                "æ—¥å¢é•¿ç‡": "daily_growth_rate",
                "å¸‚ä»·": "market_price",
                "æŠ˜ä»·ç‡": "discount_rate",
                "æ‰‹ç»­è´¹": "fee",
            }

            ops = []
            total = len(df)

            for idx, row in df.iterrows():
                fund_code = str(row.get("åŸºé‡‘ä»£ç ", "")).strip()
                if not fund_code or fund_code == "nan":
                    continue

                record = {"fund_code": fund_code, "date": current_date}

                for cn_field, en_field in field_map.items():
                    if cn_field == "åŸºé‡‘ä»£ç ":
                        continue
                    value = row.get(cn_field)
                    if pd.notna(value):
                        value_str = str(value).strip()
                        if value_str and value_str != "---" and value_str != "nan":
                            try:
                                if isinstance(value, (int, float)):
                                    record[en_field] = float(value)
                                else:
                                    record[en_field] = value_str
                            except:
                                record[en_field] = value_str

                ops.append(
                    UpdateOne(
                        {"fund_code": fund_code, "date": current_date},
                        {"$set": record},
                        upsert=True,
                    )
                )

                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            if not ops:
                return 0

            result = await self.col_fund_graded_fund_daily_em.bulk_write(ops, ordered=False)
            saved_count = (result.upserted_count or 0) + (result.modified_count or 0)
            logger.info(f"æˆåŠŸä¿å­˜ {saved_count} æ¡åˆ†çº§åŸºé‡‘å®æ—¶æ•°æ®")
            return saved_count

        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†çº§åŸºé‡‘å®æ—¶æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def clear_fund_graded_fund_daily_data(self) -> int:
        """æ¸…ç©ºåˆ†çº§åŸºé‡‘å®æ—¶æ•°æ®

        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_graded_fund_daily_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åˆ†çº§åŸºé‡‘å®æ—¶æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåˆ†çº§åŸºé‡‘å®æ—¶æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_fund_graded_fund_daily_stats(self) -> Dict[str, Any]:
        """è·å–åˆ†çº§åŸºé‡‘å®æ—¶æ•°æ®ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_graded_fund_daily_em.count_documents({})

            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 50},
            ]
            fund_code_stats = await self.col_fund_graded_fund_daily_em.aggregate(
                fund_code_pipeline
            ).to_list(50)

            date_pipeline = [
                {"$group": {"_id": "$date", "count": {"$sum": 1}}},
                {"$project": {"date": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"date": -1}},
                {"$limit": 30},
            ]
            date_stats = await self.col_fund_graded_fund_daily_em.aggregate(
                date_pipeline
            ).to_list(30)

            earliest_doc = (
                await self.col_fund_graded_fund_daily_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_graded_fund_daily_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "date_stats": date_stats,
                "earliest_date": earliest_doc[0]["date"] if earliest_doc else None,
                "latest_date": latest_doc[0]["date"] if latest_doc else None,
            }

            return result

        except Exception as e:
            logger.error(f"è·å–åˆ†çº§åŸºé‡‘å®æ—¶æ•°æ®ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "date_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_graded_fund_info_data(
        self, df: pd.DataFrame, fund_code: str, progress_callback=None
    ) -> int:
        """ä¿å­˜åˆ†çº§åŸºé‡‘å†å²æ•°æ®

        Args:
            df: åŒ…å«å†å²æ•°æ®çš„ DataFrame
            fund_code: åŸºé‡‘ä»£ç 
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning(f"åˆ†çº§åŸºé‡‘å†å²æ•°æ®ä¸ºç©ºï¼ˆ{fund_code}ï¼‰")
            return 0

        try:
            df = df.copy()
            df.columns = df.columns.str.strip()

            # å­—æ®µæ˜ å°„
            field_map = {
                "å‡€å€¼æ—¥æœŸ": "date",
                "å•ä½å‡€å€¼": "unit_net_value",
                "ç´¯è®¡å‡€å€¼": "accumulative_net_value",
                "æ—¥å¢é•¿ç‡": "daily_growth_rate",
                "ç”³è´­çŠ¶æ€": "purchase_status",
                "èµå›çŠ¶æ€": "redemption_status",
            }

            ops = []
            total = len(df)

            for idx, row in df.iterrows():
                date_value = str(row.get("å‡€å€¼æ—¥æœŸ", "")).strip()
                if not date_value or date_value == "nan":
                    continue

                record = {
                    "fund_code": fund_code,
                    "date": date_value,
                }

                # æ˜ å°„å…¶ä»–å­—æ®µ
                for cn_field, en_field in field_map.items():
                    if cn_field == "å‡€å€¼æ—¥æœŸ":
                        continue
                    value = row.get(cn_field)
                    if pd.notna(value):
                        if isinstance(value, (int, float)):
                            record[en_field] = float(value)
                        else:
                            record[en_field] = str(value).strip()

                ops.append(
                    UpdateOne(
                        {"fund_code": fund_code, "date": date_value},
                        {"$set": record},
                        upsert=True,
                    )
                )

                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            if not ops:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")
                return 0

            result = await self.col_fund_graded_fund_info_em.bulk_write(ops, ordered=False)
            saved_count = (result.upserted_count or 0) + (result.modified_count or 0)

            logger.info(f"æˆåŠŸä¿å­˜ {saved_count} æ¡åˆ†çº§åŸºé‡‘å†å²æ•°æ®ï¼ˆ{fund_code}ï¼‰")
            return saved_count

        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†çº§åŸºé‡‘å†å²æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def clear_fund_graded_fund_info_data(self) -> int:
        """æ¸…ç©ºåˆ†çº§åŸºé‡‘å†å²æ•°æ®

        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_graded_fund_info_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åˆ†çº§åŸºé‡‘å†å²æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåˆ†çº§åŸºé‡‘å†å²æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_fund_graded_fund_info_stats(self) -> Dict[str, Any]:
        """è·å–åˆ†çº§åŸºé‡‘å†å²æ•°æ®ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_graded_fund_info_em.count_documents({})

            # æŒ‰åŸºé‡‘ä»£ç ç»Ÿè®¡è®°å½•æ•°ï¼ˆTop 50ï¼‰
            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 50},
            ]
            fund_code_stats = await self.col_fund_graded_fund_info_em.aggregate(
                fund_code_pipeline
            ).to_list(50)

            # è·å–æœ€æ—©å’Œæœ€æ–°æ—¥æœŸ
            earliest_doc = (
                await self.col_fund_graded_fund_info_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_graded_fund_info_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            earliest_date = earliest_doc[0]["date"] if earliest_doc else None
            latest_date = latest_doc[0]["date"] if latest_doc else None

            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            return result

        except Exception as e:
            logger.error(f"è·å–åˆ†çº§åŸºé‡‘å†å²æ•°æ®ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_etf_fund_daily_data(
        self, df: pd.DataFrame, progress_callback=None
    ) -> int:
        """ä¿å­˜åœºå†…äº¤æ˜“åŸºé‡‘å®æ—¶æ•°æ®
        
        åªä¿ç•™10ä¸ªå­—æ®µï¼Œä½¿ç”¨ä¸­æ–‡å­—æ®µåï¼šåŸºé‡‘ä»£ç ã€åŸºé‡‘ç®€ç§°ã€ç±»å‹ã€æ—¥æœŸã€å•ä½å‡€å€¼ã€ç´¯è®¡å‡€å€¼ã€å¢é•¿å€¼ã€å¢é•¿ç‡ã€å¸‚ä»·ã€æŠ˜ä»·ç‡

        Args:
            df: åŒ…å«åœºå†…äº¤æ˜“åŸºé‡‘å®æ—¶æ•°æ®çš„ DataFrameï¼ˆä»AKShareè·å–ï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("åœºå†…äº¤æ˜“åŸºé‡‘å®æ—¶æ•°æ®ä¸ºç©º")
            return 0

        try:
            import numpy as np
            # æ¸…ç†æ— æ•ˆçš„æµ®ç‚¹æ•°å€¼
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            df = df.copy()
            
            current_date = datetime.now().strftime("%Y-%m-%d")
            df.columns = df.columns.str.strip()

            total = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total} æ¡åœºå†…äº¤æ˜“åŸºé‡‘å®æ—¶æ•°æ®...")
            logger.info(f"ğŸ“‹ åŸå§‹å­—æ®µ: {df.columns.tolist()[:20]}...")  # åªæ˜¾ç¤ºå‰20ä¸ªå­—æ®µ
            
            # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹500æ¡
            batch_size = 500
            total_saved = 0
            total_batches = (total + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"ğŸ“ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ï¼Œè®°å½•èŒƒå›´: {start_idx + 1}-{end_idx}")
                
                ops = []
                for idx, row in batch_df.iterrows():
                    fund_code = str(row.get("åŸºé‡‘ä»£ç ", "")).strip()
                    if not fund_code or fund_code == "nan":
                        continue

                    # ä½¿ç”¨ä¸­æ–‡å­—æ®µåä¿å­˜ï¼Œåªä¿ç•™10ä¸ªå›ºå®šå­—æ®µ
                    record = {
                        "åŸºé‡‘ä»£ç ": fund_code,
                        "æ—¥æœŸ": current_date,
                        "åŸºé‡‘ç®€ç§°": None,
                        "ç±»å‹": None,
                        "å•ä½å‡€å€¼": None,
                        "ç´¯è®¡å‡€å€¼": None,
                        "å¢é•¿å€¼": None,
                        "å¢é•¿ç‡": None,
                        "å¸‚ä»·": None,
                        "æŠ˜ä»·ç‡": None,
                    }

                    # é™æ€å­—æ®µï¼šç›´æ¥æ˜ å°„
                    static_fields = ["åŸºé‡‘ç®€ç§°", "ç±»å‹", "å¢é•¿å€¼", "å¢é•¿ç‡", "å¸‚ä»·", "æŠ˜ä»·ç‡"]
                    for field in static_fields:
                        value = row.get(field)
                        if pd.notna(value):
                            value_str = str(value).strip()
                            if value_str and value_str != "---" and value_str != "nan":
                                try:
                                    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                                    if isinstance(value, (int, float)):
                                        record[field] = float(value)
                                    else:
                                        record[field] = value_str
                                except:
                                    record[field] = value_str

                    # åŠ¨æ€æ—¥æœŸå­—æ®µï¼šå»æ‰æ—¥æœŸéƒ¨åˆ†ï¼Œåªä¿ç•™"å•ä½å‡€å€¼"å’Œ"ç´¯è®¡å‡€å€¼"
                    # ä»æ‰€æœ‰åˆ—ä¸­æŸ¥æ‰¾åŒ…å«"-å•ä½å‡€å€¼"å’Œ"-ç´¯è®¡å‡€å€¼"çš„åˆ—ï¼Œå–æœ€åä¸€ä¸ªï¼ˆæœ€æ–°æ—¥æœŸï¼‰
                    unit_net_value_cols = [col for col in df.columns if "-å•ä½å‡€å€¼" in str(col)]
                    accumulative_net_value_cols = [col for col in df.columns if "-ç´¯è®¡å‡€å€¼" in str(col)]
                    
                    # å–æœ€åä¸€åˆ—ä½œä¸ºå½“å‰å‡€å€¼
                    if unit_net_value_cols:
                        last_unit_col = unit_net_value_cols[-1]
                        value = row.get(last_unit_col)
                        if pd.notna(value) and str(value).strip() not in ["", "---", "nan"]:
                            try:
                                record["å•ä½å‡€å€¼"] = float(value)
                            except (ValueError, TypeError):
                                pass
                    
                    if accumulative_net_value_cols:
                        last_acc_col = accumulative_net_value_cols[-1]
                        value = row.get(last_acc_col)
                        if pd.notna(value) and str(value).strip() not in ["", "---", "nan"]:
                            try:
                                record["ç´¯è®¡å‡€å€¼"] = float(value)
                            except (ValueError, TypeError):
                                pass

                    # å”¯ä¸€é”®ï¼šåŸºé‡‘ä»£ç  + æ—¥æœŸ
                    ops.append(
                        UpdateOne(
                            {"åŸºé‡‘ä»£ç ": fund_code, "æ—¥æœŸ": current_date},
                            {"$set": record},
                            upsert=True,
                        )
                    )

                # æ‰§è¡Œæ‰¹é‡å†™å…¥
                if ops:
                    result = await self.col_fund_etf_fund_daily_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"âœ… ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹å†™å…¥å®Œæˆ: "
                        f"æ–°å¢={result.upserted_count}, æ›´æ–°={result.matched_count}, "
                        f"æœ¬æ‰¹ä¿å­˜={batch_saved}, ç´¯è®¡={total_saved}/{total}"
                    )
                    
                    if progress_callback:
                        progress = int((end_idx / total) * 100)
                        await progress_callback(
                            current=end_idx,
                            total=total,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total} æ¡æ•°æ® ({progress}%)"
                        )

            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total} æ¡åœºå†…äº¤æ˜“åŸºé‡‘å®æ—¶æ•°æ®")
            return total_saved

        except Exception as e:
            logger.error(f"ä¿å­˜åœºå†…äº¤æ˜“åŸºé‡‘å®æ—¶æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def clear_fund_etf_fund_daily_data(self) -> int:
        """æ¸…ç©ºåœºå†…äº¤æ˜“åŸºé‡‘å®æ—¶æ•°æ®

        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_etf_fund_daily_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åœºå†…äº¤æ˜“åŸºé‡‘å®æ—¶æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåœºå†…äº¤æ˜“åŸºé‡‘å®æ—¶æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise

    async def get_fund_etf_fund_daily_stats(self) -> Dict[str, Any]:
        """è·å–åœºå†…äº¤æ˜“åŸºé‡‘å®æ—¶æ•°æ®ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_etf_fund_daily_em.count_documents({})

            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 50},
            ]
            fund_code_stats = await self.col_fund_etf_fund_daily_em.aggregate(
                fund_code_pipeline
            ).to_list(50)

            date_pipeline = [
                {"$group": {"_id": "$date", "count": {"$sum": 1}}},
                {"$project": {"date": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"date": -1}},
                {"$limit": 30},
            ]
            date_stats = await self.col_fund_etf_fund_daily_em.aggregate(
                date_pipeline
            ).to_list(30)

            earliest_doc = (
                await self.col_fund_etf_fund_daily_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_etf_fund_daily_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "date_stats": date_stats,
                "earliest_date": earliest_doc[0]["date"] if earliest_doc else None,
                "latest_date": latest_doc[0]["date"] if latest_doc else None,
            }

            return result

        except Exception as e:
            logger.error(f"è·å–åœºå†…äº¤æ˜“åŸºé‡‘å®æ—¶æ•°æ®ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "date_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }
    
    # ========== é¦™æ¸¯åŸºé‡‘å†å²æ•°æ® ==========
    async def save_fund_hk_hist_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜é¦™æ¸¯åŸºé‡‘å†å²æ•°æ®ï¼ˆå†å²å‡€å€¼æ˜ç»†æˆ–åˆ†çº¢é€é…è¯¦æƒ…ï¼‰
        
        Args:
            df: åŒ…å«é¦™æ¸¯åŸºé‡‘å†å²æ•°æ®çš„DataFrame
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰é¦™æ¸¯åŸºé‡‘å†å²æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡é¦™æ¸¯åŸºé‡‘å†å²æ•°æ®...")
            
            # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹500æ¡
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"ğŸ“ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ï¼Œè®°å½•èŒƒå›´: {start_idx + 1}-{end_idx}")
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # æ¸…ç†NaN/Infinityå€¼
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_code = str(doc.get('code', ''))
                    symbol = str(doc.get('symbol', 'å†å²å‡€å€¼æ˜ç»†'))
                    
                    # ç¡®å®šæ—¥æœŸå­—æ®µ
                    date_field = None
                    if 'å‡€å€¼æ—¥æœŸ' in doc:
                        date_field = str(doc.get('å‡€å€¼æ—¥æœŸ', ''))
                    elif 'é™¤æ¯æ—¥' in doc:
                        date_field = str(doc.get('é™¤æ¯æ—¥', ''))
                    
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_hk_fund_hist_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # æ„å»ºå”¯ä¸€æ ‡è¯†ï¼ˆcode + date + symbolï¼‰
                    filter_query = {'code': fund_code, 'symbol': symbol}
                    if date_field:
                        filter_query['date'] = date_field
                        doc['date'] = date_field
                    
                    ops.append(
                        UpdateOne(
                            filter_query,
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # æ‰§è¡Œæ‰¹é‡å†™å…¥
                if ops:
                    result = await self.col_fund_hk_hist_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"âœ… ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹å†™å…¥å®Œæˆ: "
                        f"æ–°å¢={result.upserted_count}, æ›´æ–°={result.matched_count}, "
                        f"æœ¬æ‰¹ä¿å­˜={batch_saved}, ç´¯è®¡={total_saved}/{total_count}"
                    )
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        await progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡é¦™æ¸¯åŸºé‡‘å†å²æ•°æ®")
            return total_saved
            
        except Exception as e:
            logger.error(f"ä¿å­˜é¦™æ¸¯åŸºé‡‘å†å²æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_hk_hist_em_stats(self) -> Dict[str, Any]:
        """è·å–é¦™æ¸¯åŸºé‡‘å†å²æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_hk_hist_em.count_documents({})
            
            # åŸºé‡‘æ•°é‡ç»Ÿè®¡
            fund_count_pipeline = [
                {"$group": {"_id": "$code"}},
                {"$count": "count"}
            ]
            fund_count_result = await self.col_fund_hk_hist_em.aggregate(fund_count_pipeline).to_list(1)
            fund_count = fund_count_result[0]["count"] if fund_count_result else 0
            
            # symbolåˆ†å¸ƒç»Ÿè®¡
            symbol_pipeline = [
                {"$group": {"_id": "$symbol", "count": {"$sum": 1}}},
                {"$project": {"symbol": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}}
            ]
            symbol_stats = await self.col_fund_hk_hist_em.aggregate(symbol_pipeline).to_list(10)
            
            # æ—¥æœŸèŒƒå›´ç»Ÿè®¡
            earliest_doc = (
                await self.col_fund_hk_hist_em.find({"date": {"$exists": True, "$ne": None}}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_hk_hist_em.find({"date": {"$exists": True, "$ne": None}}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )
            
            # åŸºé‡‘ä»£ç åˆ†å¸ƒ
            fund_code_pipeline = [
                {"$group": {"_id": "$code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 20}
            ]
            fund_code_stats = await self.col_fund_hk_hist_em.aggregate(fund_code_pipeline).to_list(20)
            
            result = {
                "total_count": total_count,
                "fund_count": fund_count,
                "symbol_distribution": symbol_stats,
                "fund_code_stats": fund_code_stats,
                "earliest_date": earliest_doc[0]["date"] if earliest_doc else None,
                "latest_date": latest_doc[0]["date"] if latest_doc else None,
            }
            
            return result
            
        except Exception as e:
            logger.error(f"è·å–é¦™æ¸¯åŸºé‡‘å†å²æ•°æ®ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_count": 0,
                "symbol_distribution": [],
                "fund_code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }
    
    async def import_fund_hk_hist_em_from_file(self, content: bytes, filename: Optional[str] = None) -> Dict[str, Any]:
        """ä»æ–‡ä»¶å¯¼å…¥é¦™æ¸¯åŸºé‡‘å†å²æ•°æ®
        
        Args:
            content: æ–‡ä»¶å†…å®¹ï¼ˆå­—èŠ‚ï¼‰
            filename: æ–‡ä»¶å
            
        Returns:
            å¯¼å…¥ç»“æœå­—å…¸
        """
        if not content:
            raise ValueError("ä¸Šä¼ æ–‡ä»¶ä¸ºç©º")
        
        name = (filename or "").lower()
        buffer = io.BytesIO(content)
        
        df: Optional[pd.DataFrame] = None
        try:
            if name.endswith(".csv") or name.endswith(".txt"):
                df = pd.read_csv(buffer)
            elif name.endswith(".xls") or name.endswith(".xlsx"):
                df = pd.read_excel(buffer)
            else:
                try:
                    df = pd.read_csv(buffer)
                except Exception:
                    buffer.seek(0)
                    df = pd.read_excel(buffer)
        except Exception as e:
            logger.error(f"âŒ [fund_hk_hist_em å¯¼å…¥] è¯»å–æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
            raise ValueError("æ— æ³•è§£æä¸Šä¼ æ–‡ä»¶ï¼Œè¯·ç¡®è®¤ä¸ºæœ‰æ•ˆçš„ CSV æˆ– Excel æ–‡ä»¶")
        
        if df is None or df.empty:
            logger.warning("âš ï¸ [fund_hk_hist_em å¯¼å…¥] è§£æç»“æœä¸ºç©º DataFrame")
            return {"saved": 0, "rows": 0}
        
        rows = len(df)
        saved = await self.save_fund_hk_hist_em_data(df)
        logger.info(f"ğŸ’¾ [fund_hk_hist_em å¯¼å…¥] ä»æ–‡ä»¶ {filename} å¯¼å…¥ {rows} è¡Œï¼Œä¿å­˜ {saved} æ¡è®°å½•")
        
        return {"saved": saved, "rows": rows}
    
    async def sync_fund_hk_hist_em_from_remote(
        self,
        remote_host: str,
        batch_size: int = 1000,
        remote_collection: Optional[str] = None,
        remote_username: Optional[str] = None,
        remote_password: Optional[str] = None,
        remote_auth_source: Optional[str] = None,
    ) -> Dict[str, Any]:
        """ä»è¿œç¨‹MongoDBåŒæ­¥é¦™æ¸¯åŸºé‡‘å†å²æ•°æ®
        
        Args:
            remote_host: è¿œç¨‹ä¸»æœºåœ°å€
            batch_size: æ‰¹é‡å¤§å°
            remote_collection: è¿œç¨‹é›†åˆåç§°
            remote_username: è¿œç¨‹ç”¨æˆ·å
            remote_password: è¿œç¨‹å¯†ç 
            remote_auth_source: è®¤è¯æ•°æ®åº“
            
        Returns:
            åŒæ­¥ç»“æœå­—å…¸
        """
        from motor.motor_asyncio import AsyncIOMotorClient
        
        if not remote_host:
            raise ValueError("è¿œç¨‹ä¸»æœºåœ°å€ä¸èƒ½ä¸ºç©º")
        
        # æ„å»ºè¿œç¨‹è¿æ¥URI
        if remote_username and remote_password:
            auth_source = remote_auth_source or self.db.name
            remote_uri = f"mongodb://{remote_username}:{remote_password}@{remote_host}/?authSource={auth_source}"
        else:
            remote_uri = f"mongodb://{remote_host}"
        
        remote_client = None
        try:
            remote_client = AsyncIOMotorClient(remote_uri)
            remote_db = remote_client[self.db.name]
            remote_col_name = remote_collection or "fund_hk_hist_em"
            remote_col = remote_db[remote_col_name]
            
            # è·å–è¿œç¨‹æ•°æ®æ€»æ•°
            remote_total = await remote_col.count_documents({})
            logger.info(f"ğŸ”— è¿œç¨‹é›†åˆ {remote_col_name} å…±æœ‰ {remote_total} æ¡æ•°æ®")
            
            if remote_total == 0:
                return {"remote_total": 0, "synced": 0, "message": "è¿œç¨‹é›†åˆä¸ºç©º"}
            
            # åˆ†æ‰¹åŒæ­¥
            synced = 0
            skip = 0
            
            while skip < remote_total:
                cursor = remote_col.find({}).skip(skip).limit(batch_size)
                batch_docs = await cursor.to_list(batch_size)
                
                if not batch_docs:
                    break
                
                # è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
                df = pd.DataFrame(batch_docs)
                if '_id' in df.columns:
                    df = df.drop('_id', axis=1)
                
                batch_saved = await self.save_fund_hk_hist_em_data(df)
                synced += batch_saved
                skip += len(batch_docs)
                
                logger.info(f"ğŸ“¥ å·²åŒæ­¥ {skip}/{remote_total} æ¡æ•°æ®")
            
            logger.info(f"âœ… åŒæ­¥å®Œæˆ: è¿œç¨‹ {remote_total} æ¡ï¼Œæœ¬åœ°ä¿å­˜/æ›´æ–° {synced} æ¡")
            
            return {
                "remote_total": remote_total,
                "synced": synced,
                "message": f"æˆåŠŸåŒæ­¥ {synced} æ¡é¦™æ¸¯åŸºé‡‘å†å²æ•°æ®"
            }
            
        except Exception as e:
            logger.error(f"âŒ ä»è¿œç¨‹åŒæ­¥é¦™æ¸¯åŸºé‡‘å†å²æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
        finally:
            if remote_client:
                remote_client.close()
    
    async def clear_fund_hk_hist_em_data(self) -> int:
        """æ¸…ç©ºé¦™æ¸¯åŸºé‡‘å†å²æ•°æ®
        
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_hk_hist_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡é¦™æ¸¯åŸºé‡‘å†å²æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºé¦™æ¸¯åŸºé‡‘å†å²æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    # ========== åœºå†…äº¤æ˜“åŸºé‡‘å†å²è¡Œæƒ… ==========
    async def save_fund_etf_fund_info_data(self, df: pd.DataFrame, fund_code: str = None, progress_callback=None) -> int:
        """ä¿å­˜åœºå†…äº¤æ˜“åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®
        
        åªä¿ç•™7ä¸ªå­—æ®µï¼Œä½¿ç”¨ä¸­æ–‡å­—æ®µåï¼šåŸºé‡‘ä»£ç ã€æ—¥æœŸã€å•ä½å‡€å€¼ã€ç´¯è®¡å‡€å€¼ã€æ—¥å¢é•¿ç‡ã€ç”³è´­çŠ¶æ€ã€èµå›çŠ¶æ€
        
        Args:
            df: åŒ…å«å†å²è¡Œæƒ…æ•°æ®çš„DataFrameï¼ˆä»AKShareè·å–ï¼‰
            fund_code: åŸºé‡‘ä»£ç 
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åœºå†…äº¤æ˜“åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            df = df.copy()
            df.columns = df.columns.str.strip()
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åœºå†…äº¤æ˜“åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®...")
            logger.info(f"ğŸ“‹ åŸå§‹å­—æ®µ: {df.columns.tolist()}")
            
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if "å‡€å€¼æ—¥æœŸ" not in df.columns:
                logger.error("åœºå†…äº¤æ˜“åŸºé‡‘å†å²è¡Œæƒ…ç¼ºå°‘'å‡€å€¼æ—¥æœŸ'å­—æ®µ")
                return 0
            
            # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹1000æ¡
            batch_size = 1000
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"ğŸ“ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ï¼Œè®°å½•èŒƒå›´: {start_idx + 1}-{end_idx}")
                
                ops = []
                for idx, row in batch_df.iterrows():
                    # è·å–æ—¥æœŸå­—æ®µ
                    date_value = str(row.get("å‡€å€¼æ—¥æœŸ", "")).strip()
                    if not date_value or date_value == "nan":
                        continue
                    
                    # è·å–åŸºé‡‘ä»£ç 
                    code = fund_code if fund_code else str(row.get("åŸºé‡‘ä»£ç ", "")).strip()
                    if not code or code == "nan":
                        continue
                    
                    # åªä¿ç•™7ä¸ªå­—æ®µï¼šåŸºé‡‘ä»£ç ã€æ—¥æœŸã€å•ä½å‡€å€¼ã€ç´¯è®¡å‡€å€¼ã€æ—¥å¢é•¿ç‡ã€ç”³è´­çŠ¶æ€ã€èµå›çŠ¶æ€
                    record = {
                        "åŸºé‡‘ä»£ç ": code,
                        "æ—¥æœŸ": date_value,
                        "å•ä½å‡€å€¼": float(row["å•ä½å‡€å€¼"]) if pd.notna(row.get("å•ä½å‡€å€¼")) else None,
                        "ç´¯è®¡å‡€å€¼": float(row["ç´¯è®¡å‡€å€¼"]) if pd.notna(row.get("ç´¯è®¡å‡€å€¼")) else None,
                        "æ—¥å¢é•¿ç‡": str(row["æ—¥å¢é•¿ç‡"]).strip() if pd.notna(row.get("æ—¥å¢é•¿ç‡")) else None,
                        "ç”³è´­çŠ¶æ€": str(row["ç”³è´­çŠ¶æ€"]).strip() if pd.notna(row.get("ç”³è´­çŠ¶æ€")) else None,
                        "èµå›çŠ¶æ€": str(row["èµå›çŠ¶æ€"]).strip() if pd.notna(row.get("èµå›çŠ¶æ€")) else None,
                    }
                    
                    # å”¯ä¸€é”®ï¼šåŸºé‡‘ä»£ç  + æ—¥æœŸ
                    ops.append(
                        UpdateOne(
                            {"åŸºé‡‘ä»£ç ": code, "æ—¥æœŸ": date_value},
                            {"$set": record},
                            upsert=True
                        )
                    )
                
                # æ‰§è¡Œæ‰¹é‡å†™å…¥
                if ops:
                    result = await self.col_fund_etf_fund_info_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"âœ… ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹å†™å…¥å®Œæˆ: "
                        f"æ–°å¢={result.upserted_count}, æ›´æ–°={result.matched_count}, "
                        f"æœ¬æ‰¹ä¿å­˜={batch_saved}, ç´¯è®¡={total_saved}/{total_count}"
                    )
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        await progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åœºå†…äº¤æ˜“åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®")
            return total_saved
            
        except Exception as e:
            logger.error(f"ä¿å­˜åœºå†…äº¤æ˜“åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_etf_fund_info_stats(self) -> Dict[str, Any]:
        """è·å–åœºå†…äº¤æ˜“åŸºé‡‘å†å²è¡Œæƒ…ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_etf_fund_info_em.count_documents({})
            
            # åŸºé‡‘ä»£ç åˆ†å¸ƒ
            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 20}
            ]
            fund_code_stats = await self.col_fund_etf_fund_info_em.aggregate(fund_code_pipeline).to_list(20)
            
            # æ—¥æœŸèŒƒå›´ç»Ÿè®¡
            earliest_doc = (
                await self.col_fund_etf_fund_info_em.find({"date": {"$exists": True, "$ne": None}}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_etf_fund_info_em.find({"date": {"$exists": True, "$ne": None}}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )
            
            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "earliest_date": earliest_doc[0]["date"] if earliest_doc else None,
                "latest_date": latest_doc[0]["date"] if latest_doc else None,
            }
            
            return result
            
        except Exception as e:
            logger.error(f"è·å–åœºå†…äº¤æ˜“åŸºé‡‘å†å²è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }
    
    async def clear_fund_etf_fund_info_data(self) -> int:
        """æ¸…ç©ºåœºå†…äº¤æ˜“åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®
        
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_etf_fund_info_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åœºå†…äº¤æ˜“åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåœºå†…äº¤æ˜“åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_etf_dividend_sina_data(self, df: pd.DataFrame, fund_code: str, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘ç´¯è®¡åˆ†çº¢æ•°æ®åˆ°MongoDB
        
        Args:
            df: åŒ…å«åŸºé‡‘ç´¯è®¡åˆ†çº¢æ•°æ®çš„DataFrame
            fund_code: åŸºé‡‘ä»£ç ï¼ˆå¦‚ sh510050ï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning(f"æ²¡æœ‰åŸºé‡‘ç´¯è®¡åˆ†çº¢æ•°æ®éœ€è¦ä¿å­˜: {fund_code}")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {fund_code} çš„ {total_count} æ¡ç´¯è®¡åˆ†çº¢æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime("%Y-%m-%d")
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime("%Y-%m-%d %H:%M:%S")
                        elif pd.isna(value):
                            doc[key] = None
                    
                    doc["fund_code"] = fund_code
                    doc["code"] = fund_code.replace("sh", "").replace("sz", "")
                    doc["source"] = "sina"
                    doc["updated_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    
                    date_str = doc.get("æ—¥æœŸ")
                    if date_str:
                        filter_query = {"fund_code": fund_code, "æ—¥æœŸ": date_str}
                        ops.append(UpdateOne(filter_query, {"$set": doc}, upsert=True))
                
                if ops:
                    result = await self.col_fund_etf_dividend_sina.bulk_write(ops, ordered=False)
                    batch_saved = result.upserted_count + result.modified_count
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        await progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"âœ… {fund_code} ç´¯è®¡åˆ†çº¢æ•°æ®ä¿å­˜å®Œæˆ: {total_saved}/{total_count}")
            return total_saved
            
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘ç´¯è®¡åˆ†çº¢æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_etf_dividend_sina_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘ç´¯è®¡åˆ†çº¢ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_etf_dividend_sina.count_documents({})
            
            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 20}
            ]
            fund_code_stats = await self.col_fund_etf_dividend_sina.aggregate(fund_code_pipeline).to_list(20)
            
            earliest_doc = (
                await self.col_fund_etf_dividend_sina.find({"æ—¥æœŸ": {"$exists": True, "$ne": None}}, {"æ—¥æœŸ": 1})
                .sort("æ—¥æœŸ", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_etf_dividend_sina.find({"æ—¥æœŸ": {"$exists": True, "$ne": None}}, {"æ—¥æœŸ": 1})
                .sort("æ—¥æœŸ", -1)
                .limit(1)
                .to_list(1)
            )
            
            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "earliest_date": earliest_doc[0]["æ—¥æœŸ"] if earliest_doc else None,
                "latest_date": latest_doc[0]["æ—¥æœŸ"] if latest_doc else None,
            }
            
            return result
            
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘ç´¯è®¡åˆ†çº¢ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }
    
    async def clear_fund_etf_dividend_sina_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘ç´¯è®¡åˆ†çº¢æ•°æ®
        
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_etf_dividend_sina.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘ç´¯è®¡åˆ†çº¢æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘ç´¯è®¡åˆ†çº¢æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def import_fund_etf_dividend_sina_from_file(self, content: bytes, filename: Optional[str] = None) -> Dict[str, Any]:
        """ä»æ–‡ä»¶å¯¼å…¥åŸºé‡‘ç´¯è®¡åˆ†çº¢æ•°æ®
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            filename: æ–‡ä»¶å
            
        Returns:
            å¯¼å…¥ç»“æœ
        """
        if not content:
            raise ValueError("ä¸Šä¼ æ–‡ä»¶ä¸ºç©º")
        
        name = (filename or "").lower()
        buffer = io.BytesIO(content)
        
        df: Optional[pd.DataFrame] = None
        try:
            if name.endswith(".csv") or name.endswith(".txt"):
                df = pd.read_csv(buffer)
            elif name.endswith(".xls") or name.endswith(".xlsx"):
                df = pd.read_excel(buffer)
            else:
                try:
                    df = pd.read_csv(buffer)
                except Exception:
                    buffer.seek(0)
                    df = pd.read_excel(buffer)
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
            raise ValueError("æ— æ³•è§£æä¸Šä¼ æ–‡ä»¶ï¼Œè¯·ç¡®è®¤ä¸ºæœ‰æ•ˆçš„ CSV æˆ– Excel æ–‡ä»¶")
        
        if df is None or df.empty:
            logger.warning("è§£æç»“æœä¸ºç©º DataFrame")
            return {"saved": 0, "rows": 0}
        
        rows = len(df)
        fund_code = df.iloc[0].get("fund_code", "unknown") if "fund_code" in df.columns else "unknown"
        saved = await self.save_fund_etf_dividend_sina_data(df, fund_code)
        logger.info(f"ä»æ–‡ä»¶ {filename} å¯¼å…¥ {rows} è¡Œï¼Œä¿å­˜ {saved} æ¡è®°å½•")
        
        return {"saved": saved, "rows": rows}
    
    async def sync_fund_etf_dividend_sina_from_remote(self, remote_host: str, batch_size: int = 5000, 
                                                       remote_collection: Optional[str] = None,
                                                       remote_username: Optional[str] = None,
                                                       remote_password: Optional[str] = None,
                                                       remote_auth_source: Optional[str] = None) -> Dict[str, Any]:
        """ä»è¿œç¨‹MongoDBåŒæ­¥åŸºé‡‘ç´¯è®¡åˆ†çº¢æ•°æ®
        
        Args:
            remote_host: è¿œç¨‹ä¸»æœºåœ°å€
            batch_size: æ‰¹æ¬¡å¤§å°
            remote_collection: è¿œç¨‹é›†åˆåç§°
            remote_username: è¿œç¨‹ç”¨æˆ·å
            remote_password: è¿œç¨‹å¯†ç 
            remote_auth_source: è®¤è¯æ•°æ®åº“
            
        Returns:
            åŒæ­¥ç»“æœ
        """
        from motor.motor_asyncio import AsyncIOMotorClient
        from bson import ObjectId
        
        if not remote_host:
            raise ValueError("è¿œç¨‹ä¸»æœºåœ°å€ä¸èƒ½ä¸ºç©º")
        
        try:
            batch = int(batch_size)
        except Exception:
            batch = 5000
        if batch <= 0:
            batch = 5000
        
        db_name = self.db.name
        auth_source = (remote_auth_source or db_name) if remote_username else None
        
        if remote_host.startswith("mongodb://") or remote_host.startswith("mongodb+srv://"):
            uri = remote_host
        else:
            host = remote_host
            port = 27017
            if ":" in remote_host:
                host_part, port_str = remote_host.split(":", 1)
                host = host_part or host
                try:
                    port = int(port_str)
                except Exception:
                    port = 27017
            
            if remote_username:
                if remote_password:
                    cred = f"{remote_username}:{remote_password}"
                else:
                    cred = remote_username
                
                if auth_source:
                    uri = f"mongodb://{cred}@{host}:{port}/{db_name}?authSource={auth_source}"
                else:
                    uri = f"mongodb://{cred}@{host}:{port}/{db_name}"
            else:
                uri = f"mongodb://{host}:{port}/{db_name}"
        
        logger.info(f"å¼€å§‹ä» {uri} åŒæ­¥åŸºé‡‘ç´¯è®¡åˆ†çº¢æ•°æ®ï¼Œbatch_size={batch}")
        
        client: AsyncIOMotorClient = AsyncIOMotorClient(uri, serverSelectionTimeoutMS=5000)
        try:
            try:
                remote_db = client.get_default_database() or client[self.db.name]
            except Exception:
                remote_db = client[self.db.name]
            
            target_collection = remote_collection or "fund_etf_dividend_sina"
            remote_col = remote_db[target_collection]
            
            base_filter: Dict[str, Any] = {}
            
            try:
                remote_total = await remote_col.count_documents(base_filter)
            except Exception as e:
                logger.warning(f"ç»Ÿè®¡è¿œç¨‹æ–‡æ¡£æ•°é‡å¤±è´¥: {e}")
                remote_total = 0
            
            synced = 0
            last_id: Optional[ObjectId] = None
            
            while True:
                if last_id is not None:
                    query: Dict[str, Any] = {"_id": {"$gt": last_id}}
                else:
                    query = base_filter
                
                cursor = remote_col.find(query).sort("_id", 1).limit(batch)
                docs = await cursor.to_list(length=batch)
                if not docs:
                    break
                
                last_id = docs[-1].get("_id")
                
                for d in docs:
                    d.pop("_id", None)
                
                ops = []
                for doc in docs:
                    fund_code = doc.get("fund_code", "unknown")
                    date_str = doc.get("æ—¥æœŸ")
                    if date_str:
                        filter_query = {"fund_code": fund_code, "æ—¥æœŸ": date_str}
                        ops.append(UpdateOne(filter_query, {"$set": doc}, upsert=True))
                
                if ops:
                    result = await self.col_fund_etf_dividend_sina.bulk_write(ops, ordered=False)
                    synced += result.upserted_count + result.modified_count
            
            logger.info(f"å®ŒæˆåŒæ­¥ï¼šremote_total={remote_total}, synced={synced}")
            
            return {"collection_name": "fund_etf_dividend_sina", "remote_total": remote_total, "synced": synced}
        finally:
            try:
                client.close()
            except Exception:
                pass
    
    async def save_fund_fh_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘åˆ†çº¢æ•°æ®åˆ°MongoDB
        
        Args:
            df: åŒ…å«åŸºé‡‘åˆ†çº¢æ•°æ®çš„DataFrame
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘åˆ†çº¢æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘åˆ†çº¢æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime("%Y-%m-%d")
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime("%Y-%m-%d %H:%M:%S")
                        elif pd.isna(value):
                            doc[key] = None
                    
                    doc["source"] = "eastmoney"
                    doc["updated_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    
                    # å”¯ä¸€æ ‡è¯†ï¼šåŸºé‡‘ä»£ç  + æƒç›Šç™»è®°æ—¥ + é™¤æ¯æ—¥æœŸ
                    fund_code = doc.get("åŸºé‡‘ä»£ç ")
                    equity_date = doc.get("æƒç›Šç™»è®°æ—¥")
                    ex_dividend_date = doc.get("é™¤æ¯æ—¥æœŸ")
                    
                    if fund_code and equity_date and ex_dividend_date:
                        filter_query = {
                            "åŸºé‡‘ä»£ç ": fund_code,
                            "æƒç›Šç™»è®°æ—¥": equity_date,
                            "é™¤æ¯æ—¥æœŸ": ex_dividend_date
                        }
                        ops.append(UpdateOne(filter_query, {"$set": doc}, upsert=True))
                
                if ops:
                    result = await self.col_fund_fh_em.bulk_write(ops, ordered=False)
                    batch_saved = result.upserted_count + result.modified_count
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        await progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"âœ… åŸºé‡‘åˆ†çº¢æ•°æ®ä¿å­˜å®Œæˆ: {total_saved}/{total_count}")
            return total_saved
            
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘åˆ†çº¢æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_fh_em_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘åˆ†çº¢ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_fh_em.count_documents({})
            
            # åŸºé‡‘ä»£ç åˆ†å¸ƒï¼ˆåˆ†çº¢æ¬¡æ•°æœ€å¤šçš„åŸºé‡‘ï¼‰
            fund_code_pipeline = [
                {"$group": {"_id": "$åŸºé‡‘ä»£ç ", "count": {"$sum": 1}, "åŸºé‡‘ç®€ç§°": {"$first": "$åŸºé‡‘ç®€ç§°"}}},
                {"$project": {"åŸºé‡‘ä»£ç ": "$_id", "åŸºé‡‘ç®€ç§°": 1, "åˆ†çº¢æ¬¡æ•°": "$count", "_id": 0}},
                {"$sort": {"åˆ†çº¢æ¬¡æ•°": -1}},
                {"$limit": 20}
            ]
            fund_code_stats = await self.col_fund_fh_em.aggregate(fund_code_pipeline).to_list(20)
            
            # æ—¥æœŸèŒƒå›´ç»Ÿè®¡
            earliest_doc = (
                await self.col_fund_fh_em.find({"æƒç›Šç™»è®°æ—¥": {"$exists": True, "$ne": None}}, {"æƒç›Šç™»è®°æ—¥": 1})
                .sort("æƒç›Šç™»è®°æ—¥", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_fh_em.find({"æƒç›Šç™»è®°æ—¥": {"$exists": True, "$ne": None}}, {"æƒç›Šç™»è®°æ—¥": 1})
                .sort("æƒç›Šç™»è®°æ—¥", -1)
                .limit(1)
                .to_list(1)
            )
            
            # åˆ†çº¢é‡‘é¢ç»Ÿè®¡
            total_dividend_pipeline = [
                {"$group": {"_id": None, "total_dividend": {"$sum": "$åˆ†çº¢"}}}
            ]
            total_dividend_result = await self.col_fund_fh_em.aggregate(total_dividend_pipeline).to_list(1)
            total_dividend = total_dividend_result[0]["total_dividend"] if total_dividend_result else 0
            
            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "earliest_date": earliest_doc[0]["æƒç›Šç™»è®°æ—¥"] if earliest_doc else None,
                "latest_date": latest_doc[0]["æƒç›Šç™»è®°æ—¥"] if latest_doc else None,
                "total_dividend": round(total_dividend, 4) if total_dividend else 0,
            }
            
            return result
            
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘åˆ†çº¢ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "earliest_date": None,
                "latest_date": None,
                "total_dividend": 0,
            }
    
    async def clear_fund_fh_em_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘åˆ†çº¢æ•°æ®
        
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_fh_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘åˆ†çº¢æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘åˆ†çº¢æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def import_fund_fh_em_from_file(self, content: bytes, filename: Optional[str] = None) -> Dict[str, Any]:
        """ä»æ–‡ä»¶å¯¼å…¥åŸºé‡‘åˆ†çº¢æ•°æ®
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            filename: æ–‡ä»¶å
            
        Returns:
            å¯¼å…¥ç»“æœ
        """
        if not content:
            raise ValueError("ä¸Šä¼ æ–‡ä»¶ä¸ºç©º")
        
        name = (filename or "").lower()
        buffer = io.BytesIO(content)
        
        df: Optional[pd.DataFrame] = None
        try:
            if name.endswith(".csv") or name.endswith(".txt"):
                df = pd.read_csv(buffer)
            elif name.endswith(".xls") or name.endswith(".xlsx"):
                df = pd.read_excel(buffer)
            else:
                try:
                    df = pd.read_csv(buffer)
                except Exception:
                    buffer.seek(0)
                    df = pd.read_excel(buffer)
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
            raise ValueError("æ— æ³•è§£æä¸Šä¼ æ–‡ä»¶ï¼Œè¯·ç¡®è®¤ä¸ºæœ‰æ•ˆçš„ CSV æˆ– Excel æ–‡ä»¶")
        
        if df is None or df.empty:
            logger.warning("è§£æç»“æœä¸ºç©º DataFrame")
            return {"saved": 0, "rows": 0}
        
        rows = len(df)
        saved = await self.save_fund_fh_em_data(df)
        logger.info(f"ä»æ–‡ä»¶ {filename} å¯¼å…¥ {rows} è¡Œï¼Œä¿å­˜ {saved} æ¡è®°å½•")
        
        return {"saved": saved, "rows": rows}
    
    async def sync_fund_fh_em_from_remote(self, remote_host: str, batch_size: int = 5000, 
                                          remote_collection: Optional[str] = None,
                                          remote_username: Optional[str] = None,
                                          remote_password: Optional[str] = None,
                                          remote_auth_source: Optional[str] = None) -> Dict[str, Any]:
        """ä»è¿œç¨‹MongoDBåŒæ­¥åŸºé‡‘åˆ†çº¢æ•°æ®
        
        Args:
            remote_host: è¿œç¨‹ä¸»æœºåœ°å€
            batch_size: æ‰¹æ¬¡å¤§å°
            remote_collection: è¿œç¨‹é›†åˆåç§°
            remote_username: è¿œç¨‹ç”¨æˆ·å
            remote_password: è¿œç¨‹å¯†ç 
            remote_auth_source: è®¤è¯æ•°æ®åº“
            
        Returns:
            åŒæ­¥ç»“æœ
        """
        from motor.motor_asyncio import AsyncIOMotorClient
        from bson import ObjectId
        
        if not remote_host:
            raise ValueError("è¿œç¨‹ä¸»æœºåœ°å€ä¸èƒ½ä¸ºç©º")
        
        try:
            batch = int(batch_size)
        except Exception:
            batch = 5000
        if batch <= 0:
            batch = 5000
        
        db_name = self.db.name
        auth_source = (remote_auth_source or db_name) if remote_username else None
        
        if remote_host.startswith("mongodb://") or remote_host.startswith("mongodb+srv://"):
            uri = remote_host
        else:
            host = remote_host
            port = 27017
            if ":" in remote_host:
                host_part, port_str = remote_host.split(":", 1)
                host = host_part or host
                try:
                    port = int(port_str)
                except Exception:
                    port = 27017
            
            if remote_username:
                if remote_password:
                    cred = f"{remote_username}:{remote_password}"
                else:
                    cred = remote_username
                
                if auth_source:
                    uri = f"mongodb://{cred}@{host}:{port}/{db_name}?authSource={auth_source}"
                else:
                    uri = f"mongodb://{cred}@{host}:{port}/{db_name}"
            else:
                uri = f"mongodb://{host}:{port}/{db_name}"
        
        logger.info(f"å¼€å§‹ä» {uri} åŒæ­¥åŸºé‡‘åˆ†çº¢æ•°æ®ï¼Œbatch_size={batch}")
        
        client: AsyncIOMotorClient = AsyncIOMotorClient(uri, serverSelectionTimeoutMS=5000)
        try:
            try:
                remote_db = client.get_default_database() or client[self.db.name]
            except Exception:
                remote_db = client[self.db.name]
            
            target_collection = remote_collection or "fund_fh_em"
            remote_col = remote_db[target_collection]
            
            base_filter: Dict[str, Any] = {}
            
            try:
                remote_total = await remote_col.count_documents(base_filter)
            except Exception as e:
                logger.warning(f"ç»Ÿè®¡è¿œç¨‹æ–‡æ¡£æ•°é‡å¤±è´¥: {e}")
                remote_total = 0
            
            synced = 0
            last_id: Optional[ObjectId] = None
            
            while True:
                if last_id is not None:
                    query: Dict[str, Any] = {"_id": {"$gt": last_id}}
                else:
                    query = base_filter
                
                cursor = remote_col.find(query).sort("_id", 1).limit(batch)
                docs = await cursor.to_list(length=batch)
                if not docs:
                    break
                
                last_id = docs[-1].get("_id")
                
                for d in docs:
                    d.pop("_id", None)
                
                ops = []
                for doc in docs:
                    fund_code = doc.get("åŸºé‡‘ä»£ç ")
                    equity_date = doc.get("æƒç›Šç™»è®°æ—¥")
                    ex_dividend_date = doc.get("é™¤æ¯æ—¥æœŸ")
                    
                    if fund_code and equity_date and ex_dividend_date:
                        filter_query = {
                            "åŸºé‡‘ä»£ç ": fund_code,
                            "æƒç›Šç™»è®°æ—¥": equity_date,
                            "é™¤æ¯æ—¥æœŸ": ex_dividend_date
                        }
                        ops.append(UpdateOne(filter_query, {"$set": doc}, upsert=True))
                
                if ops:
                    result = await self.col_fund_fh_em.bulk_write(ops, ordered=False)
                    synced += result.upserted_count + result.modified_count
            
            logger.info(f"å®ŒæˆåŒæ­¥ï¼šremote_total={remote_total}, synced={synced}")
            
            return {"collection_name": "fund_fh_em", "remote_total": remote_total, "synced": synced}
        finally:
            try:
                client.close()
            except Exception:
                pass
    
    async def save_fund_cf_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ä¿å­˜åŸºé‡‘æ‹†åˆ†æ•°æ®åˆ°MongoDB
        
        Args:
            df: åŒ…å«åŸºé‡‘æ‹†åˆ†ä¿¡æ¯çš„DataFrame
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘æ‹†åˆ†æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            # æ¸…ç†æ— æ•ˆçš„æµ®ç‚¹æ•°å€¼ï¼ˆNaN, Infinityç­‰ï¼‰ï¼Œé˜²æ­¢JSONåºåˆ—åŒ–é”™è¯¯
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘æ‹†åˆ†æ•°æ®...")
            
            # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹1000æ¡
            batch_size = 1000
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ“¦ å°†åˆ† {total_batches} æ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} æ¡")
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"ğŸ“ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ï¼Œè®°å½•èŒƒå›´: {start_idx + 1}-{end_idx}")
                
                # æ„å»ºæ‰¹é‡æ“ä½œ
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # æ¸…ç†NaN/Infinityå€¼
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        # è½¬æ¢ datetime.date å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        # è½¬æ¢ datetime.datetime å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    split_date = str(doc.get('æ‹†åˆ†æŠ˜ç®—æ—¥', ''))
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_cf_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç å’Œæ‹†åˆ†æŠ˜ç®—æ—¥ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'æ‹†åˆ†æŠ˜ç®—æ—¥': split_date},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # æ‰§è¡Œæ‰¹é‡å†™å…¥
                if ops:
                    result = await self.col_fund_cf_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"âœ… ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹å†™å…¥å®Œæˆ: "
                        f"æ–°å¢={result.upserted_count}, æ›´æ–°={result.matched_count}, "
                        f"æœ¬æ‰¹ä¿å­˜={batch_saved}, ç´¯è®¡={total_saved}/{total_count}"
                    )
                    
                    # è°ƒç”¨è¿›åº¦å›è°ƒï¼ˆå¦‚æœæä¾›ï¼‰
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘æ‹†åˆ†æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘æ‹†åˆ†æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_cf_em_data(self) -> int:
        """
        æ¸…ç©ºåŸºé‡‘æ‹†åˆ†æ•°æ®
        
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_cf_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘æ‹†åˆ†æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘æ‹†åˆ†æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_cf_em_stats(self) -> Dict[str, Any]:
        """
        è·å–åŸºé‡‘æ‹†åˆ†ç»Ÿè®¡
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_cf_em.count_documents({})
            
            # æŒ‰æ‹†åˆ†ç±»å‹ç»Ÿè®¡
            pipeline = [
                {
                    '$group': {
                        '_id': '$æ‹†åˆ†ç±»å‹',
                        'count': {'$sum': 1}
                    }
                },
                {
                    '$sort': {'count': -1}
                }
            ]
            
            type_stats = []
            async for doc in self.col_fund_cf_em.aggregate(pipeline):
                type_stats.append({
                    'type': doc['_id'],
                    'count': doc['count']
                })
            
            # è·å–æœ€æ—©å’Œæœ€æ™šçš„æ‹†åˆ†æ—¥æœŸ
            earliest_date = None
            latest_date = None
            pipeline_date = [
                {
                    '$group': {
                        '_id': None,
                        'earliest': {'$min': '$æ‹†åˆ†æŠ˜ç®—æ—¥'},
                        'latest': {'$max': '$æ‹†åˆ†æŠ˜ç®—æ—¥'}
                    }
                }
            ]
            
            async for doc in self.col_fund_cf_em.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            return {
                'total_count': total_count,
                'type_stats': type_stats,
                'earliest_date': earliest_date,
                'latest_date': latest_date
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘æ‹†åˆ†ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_fh_rank_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ä¿å­˜åŸºé‡‘åˆ†çº¢æ’è¡Œæ•°æ®åˆ°MongoDB
        
        Args:
            df: åŒ…å«åŸºé‡‘åˆ†çº¢æ’è¡Œä¿¡æ¯çš„DataFrame
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘åˆ†çº¢æ’è¡Œæ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            # æ¸…ç†æ— æ•ˆçš„æµ®ç‚¹æ•°å€¼ï¼ˆNaN, Infinityç­‰ï¼‰ï¼Œé˜²æ­¢JSONåºåˆ—åŒ–é”™è¯¯
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘åˆ†çº¢æ’è¡Œæ•°æ®...")
            
            # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹500æ¡
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ“¦ å°†åˆ† {total_batches} æ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} æ¡")
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"ğŸ“ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ï¼Œè®°å½•èŒƒå›´: {start_idx + 1}-{end_idx}")
                
                # æ„å»ºæ‰¹é‡æ“ä½œ
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # æ¸…ç†NaN/Infinityå€¼
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        # è½¬æ¢ datetime.date å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        # è½¬æ¢ datetime.datetime å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_fh_rank_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    ops.append(
                        UpdateOne(
                            {'code': fund_code},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # æ‰§è¡Œæ‰¹é‡å†™å…¥
                if ops:
                    result = await self.col_fund_fh_rank_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"âœ… ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹å†™å…¥å®Œæˆ: "
                        f"æ–°å¢={result.upserted_count}, æ›´æ–°={result.matched_count}, "
                        f"æœ¬æ‰¹ä¿å­˜={batch_saved}, ç´¯è®¡={total_saved}/{total_count}"
                    )
                    
                    # è°ƒç”¨è¿›åº¦å›è°ƒï¼ˆå¦‚æœæä¾›ï¼‰
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘åˆ†çº¢æ’è¡Œæ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘åˆ†çº¢æ’è¡Œæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_fh_rank_em_data(self) -> int:
        """
        æ¸…ç©ºåŸºé‡‘åˆ†çº¢æ’è¡Œæ•°æ®
        
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_fh_rank_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘åˆ†çº¢æ’è¡Œæ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘åˆ†çº¢æ’è¡Œæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_fh_rank_em_stats(self) -> Dict[str, Any]:
        """
        è·å–åŸºé‡‘åˆ†çº¢æ’è¡Œç»Ÿè®¡
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_fh_rank_em.count_documents({})
            
            # è·å–æœ€æ—©å’Œæœ€æ™šçš„æˆç«‹æ—¥æœŸ
            earliest_date = None
            latest_date = None
            pipeline_date = [
                {
                    '$group': {
                        '_id': None,
                        'earliest': {'$min': '$æˆç«‹æ—¥æœŸ'},
                        'latest': {'$max': '$æˆç«‹æ—¥æœŸ'}
                    }
                }
            ]
            
            async for doc in self.col_fund_fh_rank_em.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            # è·å–ç´¯è®¡åˆ†çº¢TOP10
            pipeline_top_dividend = [
                {
                    '$sort': {'ç´¯è®¡åˆ†çº¢': -1}
                },
                {
                    '$limit': 10
                },
                {
                    '$project': {
                        'code': '$åŸºé‡‘ä»£ç ',
                        'name': '$åŸºé‡‘ç®€ç§°',
                        'total_dividend': '$ç´¯è®¡åˆ†çº¢',
                        'dividend_times': '$ç´¯è®¡æ¬¡æ•°'
                    }
                }
            ]
            
            top_dividend = []
            async for doc in self.col_fund_fh_rank_em.aggregate(pipeline_top_dividend):
                top_dividend.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'total_dividend': doc.get('total_dividend'),
                    'dividend_times': doc.get('dividend_times')
                })
            
            # è·å–ç´¯è®¡æ¬¡æ•°TOP10
            pipeline_top_times = [
                {
                    '$sort': {'ç´¯è®¡æ¬¡æ•°': -1}
                },
                {
                    '$limit': 10
                },
                {
                    '$project': {
                        'code': '$åŸºé‡‘ä»£ç ',
                        'name': '$åŸºé‡‘ç®€ç§°',
                        'total_dividend': '$ç´¯è®¡åˆ†çº¢',
                        'dividend_times': '$ç´¯è®¡æ¬¡æ•°'
                    }
                }
            ]
            
            top_times = []
            async for doc in self.col_fund_fh_rank_em.aggregate(pipeline_top_times):
                top_times.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'total_dividend': doc.get('total_dividend'),
                    'dividend_times': doc.get('dividend_times')
                })
            
            return {
                'total_count': total_count,
                'earliest_date': earliest_date,
                'latest_date': latest_date,
                'top_dividend': top_dividend,
                'top_times': top_times
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘åˆ†çº¢æ’è¡Œç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_open_fund_rank_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ä¿å­˜å¼€æ”¾å¼åŸºé‡‘æ’è¡Œæ•°æ®åˆ°MongoDB
        
        Args:
            df: åŒ…å«å¼€æ”¾å¼åŸºé‡‘æ’è¡Œä¿¡æ¯çš„DataFrame
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰å¼€æ”¾å¼åŸºé‡‘æ’è¡Œæ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            # æ¸…ç†æ— æ•ˆçš„æµ®ç‚¹æ•°å€¼ï¼ˆNaN, Infinityç­‰ï¼‰ï¼Œé˜²æ­¢JSONåºåˆ—åŒ–é”™è¯¯
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡å¼€æ”¾å¼åŸºé‡‘æ’è¡Œæ•°æ®...")
            
            # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹500æ¡
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ“¦ å°†åˆ† {total_batches} æ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} æ¡")
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"ğŸ“ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ï¼Œè®°å½•èŒƒå›´: {start_idx + 1}-{end_idx}")
                
                # æ„å»ºæ‰¹é‡æ“ä½œ
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # æ¸…ç†NaN/Infinityå€¼
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        # è½¬æ¢ datetime.date å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        # è½¬æ¢ datetime.datetime å¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    date_str = str(doc.get('æ—¥æœŸ', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_open_fund_rank_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç å’Œæ—¥æœŸä½œä¸ºå”¯ä¸€æ ‡è¯†
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'date': date_str},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # æ‰§è¡Œæ‰¹é‡å†™å…¥
                if ops:
                    result = await self.col_fund_open_fund_rank_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"âœ… ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹å†™å…¥å®Œæˆ: "
                        f"æ–°å¢={result.upserted_count}, æ›´æ–°={result.matched_count}, "
                        f"æœ¬æ‰¹ä¿å­˜={batch_saved}, ç´¯è®¡={total_saved}/{total_count}"
                    )
                    
                    # è°ƒç”¨è¿›åº¦å›è°ƒï¼ˆå¦‚æœæä¾›ï¼‰
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡å¼€æ”¾å¼åŸºé‡‘æ’è¡Œæ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜å¼€æ”¾å¼åŸºé‡‘æ’è¡Œæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_open_fund_rank_em_data(self) -> int:
        """
        æ¸…ç©ºå¼€æ”¾å¼åŸºé‡‘æ’è¡Œæ•°æ®
        
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        try:
            result = await self.col_fund_open_fund_rank_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡å¼€æ”¾å¼åŸºé‡‘æ’è¡Œæ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºå¼€æ”¾å¼åŸºé‡‘æ’è¡Œæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_open_fund_rank_em_stats(self) -> Dict[str, Any]:
        """
        è·å–å¼€æ”¾å¼åŸºé‡‘æ’è¡Œç»Ÿè®¡
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = await self.col_fund_open_fund_rank_em.count_documents({})
            
            # è·å–æœ€æ—©å’Œæœ€æ™šçš„æ—¥æœŸ
            earliest_date = None
            latest_date = None
            pipeline_date = [
                {
                    '$group': {
                        '_id': None,
                        'earliest': {'$min': '$æ—¥æœŸ'},
                        'latest': {'$max': '$æ—¥æœŸ'}
                    }
                }
            ]
            
            async for doc in self.col_fund_open_fund_rank_em.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            # è·å–è¿‘1å¹´æ”¶ç›Šç‡TOP10
            pipeline_top_1year = [
                {
                    '$match': {'è¿‘1å¹´': {'$ne': None}}
                },
                {
                    '$sort': {'è¿‘1å¹´': -1}
                },
                {
                    '$limit': 10
                },
                {
                    '$project': {
                        'code': '$åŸºé‡‘ä»£ç ',
                        'name': '$åŸºé‡‘ç®€ç§°',
                        'return_1year': '$è¿‘1å¹´',
                        'return_ytd': '$ä»Šå¹´æ¥'
                    }
                }
            ]
            
            top_performers = []
            async for doc in self.col_fund_open_fund_rank_em.aggregate(pipeline_top_1year):
                top_performers.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'return_1year': doc.get('return_1year'),
                    'return_ytd': doc.get('return_ytd')
                })
            
            # è·å–ä»Šå¹´æ¥æ”¶ç›Šç‡TOP10
            pipeline_top_ytd = [
                {
                    '$match': {'ä»Šå¹´æ¥': {'$ne': None}}
                },
                {
                    '$sort': {'ä»Šå¹´æ¥': -1}
                },
                {
                    '$limit': 10
                },
                {
                    '$project': {
                        'code': '$åŸºé‡‘ä»£ç ',
                        'name': '$åŸºé‡‘ç®€ç§°',
                        'return_ytd': '$ä»Šå¹´æ¥',
                        'return_1year': '$è¿‘1å¹´'
                    }
                }
            ]
            
            top_ytd = []
            async for doc in self.col_fund_open_fund_rank_em.aggregate(pipeline_top_ytd):
                top_ytd.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'return_ytd': doc.get('return_ytd'),
                    'return_1year': doc.get('return_1year')
                })
            
            return {
                'total_count': total_count,
                'earliest_date': earliest_date,
                'latest_date': latest_date,
                'top_performers': top_performers,
                'top_ytd': top_ytd
            }
        except Exception as e:
            logger.error(f"è·å–å¼€æ”¾å¼åŸºé‡‘æ’è¡Œç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_exchange_rank_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åœºå†…äº¤æ˜“åŸºé‡‘æ’è¡Œæ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åœºå†…äº¤æ˜“åŸºé‡‘æ’è¡Œæ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åœºå†…äº¤æ˜“åŸºé‡‘æ’è¡Œæ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    date_str = str(doc.get('æ—¥æœŸ', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_exchange_rank_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'date': date_str},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_exchange_rank_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åœºå†…äº¤æ˜“åŸºé‡‘æ’è¡Œæ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åœºå†…äº¤æ˜“åŸºé‡‘æ’è¡Œæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_exchange_rank_em_data(self) -> int:
        """æ¸…ç©ºåœºå†…äº¤æ˜“åŸºé‡‘æ’è¡Œæ•°æ®"""
        try:
            result = await self.col_fund_exchange_rank_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åœºå†…äº¤æ˜“åŸºé‡‘æ’è¡Œæ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåœºå†…äº¤æ˜“åŸºé‡‘æ’è¡Œæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_exchange_rank_em_stats(self) -> Dict[str, Any]:
        """è·å–åœºå†…äº¤æ˜“åŸºé‡‘æ’è¡Œç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_exchange_rank_em.count_documents({})
            
            pipeline_date = [
                {'$group': {'_id': None, 'earliest': {'$min': '$æ—¥æœŸ'}, 'latest': {'$max': '$æ—¥æœŸ'}}}
            ]
            
            earliest_date = None
            latest_date = None
            async for doc in self.col_fund_exchange_rank_em.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            pipeline_type = [
                {'$group': {'_id': '$ç±»å‹', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            type_stats = []
            async for doc in self.col_fund_exchange_rank_em.aggregate(pipeline_type):
                type_stats.append({'type': doc['_id'], 'count': doc['count']})
            
            pipeline_top = [
                {'$match': {'è¿‘1å¹´': {'$ne': None}}},
                {'$sort': {'è¿‘1å¹´': -1}},
                {'$limit': 10},
                {'$project': {'code': '$åŸºé‡‘ä»£ç ', 'name': '$åŸºé‡‘ç®€ç§°', 'type': '$ç±»å‹', 'return_1year': '$è¿‘1å¹´'}}
            ]
            
            top_performers = []
            async for doc in self.col_fund_exchange_rank_em.aggregate(pipeline_top):
                top_performers.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'type': doc.get('type'),
                    'return_1year': doc.get('return_1year')
                })
            
            return {
                'total_count': total_count,
                'earliest_date': earliest_date,
                'latest_date': latest_date,
                'type_stats': type_stats,
                'top_performers': top_performers
            }
        except Exception as e:
            logger.error(f"è·å–åœºå†…äº¤æ˜“åŸºé‡‘æ’è¡Œç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_money_rank_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜è´§å¸å‹åŸºé‡‘æ’è¡Œæ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰è´§å¸å‹åŸºé‡‘æ’è¡Œæ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡è´§å¸å‹åŸºé‡‘æ’è¡Œæ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    date_str = str(doc.get('æ—¥æœŸ', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_money_rank_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'date': date_str},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_money_rank_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡è´§å¸å‹åŸºé‡‘æ’è¡Œæ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜è´§å¸å‹åŸºé‡‘æ’è¡Œæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_money_rank_em_data(self) -> int:
        """æ¸…ç©ºè´§å¸å‹åŸºé‡‘æ’è¡Œæ•°æ®"""
        try:
            result = await self.col_fund_money_rank_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡è´§å¸å‹åŸºé‡‘æ’è¡Œæ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºè´§å¸å‹åŸºé‡‘æ’è¡Œæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_money_rank_em_stats(self) -> Dict[str, Any]:
        """è·å–è´§å¸å‹åŸºé‡‘æ’è¡Œç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_money_rank_em.count_documents({})
            
            pipeline_date = [
                {'$group': {'_id': None, 'earliest': {'$min': '$æ—¥æœŸ'}, 'latest': {'$max': '$æ—¥æœŸ'}}}
            ]
            
            earliest_date = None
            latest_date = None
            async for doc in self.col_fund_money_rank_em.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            # è·å–å¹´åŒ–æ”¶ç›Šç‡7æ—¥TOP10
            pipeline_top_7d = [
                {'$match': {'å¹´åŒ–æ”¶ç›Šç‡7æ—¥': {'$ne': None}}},
                {'$sort': {'å¹´åŒ–æ”¶ç›Šç‡7æ—¥': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$åŸºé‡‘ä»£ç ',
                    'name': '$åŸºé‡‘ç®€ç§°',
                    'yield_7d': '$å¹´åŒ–æ”¶ç›Šç‡7æ—¥',
                    'yield_10k': '$ä¸‡ä»½æ”¶ç›Š'
                }}
            ]
            
            top_yield_7d = []
            async for doc in self.col_fund_money_rank_em.aggregate(pipeline_top_7d):
                top_yield_7d.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'yield_7d': doc.get('yield_7d'),
                    'yield_10k': doc.get('yield_10k')
                })
            
            # è·å–è¿‘1å¹´æ”¶ç›ŠTOP10
            pipeline_top_1y = [
                {'$match': {'è¿‘1å¹´': {'$ne': None}}},
                {'$sort': {'è¿‘1å¹´': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$åŸºé‡‘ä»£ç ',
                    'name': '$åŸºé‡‘ç®€ç§°',
                    'return_1y': '$è¿‘1å¹´',
                    'yield_7d': '$å¹´åŒ–æ”¶ç›Šç‡7æ—¥'
                }}
            ]
            
            top_return_1y = []
            async for doc in self.col_fund_money_rank_em.aggregate(pipeline_top_1y):
                top_return_1y.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'return_1y': doc.get('return_1y'),
                    'yield_7d': doc.get('yield_7d')
                })
            
            return {
                'total_count': total_count,
                'earliest_date': earliest_date,
                'latest_date': latest_date,
                'top_yield_7d': top_yield_7d,
                'top_return_1y': top_return_1y
            }
        except Exception as e:
            logger.error(f"è·å–è´§å¸å‹åŸºé‡‘æ’è¡Œç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_lcx_rank_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜ç†è´¢åŸºé‡‘æ’è¡Œæ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰ç†è´¢åŸºé‡‘æ’è¡Œæ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡ç†è´¢åŸºé‡‘æ’è¡Œæ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    date_str = str(doc.get('æ—¥æœŸ', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_lcx_rank_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'date': date_str},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_lcx_rank_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡ç†è´¢åŸºé‡‘æ’è¡Œæ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜ç†è´¢åŸºé‡‘æ’è¡Œæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_lcx_rank_em_data(self) -> int:
        """æ¸…ç©ºç†è´¢åŸºé‡‘æ’è¡Œæ•°æ®"""
        try:
            result = await self.col_fund_lcx_rank_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡ç†è´¢åŸºé‡‘æ’è¡Œæ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºç†è´¢åŸºé‡‘æ’è¡Œæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_lcx_rank_em_stats(self) -> Dict[str, Any]:
        """è·å–ç†è´¢åŸºé‡‘æ’è¡Œç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_lcx_rank_em.count_documents({})
            
            pipeline_date = [
                {'$group': {'_id': None, 'earliest': {'$min': '$æ—¥æœŸ'}, 'latest': {'$max': '$æ—¥æœŸ'}}}
            ]
            
            earliest_date = None
            latest_date = None
            async for doc in self.col_fund_lcx_rank_em.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            # è·å–å¹´åŒ–æ”¶ç›Šç‡7æ—¥TOP10
            pipeline_top_7d = [
                {'$match': {'å¹´åŒ–æ”¶ç›Šç‡7æ—¥': {'$ne': None}}},
                {'$sort': {'å¹´åŒ–æ”¶ç›Šç‡7æ—¥': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$åŸºé‡‘ä»£ç ',
                    'name': '$åŸºé‡‘ç®€ç§°',
                    'yield_7d': '$å¹´åŒ–æ”¶ç›Šç‡7æ—¥',
                    'yield_10k': '$ä¸‡ä»½æ”¶ç›Š',
                    'purchasable': '$å¯è´­ä¹°'
                }}
            ]
            
            top_yield_7d = []
            async for doc in self.col_fund_lcx_rank_em.aggregate(pipeline_top_7d):
                top_yield_7d.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'yield_7d': doc.get('yield_7d'),
                    'yield_10k': doc.get('yield_10k'),
                    'purchasable': doc.get('purchasable')
                })
            
            # è·å–æˆç«‹æ¥æ”¶ç›ŠTOP10
            pipeline_top_since = [
                {'$match': {'æˆç«‹æ¥': {'$ne': None}}},
                {'$sort': {'æˆç«‹æ¥': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$åŸºé‡‘ä»£ç ',
                    'name': '$åŸºé‡‘ç®€ç§°',
                    'return_since': '$æˆç«‹æ¥',
                    'yield_7d': '$å¹´åŒ–æ”¶ç›Šç‡7æ—¥',
                    'purchasable': '$å¯è´­ä¹°'
                }}
            ]
            
            top_return_since = []
            async for doc in self.col_fund_lcx_rank_em.aggregate(pipeline_top_since):
                top_return_since.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'return_since': doc.get('return_since'),
                    'yield_7d': doc.get('yield_7d'),
                    'purchasable': doc.get('purchasable')
                })
            
            return {
                'total_count': total_count,
                'earliest_date': earliest_date,
                'latest_date': latest_date,
                'top_yield_7d': top_yield_7d,
                'top_return_since': top_return_since
            }
        except Exception as e:
            logger.error(f"è·å–ç†è´¢åŸºé‡‘æ’è¡Œç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_hk_rank_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜é¦™æ¸¯åŸºé‡‘æ’è¡Œæ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰é¦™æ¸¯åŸºé‡‘æ’è¡Œæ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡é¦™æ¸¯åŸºé‡‘æ’è¡Œæ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    date_str = str(doc.get('æ—¥æœŸ', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_hk_rank_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'date': date_str},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_hk_rank_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡é¦™æ¸¯åŸºé‡‘æ’è¡Œæ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜é¦™æ¸¯åŸºé‡‘æ’è¡Œæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_hk_rank_em_data(self) -> int:
        """æ¸…ç©ºé¦™æ¸¯åŸºé‡‘æ’è¡Œæ•°æ®"""
        try:
            result = await self.col_fund_hk_rank_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡é¦™æ¸¯åŸºé‡‘æ’è¡Œæ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºé¦™æ¸¯åŸºé‡‘æ’è¡Œæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_hk_rank_em_stats(self) -> Dict[str, Any]:
        """è·å–é¦™æ¸¯åŸºé‡‘æ’è¡Œç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_hk_rank_em.count_documents({})
            
            pipeline_date = [
                {'$group': {'_id': None, 'earliest': {'$min': '$æ—¥æœŸ'}, 'latest': {'$max': '$æ—¥æœŸ'}}}
            ]
            
            earliest_date = None
            latest_date = None
            async for doc in self.col_fund_hk_rank_em.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            # è·å–å¸ç§åˆ†å¸ƒç»Ÿè®¡
            pipeline_currency = [
                {'$group': {'_id': '$å¸ç§', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            currency_stats = []
            async for doc in self.col_fund_hk_rank_em.aggregate(pipeline_currency):
                currency_stats.append({
                    'currency': doc.get('_id'),
                    'count': doc.get('count')
                })
            
            # è·å–è¿‘1å¹´æ”¶ç›ŠTOP10
            pipeline_top_1y = [
                {'$match': {'è¿‘1å¹´': {'$ne': None}}},
                {'$sort': {'è¿‘1å¹´': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$åŸºé‡‘ä»£ç ',
                    'name': '$åŸºé‡‘ç®€ç§°',
                    'currency': '$å¸ç§',
                    'return_1y': '$è¿‘1å¹´',
                    'nav': '$å•ä½å‡€å€¼',
                    'purchasable': '$å¯è´­ä¹°'
                }}
            ]
            
            top_return_1y = []
            async for doc in self.col_fund_hk_rank_em.aggregate(pipeline_top_1y):
                top_return_1y.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'currency': doc.get('currency'),
                    'return_1y': doc.get('return_1y'),
                    'nav': doc.get('nav'),
                    'purchasable': doc.get('purchasable')
                })
            
            # è·å–æˆç«‹æ¥æ”¶ç›ŠTOP10
            pipeline_top_since = [
                {'$match': {'æˆç«‹æ¥': {'$ne': None}}},
                {'$sort': {'æˆç«‹æ¥': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$åŸºé‡‘ä»£ç ',
                    'name': '$åŸºé‡‘ç®€ç§°',
                    'currency': '$å¸ç§',
                    'return_since': '$æˆç«‹æ¥',
                    'purchasable': '$å¯è´­ä¹°'
                }}
            ]
            
            top_return_since = []
            async for doc in self.col_fund_hk_rank_em.aggregate(pipeline_top_since):
                top_return_since.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'currency': doc.get('currency'),
                    'return_since': doc.get('return_since'),
                    'purchasable': doc.get('purchasable')
                })
            
            return {
                'total_count': total_count,
                'earliest_date': earliest_date,
                'latest_date': latest_date,
                'currency_stats': currency_stats,
                'top_return_1y': top_return_1y,
                'top_return_since': top_return_since
            }
        except Exception as e:
            logger.error(f"è·å–é¦™æ¸¯åŸºé‡‘æ’è¡Œç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_individual_achievement_xq_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘ä¸šç»©æ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘ä¸šç»©æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘ä¸šç»©æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    perf_type = str(doc.get('ä¸šç»©ç±»å‹', ''))
                    period = str(doc.get('å‘¨æœŸ', ''))
                    doc['code'] = fund_code
                    doc['performance_type'] = perf_type
                    doc['period'] = period
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_individual_achievement_xq'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'performance_type': perf_type, 'period': period},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_individual_achievement_xq.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘ä¸šç»©æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘ä¸šç»©æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_individual_achievement_xq_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘ä¸šç»©æ•°æ®"""
        try:
            result = await self.col_fund_individual_achievement_xq.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘ä¸šç»©æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘ä¸šç»©æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_individual_achievement_xq_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘ä¸šç»©ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_individual_achievement_xq.count_documents({})
            
            # è·å–å”¯ä¸€åŸºé‡‘æ•°é‡
            pipeline_funds = [
                {'$group': {'_id': '$code'}},
                {'$count': 'unique_funds'}
            ]
            
            unique_funds = 0
            async for doc in self.col_fund_individual_achievement_xq.aggregate(pipeline_funds):
                unique_funds = doc.get('unique_funds', 0)
            
            # è·å–ä¸šç»©ç±»å‹åˆ†å¸ƒ
            pipeline_types = [
                {'$group': {'_id': '$ä¸šç»©ç±»å‹', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            performance_types = []
            async for doc in self.col_fund_individual_achievement_xq.aggregate(pipeline_types):
                performance_types.append({
                    'type': doc.get('_id'),
                    'count': doc.get('count')
                })
            
            # è·å–æˆç«‹ä»¥æ¥æ”¶ç›ŠTOP10
            pipeline_top_return = [
                {'$match': {'å‘¨æœŸ': 'æˆç«‹ä»¥æ¥', 'æœ¬äº§å“åŒºé—´æ”¶ç›Š': {'$ne': None}}},
                {'$sort': {'æœ¬äº§å“åŒºé—´æ”¶ç›Š': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$åŸºé‡‘ä»£ç ',
                    'return': '$æœ¬äº§å“åŒºé—´æ”¶ç›Š',
                    'max_drawdown': '$æœ¬äº§å“æœ€å¤§å›æ’’',
                    'ranking': '$å‘¨æœŸæ”¶ç›ŠåŒç±»æ’å'
                }}
            ]
            
            top_return_since = []
            async for doc in self.col_fund_individual_achievement_xq.aggregate(pipeline_top_return):
                top_return_since.append({
                    'code': doc.get('code'),
                    'return': doc.get('return'),
                    'max_drawdown': doc.get('max_drawdown'),
                    'ranking': doc.get('ranking')
                })
            
            # è·å–æœ€å°å›æ’¤TOP10(æˆç«‹ä»¥æ¥)
            pipeline_min_drawdown = [
                {'$match': {'å‘¨æœŸ': 'æˆç«‹ä»¥æ¥', 'æœ¬äº§å“æœ€å¤§å›æ’’': {'$ne': None}}},
                {'$sort': {'æœ¬äº§å“æœ€å¤§å›æ’’': 1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$åŸºé‡‘ä»£ç ',
                    'return': '$æœ¬äº§å“åŒºé—´æ”¶ç›Š',
                    'max_drawdown': '$æœ¬äº§å“æœ€å¤§å›æ’’',
                    'ranking': '$å‘¨æœŸæ”¶ç›ŠåŒç±»æ’å'
                }}
            ]
            
            min_drawdown_since = []
            async for doc in self.col_fund_individual_achievement_xq.aggregate(pipeline_min_drawdown):
                min_drawdown_since.append({
                    'code': doc.get('code'),
                    'return': doc.get('return'),
                    'max_drawdown': doc.get('max_drawdown'),
                    'ranking': doc.get('ranking')
                })
            
            return {
                'total_count': total_count,
                'unique_funds': unique_funds,
                'performance_types': performance_types,
                'top_return_since': top_return_since,
                'min_drawdown_since': min_drawdown_since
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘ä¸šç»©ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_value_estimation_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜å‡€å€¼ä¼°ç®—æ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰å‡€å€¼ä¼°ç®—æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡å‡€å€¼ä¼°ç®—æ•°æ®...")
            
            batch_size = 1000  # æ¯æ‰¹å¤„ç†1000æ¡è®°å½•
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    estimation_date = str(doc.get('æ—¥æœŸ', ''))  # ä½¿ç”¨æ–°å¢çš„æ—¥æœŸå­—æ®µ
                    doc['code'] = fund_code
                    doc['date'] = estimation_date
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_value_estimation_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä»¥æ—¥æœŸ+åŸºé‡‘ä»£ç ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'date': estimation_date},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_value_estimation_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡å‡€å€¼ä¼°ç®—æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜å‡€å€¼ä¼°ç®—æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_value_estimation_em_data(self) -> int:
        """æ¸…ç©ºå‡€å€¼ä¼°ç®—æ•°æ®"""
        try:
            result = await self.col_fund_value_estimation_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡å‡€å€¼ä¼°ç®—æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºå‡€å€¼ä¼°ç®—æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_value_estimation_em_stats(self) -> Dict[str, Any]:
        """è·å–å‡€å€¼ä¼°ç®—ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_value_estimation_em.count_documents({})
            
            # è·å–æ—¥æœŸèŒƒå›´ï¼ˆä½¿ç”¨æ–°çš„æ—¥æœŸå­—æ®µï¼‰
            pipeline_date = [
                {'$group': {'_id': None, 'earliest': {'$min': '$date'}, 'latest': {'$max': '$date'}}}
            ]
            
            earliest_date = None
            latest_date = None
            async for doc in self.col_fund_value_estimation_em.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            # è·å–ä¼°ç®—å¢é•¿ç‡TOP10ï¼ˆä½¿ç”¨æ–°çš„å­—æ®µåï¼šå»é™¤æ—¥æœŸå‰ç¼€ï¼‰
            # ç®€åŒ–æŸ¥è¯¢ï¼šç›´æ¥è¿”å›æ•°æ®ï¼Œä¸åœ¨æ•°æ®åº“å±‚é¢æ’åº
            pipeline_top_growth = [
                {'$match': {'ä¼°ç®—æ•°æ®-ä¼°ç®—å¢é•¿ç‡': {'$ne': None, '$ne': '', '$exists': True}}},
                {'$limit': 100},  # å…ˆè·å–100æ¡
                {'$project': {
                    'code': '$åŸºé‡‘ä»£ç ',
                    'name': '$åŸºé‡‘åç§°',
                    'date': '$æ—¥æœŸ',
                    'estimated_value': '$ä¼°ç®—æ•°æ®-ä¼°ç®—å€¼',
                    'estimated_growth': '$ä¼°ç®—æ•°æ®-ä¼°ç®—å¢é•¿ç‡',
                    'published_nav': '$å…¬å¸ƒæ•°æ®-å•ä½å‡€å€¼',
                    'deviation': '$ä¼°ç®—åå·®'
                }}
            ]
            
            top_estimated_growth = []
            async for doc in self.col_fund_value_estimation_em.aggregate(pipeline_top_growth):
                top_estimated_growth.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'date': doc.get('date'),
                    'estimated_value': doc.get('estimated_value'),
                    'estimated_growth': doc.get('estimated_growth'),
                    'published_nav': doc.get('published_nav'),
                    'deviation': doc.get('deviation')
                })
            
            # è·å–ä¼°ç®—åå·®æœ€å°TOP10ï¼ˆç»å¯¹å€¼ï¼‰
            # ç®€åŒ–æŸ¥è¯¢ï¼šç›´æ¥è¿”å›æ•°æ®ï¼Œä¸åœ¨æ•°æ®åº“å±‚é¢æ’åº
            pipeline_min_deviation = [
                {'$match': {'ä¼°ç®—åå·®': {'$ne': None, '$ne': '', '$exists': True}}},
                {'$limit': 100},  # å…ˆè·å–100æ¡
                {'$project': {
                    'code': '$åŸºé‡‘ä»£ç ',
                    'name': '$åŸºé‡‘åç§°',
                    'date': '$æ—¥æœŸ',
                    'estimated_value': '$ä¼°ç®—æ•°æ®-ä¼°ç®—å€¼',
                    'estimated_growth': '$ä¼°ç®—æ•°æ®-ä¼°ç®—å¢é•¿ç‡',
                    'published_nav': '$å…¬å¸ƒæ•°æ®-å•ä½å‡€å€¼',
                    'deviation': '$ä¼°ç®—åå·®'
                }}
            ]
            
            min_deviation_funds = []
            async for doc in self.col_fund_value_estimation_em.aggregate(pipeline_min_deviation):
                min_deviation_funds.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'date': doc.get('date'),
                    'estimated_value': doc.get('estimated_value'),
                    'estimated_growth': doc.get('estimated_growth'),
                    'published_nav': doc.get('published_nav'),
                    'deviation': doc.get('deviation')
                })
            
            return {
                'total_count': total_count,
                'earliest_date': earliest_date,
                'latest_date': latest_date,
                'top_estimated_growth': top_estimated_growth,
                'min_deviation_funds': min_deviation_funds
            }
        except Exception as e:
            logger.error(f"è·å–å‡€å€¼ä¼°ç®—ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_individual_analysis_xq_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘æ•°æ®åˆ†æåˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘æ•°æ®åˆ†æéœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘æ•°æ®åˆ†æ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    period = str(doc.get('å‘¨æœŸ', ''))
                    doc['code'] = fund_code
                    doc['period'] = period
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_individual_analysis_xq'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'period': period},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_individual_analysis_xq.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘æ•°æ®åˆ†æ")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘æ•°æ®åˆ†æå¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_individual_analysis_xq_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘æ•°æ®åˆ†æ"""
        try:
            result = await self.col_fund_individual_analysis_xq.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘æ•°æ®åˆ†æ")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘æ•°æ®åˆ†æå¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_individual_analysis_xq_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘æ•°æ®åˆ†æç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_individual_analysis_xq.count_documents({})
            
            # è·å–å”¯ä¸€åŸºé‡‘æ•°
            unique_funds = await self.col_fund_individual_analysis_xq.distinct('code')
            
            # è·å–å‘¨æœŸåˆ†å¸ƒ
            pipeline_periods = [
                {'$group': {'_id': '$å‘¨æœŸ', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            period_distribution = []
            async for doc in self.col_fund_individual_analysis_xq.aggregate(pipeline_periods):
                period_distribution.append({
                    'period': doc['_id'],
                    'count': doc['count']
                })
            
            # è·å–å¹´åŒ–å¤æ™®æ¯”ç‡TOP10
            pipeline_top_sharpe = [
                {'$match': {'å¹´åŒ–å¤æ™®æ¯”ç‡': {'$ne': None}}},
                {'$sort': {'å¹´åŒ–å¤æ™®æ¯”ç‡': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$code',
                    'period': '$å‘¨æœŸ',
                    'sharpe_ratio': '$å¹´åŒ–å¤æ™®æ¯”ç‡',
                    'volatility': '$å¹´åŒ–æ³¢åŠ¨ç‡',
                    'max_drawdown': '$æœ€å¤§å›æ’¤',
                    'risk_return_ratio': '$è¾ƒåŒç±»é£é™©æ”¶ç›Šæ¯”'
                }}
            ]
            
            top_sharpe_ratio = []
            async for doc in self.col_fund_individual_analysis_xq.aggregate(pipeline_top_sharpe):
                top_sharpe_ratio.append({
                    'code': doc.get('code'),
                    'period': doc.get('period'),
                    'sharpe_ratio': doc.get('sharpe_ratio'),
                    'volatility': doc.get('volatility'),
                    'max_drawdown': doc.get('max_drawdown'),
                    'risk_return_ratio': doc.get('risk_return_ratio')
                })
            
            # è·å–æœ€å°å›æ’¤TOP10ï¼ˆæœ€å¤§å›æ’¤çš„ç»å¯¹å€¼æœ€å°ï¼‰
            pipeline_min_drawdown = [
                {'$match': {'æœ€å¤§å›æ’¤': {'$ne': None}}},
                {'$addFields': {'abs_drawdown': {'$abs': '$æœ€å¤§å›æ’¤'}}},
                {'$sort': {'abs_drawdown': 1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$code',
                    'period': '$å‘¨æœŸ',
                    'max_drawdown': '$æœ€å¤§å›æ’¤',
                    'sharpe_ratio': '$å¹´åŒ–å¤æ™®æ¯”ç‡',
                    'volatility': '$å¹´åŒ–æ³¢åŠ¨ç‡',
                    'anti_risk': '$è¾ƒåŒç±»æŠ—é£é™©æ³¢åŠ¨'
                }}
            ]
            
            min_drawdown_funds = []
            async for doc in self.col_fund_individual_analysis_xq.aggregate(pipeline_min_drawdown):
                min_drawdown_funds.append({
                    'code': doc.get('code'),
                    'period': doc.get('period'),
                    'max_drawdown': doc.get('max_drawdown'),
                    'sharpe_ratio': doc.get('sharpe_ratio'),
                    'volatility': doc.get('volatility'),
                    'anti_risk': doc.get('anti_risk')
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'period_distribution': period_distribution,
                'top_sharpe_ratio': top_sharpe_ratio,
                'min_drawdown_funds': min_drawdown_funds
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘æ•°æ®åˆ†æç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_individual_profit_probability_xq_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘ç›ˆåˆ©æ¦‚ç‡åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘ç›ˆåˆ©æ¦‚ç‡éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘ç›ˆåˆ©æ¦‚ç‡...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    holding_period = str(doc.get('æŒæœ‰æ—¶é•¿', ''))
                    date = str(doc.get('æ—¥æœŸ', ''))
                    doc['code'] = fund_code
                    doc['holding_period'] = holding_period
                    doc['date'] = date
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_individual_profit_probability_xq'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨æ—¥æœŸã€åŸºé‡‘ä»£ç å’ŒæŒæœ‰æ—¶é•¿ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'holding_period': holding_period, 'date': date},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_individual_profit_probability_xq.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘ç›ˆåˆ©æ¦‚ç‡")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘ç›ˆåˆ©æ¦‚ç‡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_individual_profit_probability_xq_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘ç›ˆåˆ©æ¦‚ç‡"""
        try:
            result = await self.col_fund_individual_profit_probability_xq.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘ç›ˆåˆ©æ¦‚ç‡")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘ç›ˆåˆ©æ¦‚ç‡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_individual_profit_probability_xq_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘ç›ˆåˆ©æ¦‚ç‡ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_individual_profit_probability_xq.count_documents({})
            
            # è·å–å”¯ä¸€åŸºé‡‘æ•°
            unique_funds = await self.col_fund_individual_profit_probability_xq.distinct('code')
            
            # è·å–æŒæœ‰æ—¶é•¿åˆ†å¸ƒ
            pipeline_periods = [
                {'$group': {'_id': '$æŒæœ‰æ—¶é•¿', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            holding_period_distribution = []
            async for doc in self.col_fund_individual_profit_probability_xq.aggregate(pipeline_periods):
                holding_period_distribution.append({
                    'holding_period': doc['_id'],
                    'count': doc['count']
                })
            
            # è·å–ç›ˆåˆ©æ¦‚ç‡TOP10ï¼ˆé•¿æœŸæŒæœ‰ï¼‰
            pipeline_top_probability = [
                {'$match': {'ç›ˆåˆ©æ¦‚ç‡': {'$ne': None}}},
                {'$sort': {'ç›ˆåˆ©æ¦‚ç‡': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$code',
                    'holding_period': '$æŒæœ‰æ—¶é•¿',
                    'profit_probability': '$ç›ˆåˆ©æ¦‚ç‡',
                    'average_return': '$å¹³å‡æ”¶ç›Š'
                }}
            ]
            
            top_profit_probability = []
            async for doc in self.col_fund_individual_profit_probability_xq.aggregate(pipeline_top_probability):
                top_profit_probability.append({
                    'code': doc.get('code'),
                    'holding_period': doc.get('holding_period'),
                    'profit_probability': doc.get('profit_probability'),
                    'average_return': doc.get('average_return')
                })
            
            # è·å–å¹³å‡æ”¶ç›ŠTOP10
            pipeline_top_return = [
                {'$match': {'å¹³å‡æ”¶ç›Š': {'$ne': None}}},
                {'$sort': {'å¹³å‡æ”¶ç›Š': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$code',
                    'holding_period': '$æŒæœ‰æ—¶é•¿',
                    'profit_probability': '$ç›ˆåˆ©æ¦‚ç‡',
                    'average_return': '$å¹³å‡æ”¶ç›Š'
                }}
            ]
            
            top_average_return = []
            async for doc in self.col_fund_individual_profit_probability_xq.aggregate(pipeline_top_return):
                top_average_return.append({
                    'code': doc.get('code'),
                    'holding_period': doc.get('holding_period'),
                    'profit_probability': doc.get('profit_probability'),
                    'average_return': doc.get('average_return')
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'holding_period_distribution': holding_period_distribution,
                'top_profit_probability': top_profit_probability,
                'top_average_return': top_average_return
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘ç›ˆåˆ©æ¦‚ç‡ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_individual_detail_hold_xq_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘æŒä»“èµ„äº§æ¯”ä¾‹åˆ°MongoDB
        
        æ•°æ®ç»“æ„ï¼šå°†DataFrameçš„æ•°æ®è½¬æ¢ä¸ºä¸€ä¸ªæ–‡æ¡£ï¼Œä»“ä½ä¿¡æ¯å­—æ®µæ˜¯å­—å…¸æ ¼å¼ï¼ˆå…¨éƒ¨ä½¿ç”¨ä¸­æ–‡å­—æ®µï¼‰
        {
            "åŸºé‡‘ä»£ç ": "000001",
            "æ—¥æœŸ": "2024-03-30",
            "æŒä»“ä¿¡æ¯": {
                "è‚¡ç¥¨": 85.5,
                "å€ºåˆ¸": 10.2,
                "ç°é‡‘": 4.3
            },
            "æ•°æ®æº": "akshare",
            "æ¥å£åç§°": "fund_individual_detail_hold_xq",
            "æ›´æ–°æ—¶é—´": "2024-03-30T12:00:00"
        }
        
        å”¯ä¸€æ ‡è¯†ï¼šåŸºé‡‘ä»£ç  + æ—¥æœŸ
        """
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘æŒä»“èµ„äº§æ¯”ä¾‹éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            # è·å–åŸºé‡‘ä»£ç å’Œæ—¥æœŸï¼ˆå‡è®¾DataFrameä¸­æ‰€æœ‰è¡Œçš„åŸºé‡‘ä»£ç å’Œæ—¥æœŸç›¸åŒï¼‰
            fund_code = str(df['åŸºé‡‘ä»£ç '].iloc[0]) if 'åŸºé‡‘ä»£ç ' in df.columns else ''
            date_str = str(df['æ—¥æœŸ'].iloc[0]) if 'æ—¥æœŸ' in df.columns else ''
            
            # å°†DataFrameè½¬æ¢ä¸ºå­—å…¸ï¼šèµ„äº§ç±»å‹ä¸ºkeyï¼Œä»“ä½å æ¯”ä¸ºvalue
            holdings = {}
            if 'èµ„äº§ç±»å‹' in df.columns and 'ä»“ä½å æ¯”' in df.columns:
                for _, row in df.iterrows():
                    asset_type = str(row.get('èµ„äº§ç±»å‹', ''))
                    position = row.get('ä»“ä½å æ¯”')
                    if asset_type and position is not None:
                        # è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                        try:
                            holdings[asset_type] = float(position) if not pd.isna(position) else None
                        except (ValueError, TypeError):
                            holdings[asset_type] = None
            
            logger.info(f"ğŸ“Š å¤„ç†åŸºé‡‘ {fund_code} åœ¨ {date_str} çš„æŒä»“æ•°æ®: {len(holdings)} ç§èµ„äº§ç±»å‹")
            
            # æ„å»ºæ–‡æ¡£ï¼ˆå…¨éƒ¨ä½¿ç”¨ä¸­æ–‡å­—æ®µåï¼‰
            doc = {
                'åŸºé‡‘ä»£ç ': fund_code,
                'æ—¥æœŸ': date_str,
                'æŒä»“ä¿¡æ¯': holdings,  # ä»“ä½ä¿¡æ¯å­—å…¸ï¼Œä»¥èµ„äº§ç±»å‹ä¸ºkeyï¼Œä»“ä½å æ¯”ä¸ºvalue
                'æ•°æ®æº': 'akshare',
                'æ¥å£åç§°': 'fund_individual_detail_hold_xq',
                'æ›´æ–°æ—¶é—´': datetime.now().isoformat()
            }
            
            # ä½¿ç”¨ åŸºé‡‘ä»£ç  + æ—¥æœŸ ä½œä¸ºå”¯ä¸€æ ‡è¯†
            result = await self.col_fund_individual_detail_hold_xq.update_one(
                {'åŸºé‡‘ä»£ç ': fund_code, 'æ—¥æœŸ': date_str},
                {'$set': doc},
                upsert=True
            )
            
            saved = 1 if result.upserted_id or result.modified_count > 0 else 0
            
            if progress_callback:
                progress_callback(
                    current=1,
                    total=1,
                    percentage=100,
                    message=f"å·²ä¿å­˜åŸºé‡‘ {fund_code} åœ¨ {date_str} çš„æŒä»“æ•°æ®"
                )
            
            logger.info(f"ğŸ‰ æˆåŠŸä¿å­˜åŸºé‡‘ {fund_code} åœ¨ {date_str} çš„æŒä»“æ•°æ®")
            return saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘æŒä»“èµ„äº§æ¯”ä¾‹å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_individual_detail_hold_xq_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘æŒä»“èµ„äº§æ¯”ä¾‹"""
        try:
            result = await self.col_fund_individual_detail_hold_xq.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘æŒä»“èµ„äº§æ¯”ä¾‹")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘æŒä»“èµ„äº§æ¯”ä¾‹å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_individual_detail_hold_xq_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘æŒä»“èµ„äº§æ¯”ä¾‹ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_individual_detail_hold_xq.count_documents({})
            
            # è·å–å”¯ä¸€åŸºé‡‘æ•°ï¼ˆä½¿ç”¨ä¸­æ–‡å­—æ®µåï¼‰
            unique_funds = await self.col_fund_individual_detail_hold_xq.distinct('åŸºé‡‘ä»£ç ')
            
            # è·å–å”¯ä¸€æ—¥æœŸæ•°ï¼ˆä½¿ç”¨ä¸­æ–‡å­—æ®µåï¼‰
            unique_dates = await self.col_fund_individual_detail_hold_xq.distinct('æ—¥æœŸ')
            
            # æ³¨æ„ï¼šæ–°çš„æ•°æ®ç»“æ„ä¸­ï¼Œèµ„äº§ç±»å‹å­˜å‚¨åœ¨æŒä»“ä¿¡æ¯å­—å…¸çš„keyä¸­ï¼Œä¸å†æ˜¯å•ç‹¬å­—æ®µ
            # ç»Ÿè®¡åŠŸèƒ½éœ€è¦é‡æ–°è®¾è®¡ï¼Œè¿™é‡Œå…ˆè¿”å›åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            
            # è·å–æœ€æ–°æ—¥æœŸ
            pipeline_latest_date = [
                {'$sort': {'æ—¥æœŸ': -1}},
                {'$limit': 1},
                {'$project': {'æ—¥æœŸ': 1}}
            ]
            
            latest_date = None
            async for doc in self.col_fund_individual_detail_hold_xq.aggregate(pipeline_latest_date):
                latest_date = doc.get('æ—¥æœŸ')
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'unique_dates': len(unique_dates),
                'latest_date': latest_date
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘æŒä»“èµ„äº§æ¯”ä¾‹ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_overview_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘åŸºæœ¬æ¦‚å†µåˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘åŸºæœ¬æ¦‚å†µéœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘åŸºæœ¬æ¦‚å†µ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_overview_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_overview_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘åŸºæœ¬æ¦‚å†µ")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘åŸºæœ¬æ¦‚å†µå¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_overview_em_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘åŸºæœ¬æ¦‚å†µ"""
        try:
            result = await self.col_fund_overview_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘åŸºæœ¬æ¦‚å†µ")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘åŸºæœ¬æ¦‚å†µå¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_overview_em_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘åŸºæœ¬æ¦‚å†µç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_overview_em.count_documents({})
            
            # è·å–åŸºé‡‘ç±»å‹åˆ†å¸ƒ
            pipeline_fund_types = [
                {'$group': {'_id': '$åŸºé‡‘ç±»å‹', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            fund_type_distribution = []
            async for doc in self.col_fund_overview_em.aggregate(pipeline_fund_types):
                fund_type_distribution.append({
                    'fund_type': doc['_id'],
                    'count': doc['count']
                })
            
            # è·å–åŸºé‡‘ç®¡ç†äººåˆ†å¸ƒï¼ˆTOP10ï¼‰
            pipeline_managers = [
                {'$group': {'_id': '$åŸºé‡‘ç®¡ç†äºº', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}},
                {'$limit': 10}
            ]
            
            top_managers = []
            async for doc in self.col_fund_overview_em.aggregate(pipeline_managers):
                top_managers.append({
                    'manager': doc['_id'],
                    'count': doc['count']
                })
            
            # è·å–åŸºé‡‘è§„æ¨¡TOP10
            pipeline_top_scale = [
                {'$match': {'åŸºé‡‘è§„æ¨¡': {'$ne': None}}},
                {'$sort': {'åŸºé‡‘è§„æ¨¡': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$code',
                    'name': '$åŸºé‡‘ç®€ç§°',
                    'scale': '$åŸºé‡‘è§„æ¨¡',
                    'manager': '$åŸºé‡‘ç®¡ç†äºº',
                    'fund_type': '$åŸºé‡‘ç±»å‹'
                }}
            ]
            
            top_scale_funds = []
            async for doc in self.col_fund_overview_em.aggregate(pipeline_top_scale):
                top_scale_funds.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'scale': doc.get('scale'),
                    'manager': doc.get('manager'),
                    'fund_type': doc.get('fund_type')
                })
            
            # è·å–æˆç«‹æ—¥æœŸæœ€æ—©çš„åŸºé‡‘TOP10
            pipeline_oldest = [
                {'$match': {'æˆç«‹æ—¥æœŸ': {'$ne': None}}},
                {'$sort': {'æˆç«‹æ—¥æœŸ': 1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$code',
                    'name': '$åŸºé‡‘ç®€ç§°',
                    'established_date': '$æˆç«‹æ—¥æœŸ',
                    'manager': '$åŸºé‡‘ç®¡ç†äºº',
                    'fund_type': '$åŸºé‡‘ç±»å‹'
                }}
            ]
            
            oldest_funds = []
            async for doc in self.col_fund_overview_em.aggregate(pipeline_oldest):
                oldest_funds.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'established_date': doc.get('established_date'),
                    'manager': doc.get('manager'),
                    'fund_type': doc.get('fund_type')
                })
            
            return {
                'total_count': total_count,
                'fund_type_distribution': fund_type_distribution,
                'top_managers': top_managers,
                'top_scale_funds': top_scale_funds,
                'oldest_funds': oldest_funds
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘åŸºæœ¬æ¦‚å†µç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_fee_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘äº¤æ˜“è´¹ç‡åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘äº¤æ˜“è´¹ç‡éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘äº¤æ˜“è´¹ç‡...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    fee_type = str(doc.get('è´¹ç”¨ç±»å‹', ''))
                    condition = str(doc.get('æ¡ä»¶', ''))
                    doc['code'] = fund_code
                    doc['fee_type'] = fee_type
                    doc['condition'] = condition
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_fee_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'fee_type': fee_type, 'condition': condition},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_fee_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘äº¤æ˜“è´¹ç‡")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘äº¤æ˜“è´¹ç‡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_fee_em_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘äº¤æ˜“è´¹ç‡"""
        try:
            result = await self.col_fund_fee_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘äº¤æ˜“è´¹ç‡")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘äº¤æ˜“è´¹ç‡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_fee_em_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘äº¤æ˜“è´¹ç‡ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_fee_em.count_documents({})
            
            # è·å–å”¯ä¸€åŸºé‡‘æ•°
            unique_funds = await self.col_fund_fee_em.distinct('code')
            
            # è·å–è´¹ç”¨ç±»å‹åˆ†å¸ƒ
            pipeline_fee_types = [
                {'$group': {'_id': '$è´¹ç”¨ç±»å‹', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            fee_type_distribution = []
            async for doc in self.col_fund_fee_em.aggregate(pipeline_fee_types):
                fee_type_distribution.append({
                    'fee_type': doc['_id'],
                    'count': doc['count']
                })
            
            # è·å–ç”³è´­è´¹æœ€ä½çš„åŸºé‡‘TOP10
            pipeline_lowest_purchase = [
                {'$match': {'è´¹ç”¨ç±»å‹': 'ç”³è´­è´¹', 'ä¼˜æƒ è´¹ç‡': {'$ne': None}}},
                {'$group': {'_id': '$code', 'avg_fee': {'$avg': '$ä¼˜æƒ è´¹ç‡'}}},
                {'$sort': {'avg_fee': 1}},
                {'$limit': 10}
            ]
            
            lowest_purchase_fee_funds = []
            async for doc in self.col_fund_fee_em.aggregate(pipeline_lowest_purchase):
                lowest_purchase_fee_funds.append({
                    'code': doc['_id'],
                    'avg_fee': round(doc.get('avg_fee', 0), 3) if doc.get('avg_fee') else None
                })
            
            # è·å–èµå›è´¹æœ€ä½çš„åŸºé‡‘TOP10
            pipeline_lowest_redemption = [
                {'$match': {'è´¹ç”¨ç±»å‹': 'èµå›è´¹', 'è´¹ç‡': {'$ne': None}}},
                {'$group': {'_id': '$code', 'avg_fee': {'$avg': '$è´¹ç‡'}}},
                {'$sort': {'avg_fee': 1}},
                {'$limit': 10}
            ]
            
            lowest_redemption_fee_funds = []
            async for doc in self.col_fund_fee_em.aggregate(pipeline_lowest_redemption):
                lowest_redemption_fee_funds.append({
                    'code': doc['_id'],
                    'avg_fee': round(doc.get('avg_fee', 0), 3) if doc.get('avg_fee') else None
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'fee_type_distribution': fee_type_distribution,
                'lowest_purchase_fee_funds': lowest_purchase_fee_funds,
                'lowest_redemption_fee_funds': lowest_redemption_fee_funds
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘äº¤æ˜“è´¹ç‡ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_individual_detail_info_xq_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘äº¤æ˜“è§„åˆ™åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘äº¤æ˜“è§„åˆ™éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘äº¤æ˜“è§„åˆ™...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    fee_type = str(doc.get('è´¹ç”¨ç±»å‹', ''))
                    doc['code'] = fund_code
                    doc['fee_type'] = fee_type
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_individual_detail_info_xq'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'fee_type': fee_type},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_individual_detail_info_xq.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘äº¤æ˜“è§„åˆ™")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘äº¤æ˜“è§„åˆ™å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_individual_detail_info_xq_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘äº¤æ˜“è§„åˆ™"""
        try:
            result = await self.col_fund_individual_detail_info_xq.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘äº¤æ˜“è§„åˆ™")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘äº¤æ˜“è§„åˆ™å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_individual_detail_info_xq_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘äº¤æ˜“è§„åˆ™ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_individual_detail_info_xq.count_documents({})
            
            # è·å–å”¯ä¸€åŸºé‡‘æ•°
            unique_funds = await self.col_fund_individual_detail_info_xq.distinct('code')
            
            # è·å–è´¹ç”¨ç±»å‹åˆ†å¸ƒ
            pipeline_fee_types = [
                {'$group': {'_id': '$è´¹ç”¨ç±»å‹', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            fee_type_distribution = []
            async for doc in self.col_fund_individual_detail_info_xq.aggregate(pipeline_fee_types):
                fee_type_distribution.append({
                    'fee_type': doc['_id'],
                    'count': doc['count']
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'fee_type_distribution': fee_type_distribution
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘äº¤æ˜“è§„åˆ™ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_portfolio_hold_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘æŒä»“åˆ°MongoDB
        
        æ•°æ®ç»“æ„ï¼ˆå…¨éƒ¨ä½¿ç”¨ä¸­æ–‡å­—æ®µï¼‰ï¼š
        {
            "åŸºé‡‘ä»£ç ": "000001",
            "è‚¡ç¥¨ä»£ç ": "600519",
            "è‚¡ç¥¨åç§°": "è´µå·èŒ…å°",
            "å­£åº¦": "2024-09-30",
            "æŒä»“å æ¯”": 8.5,
            "æŒä»“å¸‚å€¼": 12500000.0,
            "æ•°æ®æº": "akshare",
            "æ¥å£åç§°": "fund_portfolio_hold_em",
            "æ›´æ–°æ—¶é—´": "2024-11-24T23:38:00"
        }
        
        å”¯ä¸€æ ‡è¯†ï¼šåŸºé‡‘ä»£ç  + è‚¡ç¥¨ä»£ç  + å­£åº¦
        """
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘æŒä»“éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘æŒä»“...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # è·å–å…³é”®å­—æ®µï¼ˆå…¨éƒ¨ä½¿ç”¨ä¸­æ–‡å­—æ®µåï¼‰
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    stock_code = str(doc.get('è‚¡ç¥¨ä»£ç ', ''))
                    quarter = str(doc.get('å­£åº¦', ''))
                    
                    # æ·»åŠ å…ƒæ•°æ®å­—æ®µï¼ˆä¸­æ–‡ï¼‰
                    doc['æ•°æ®æº'] = 'akshare'
                    doc['æ¥å£åç§°'] = 'fund_portfolio_hold_em'
                    doc['æ›´æ–°æ—¶é—´'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç  + è‚¡ç¥¨ä»£ç  + å­£åº¦ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    ops.append(
                        UpdateOne(
                            {'åŸºé‡‘ä»£ç ': fund_code, 'è‚¡ç¥¨ä»£ç ': stock_code, 'å­£åº¦': quarter},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_portfolio_hold_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘æŒä»“")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘æŒä»“å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_portfolio_hold_em_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘æŒä»“"""
        try:
            result = await self.col_fund_portfolio_hold_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘æŒä»“")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘æŒä»“å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_portfolio_hold_em_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘æŒä»“ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_portfolio_hold_em.count_documents({})
            
            # è·å–å”¯ä¸€åŸºé‡‘æ•°ï¼ˆä½¿ç”¨ä¸­æ–‡å­—æ®µåï¼‰
            unique_funds = await self.col_fund_portfolio_hold_em.distinct('åŸºé‡‘ä»£ç ')
            
            # è·å–å”¯ä¸€è‚¡ç¥¨æ•°ï¼ˆä½¿ç”¨ä¸­æ–‡å­—æ®µåï¼‰
            unique_stocks = await self.col_fund_portfolio_hold_em.distinct('è‚¡ç¥¨ä»£ç ')
            
            # è·å–å­£åº¦åˆ†å¸ƒ
            pipeline_quarters = [
                {'$group': {'_id': '$å­£åº¦', 'count': {'$sum': 1}}},
                {'$sort': {'_id': -1}}
            ]
            
            quarter_distribution = []
            async for doc in self.col_fund_portfolio_hold_em.aggregate(pipeline_quarters):
                quarter_distribution.append({
                    'quarter': doc['_id'],
                    'count': doc['count']
                })
            
            # è·å–æœ€å—åŸºé‡‘é’ççš„è‚¡ç¥¨TOP10
            pipeline_top_stocks = [
                {'$group': {'_id': '$è‚¡ç¥¨ä»£ç ', 'stock_name': {'$first': '$è‚¡ç¥¨åç§°'}, 'fund_count': {'$sum': 1}}},
                {'$sort': {'fund_count': -1}},
                {'$limit': 10}
            ]
            
            top_stocks = []
            async for doc in self.col_fund_portfolio_hold_em.aggregate(pipeline_top_stocks):
                top_stocks.append({
                    'stock_code': doc['_id'],
                    'stock_name': doc.get('stock_name'),
                    'fund_count': doc['fund_count']
                })
            
            # è·å–æŒä»“å æ¯”æœ€é«˜çš„è®°å½•TOP10
            pipeline_top_holdings = [
                {'$match': {'æŒä»“å æ¯”': {'$ne': None}}},
                {'$sort': {'æŒä»“å æ¯”': -1}},
                {'$limit': 10},
                {'$project': {'åŸºé‡‘ä»£ç ': 1, 'è‚¡ç¥¨ä»£ç ': 1, 'è‚¡ç¥¨åç§°': 1, 'å­£åº¦': 1, 'æŒä»“å æ¯”': 1}}
            ]
            
            top_holdings = []
            async for doc in self.col_fund_portfolio_hold_em.aggregate(pipeline_top_holdings):
                top_holdings.append({
                    'fund_code': doc.get('åŸºé‡‘ä»£ç '),
                    'stock_code': doc.get('è‚¡ç¥¨ä»£ç '),
                    'stock_name': doc.get('è‚¡ç¥¨åç§°'),
                    'quarter': doc.get('å­£åº¦'),
                    'holding_ratio': doc.get('æŒä»“å æ¯”')
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'unique_stocks': len(unique_stocks),
                'quarter_distribution': quarter_distribution,
                'top_stocks': top_stocks,
                'top_holdings': top_holdings
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘æŒä»“ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_portfolio_bond_hold_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜å€ºåˆ¸æŒä»“åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰å€ºåˆ¸æŒä»“éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡å€ºåˆ¸æŒä»“...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # ä½¿ç”¨ä¸­æ–‡å­—æ®µå
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    bond_code = str(doc.get('å€ºåˆ¸ä»£ç ', ''))
                    quarter = str(doc.get('å­£åº¦', ''))
                    
                    # æ·»åŠ å…ƒæ•°æ®å­—æ®µï¼ˆä½¿ç”¨ä¸­æ–‡ï¼‰
                    doc['æ•°æ®æº'] = 'akshare'
                    doc['æ¥å£åç§°'] = 'fund_portfolio_bond_hold_em'
                    doc['æ›´æ–°æ—¶é—´'] = datetime.now().isoformat()
                    
                    # åˆ é™¤åºå·å­—æ®µï¼ˆä¸éœ€è¦ä¿å­˜ï¼‰
                    doc.pop('åºå·', None)
                    
                    ops.append(
                        UpdateOne(
                            {'åŸºé‡‘ä»£ç ': fund_code, 'å€ºåˆ¸ä»£ç ': bond_code, 'å­£åº¦': quarter},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_portfolio_bond_hold_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡å€ºåˆ¸æŒä»“")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜å€ºåˆ¸æŒä»“å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_portfolio_bond_hold_em_data(self) -> int:
        """æ¸…ç©ºå€ºåˆ¸æŒä»“"""
        try:
            result = await self.col_fund_portfolio_bond_hold_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡å€ºåˆ¸æŒä»“")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºå€ºåˆ¸æŒä»“å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_portfolio_bond_hold_em_stats(self) -> Dict[str, Any]:
        """è·å–å€ºåˆ¸æŒä»“ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_portfolio_bond_hold_em.count_documents({})
            
            # ä½¿ç”¨ä¸­æ–‡å­—æ®µåè¿›è¡Œç»Ÿè®¡
            unique_funds = await self.col_fund_portfolio_bond_hold_em.distinct('åŸºé‡‘ä»£ç ')
            unique_bonds = await self.col_fund_portfolio_bond_hold_em.distinct('å€ºåˆ¸ä»£ç ')
            
            pipeline_quarters = [
                {'$group': {'_id': '$å­£åº¦', 'count': {'$sum': 1}}},
                {'$sort': {'_id': -1}}
            ]
            
            quarter_distribution = []
            async for doc in self.col_fund_portfolio_bond_hold_em.aggregate(pipeline_quarters):
                quarter_distribution.append({
                    'quarter': doc['_id'],
                    'count': doc['count']
                })
            
            pipeline_top_bonds = [
                {'$group': {'_id': '$å€ºåˆ¸ä»£ç ', 'bond_name': {'$first': '$å€ºåˆ¸åç§°'}, 'fund_count': {'$sum': 1}}},
                {'$sort': {'fund_count': -1}},
                {'$limit': 10}
            ]
            
            top_bonds = []
            async for doc in self.col_fund_portfolio_bond_hold_em.aggregate(pipeline_top_bonds):
                top_bonds.append({
                    'bond_code': doc['_id'],
                    'bond_name': doc.get('bond_name'),
                    'fund_count': doc['fund_count']
                })
            
            pipeline_top_holdings = [
                {'$match': {'æŒä»“å æ¯”': {'$ne': None}}},
                {'$sort': {'æŒä»“å æ¯”': -1}},
                {'$limit': 10},
                {'$project': {'åŸºé‡‘ä»£ç ': 1, 'å€ºåˆ¸ä»£ç ': 1, 'å€ºåˆ¸åç§°': 1, 'å­£åº¦': 1, 'æŒä»“å æ¯”': 1}}
            ]
            
            top_holdings = []
            async for doc in self.col_fund_portfolio_bond_hold_em.aggregate(pipeline_top_holdings):
                top_holdings.append({
                    'fund_code': doc.get('åŸºé‡‘ä»£ç '),
                    'bond_code': doc.get('å€ºåˆ¸ä»£ç '),
                    'bond_name': doc.get('å€ºåˆ¸åç§°'),
                    'quarter': doc.get('å­£åº¦'),
                    'holding_ratio': doc.get('æŒä»“å æ¯”')
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'unique_bonds': len(unique_bonds),
                'quarter_distribution': quarter_distribution,
                'top_bonds': top_bonds,
                'top_holdings': top_holdings
            }
        except Exception as e:
            logger.error(f"è·å–å€ºåˆ¸æŒä»“ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_portfolio_industry_allocation_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜è¡Œä¸šé…ç½®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰è¡Œä¸šé…ç½®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡è¡Œä¸šé…ç½®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    industry = str(doc.get('è¡Œä¸šç±»åˆ«', ''))
                    end_date = str(doc.get('æˆªæ­¢æ—¶é—´', ''))
                    
                    # æ·»åŠ å…ƒæ•°æ®å­—æ®µï¼ˆä½¿ç”¨ä¸­æ–‡ï¼‰
                    doc['æ•°æ®æº'] = 'akshare'
                    doc['æ¥å£åç§°'] = 'fund_portfolio_industry_allocation_em'
                    doc['æ›´æ–°æ—¶é—´'] = datetime.now().isoformat()
                    
                    # åˆ é™¤åºå·å­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    doc.pop('åºå·', None)
                    
                    ops.append(
                        UpdateOne(
                            {'åŸºé‡‘ä»£ç ': fund_code, 'è¡Œä¸šç±»åˆ«': industry, 'æˆªæ­¢æ—¶é—´': end_date},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_portfolio_industry_allocation_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡è¡Œä¸šé…ç½®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜è¡Œä¸šé…ç½®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_portfolio_industry_allocation_em_data(self) -> int:
        """æ¸…ç©ºè¡Œä¸šé…ç½®"""
        try:
            result = await self.col_fund_portfolio_industry_allocation_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡è¡Œä¸šé…ç½®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºè¡Œä¸šé…ç½®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_portfolio_industry_allocation_em_stats(self) -> Dict[str, Any]:
        """è·å–è¡Œä¸šé…ç½®ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_portfolio_industry_allocation_em.count_documents({})
            
            unique_funds = await self.col_fund_portfolio_industry_allocation_em.distinct('code')
            unique_industries = await self.col_fund_portfolio_industry_allocation_em.distinct('industry')
            
            pipeline_industries = [
                {'$group': {'_id': '$è¡Œä¸šç±»åˆ«', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}},
                {'$limit': 20}
            ]
            
            industry_distribution = []
            async for doc in self.col_fund_portfolio_industry_allocation_em.aggregate(pipeline_industries):
                industry_distribution.append({
                    'industry': doc['_id'],
                    'count': doc['count']
                })
            
            pipeline_top_allocation = [
                {'$match': {'å å‡€å€¼æ¯”ä¾‹': {'$ne': None}}},
                {'$sort': {'å å‡€å€¼æ¯”ä¾‹': -1}},
                {'$limit': 10},
                {'$project': {'åŸºé‡‘ä»£ç ': 1, 'è¡Œä¸šç±»åˆ«': 1, 'æˆªæ­¢æ—¶é—´': 1, 'å å‡€å€¼æ¯”ä¾‹': 1}}
            ]
            
            top_allocations = []
            async for doc in self.col_fund_portfolio_industry_allocation_em.aggregate(pipeline_top_allocation):
                top_allocations.append({
                    'fund_code': doc.get('åŸºé‡‘ä»£ç '),
                    'industry': doc.get('è¡Œä¸šç±»åˆ«'),
                    'end_date': doc.get('æˆªæ­¢æ—¶é—´'),
                    'ratio': doc.get('å å‡€å€¼æ¯”ä¾‹')
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'unique_industries': len(unique_industries),
                'industry_distribution': industry_distribution,
                'top_allocations': top_allocations
            }
        except Exception as e:
            logger.error(f"è·å–è¡Œä¸šé…ç½®ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def save_fund_portfolio_change_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜é‡å¤§å˜åŠ¨åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰é‡å¤§å˜åŠ¨éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡é‡å¤§å˜åŠ¨...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    stock_code = str(doc.get('è‚¡ç¥¨ä»£ç ', ''))
                    indicator_type = str(doc.get('æŒ‡æ ‡ç±»å‹', ''))
                    quarter = str(doc.get('å­£åº¦', ''))
                    
                    # æ·»åŠ å…ƒæ•°æ®å­—æ®µï¼ˆä½¿ç”¨ä¸­æ–‡ï¼‰
                    doc['æ•°æ®æº'] = 'akshare'
                    doc['æ¥å£åç§°'] = 'fund_portfolio_change_em'
                    doc['æ›´æ–°æ—¶é—´'] = datetime.now().isoformat()
                    
                    # åˆ é™¤åºå·å­—æ®µï¼ˆä¸éœ€è¦ä¿å­˜ï¼‰
                    doc.pop('åºå·', None)
                    
                    ops.append(
                        UpdateOne(
                            {'åŸºé‡‘ä»£ç ': fund_code, 'è‚¡ç¥¨ä»£ç ': stock_code, 'æŒ‡æ ‡ç±»å‹': indicator_type, 'å­£åº¦': quarter},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_portfolio_change_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡é‡å¤§å˜åŠ¨")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜é‡å¤§å˜åŠ¨å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_portfolio_change_em_data(self) -> int:
        """æ¸…ç©ºé‡å¤§å˜åŠ¨"""
        try:
            result = await self.col_fund_portfolio_change_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡é‡å¤§å˜åŠ¨")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºé‡å¤§å˜åŠ¨å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_portfolio_change_em_stats(self) -> Dict[str, Any]:
        """è·å–é‡å¤§å˜åŠ¨ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_portfolio_change_em.count_documents({})
            
            unique_funds = await self.col_fund_portfolio_change_em.distinct('code')
            unique_stocks = await self.col_fund_portfolio_change_em.distinct('stock_code')
            
            pipeline_indicator_types = [
                {'$group': {'_id': '$æŒ‡æ ‡ç±»å‹', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            indicator_type_distribution = []
            async for doc in self.col_fund_portfolio_change_em.aggregate(pipeline_indicator_types):
                indicator_type_distribution.append({
                    'indicator_type': doc['_id'],
                    'count': doc['count']
                })
            
            pipeline_quarters = [
                {'$group': {'_id': '$å­£åº¦', 'count': {'$sum': 1}}},
                {'$sort': {'_id': -1}}
            ]
            
            quarter_distribution = []
            async for doc in self.col_fund_portfolio_change_em.aggregate(pipeline_quarters):
                quarter_distribution.append({
                    'quarter': doc['_id'],
                    'count': doc['count']
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'unique_stocks': len(unique_stocks),
                'indicator_type_distribution': indicator_type_distribution,
                'quarter_distribution': quarter_distribution
            }
        except Exception as e:
            logger.error(f"è·å–é‡å¤§å˜åŠ¨ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_rating_all_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘è¯„çº§æ€»æ±‡åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘è¯„çº§æ€»æ±‡æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘è¯„çº§æ€»æ±‡æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_code = str(doc.get('ä»£ç ', ''))
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_rating_all'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if fund_code:
                        ops.append(
                            UpdateOne(
                                {'code': fund_code},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_rating_all_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘è¯„çº§æ€»æ±‡æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘è¯„çº§æ€»æ±‡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_rating_all_em_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘è¯„çº§æ€»æ±‡æ•°æ®"""
        try:
            result = await self.col_fund_rating_all_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘è¯„çº§æ€»æ±‡æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘è¯„çº§æ€»æ±‡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_rating_all_em_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘è¯„çº§æ€»æ±‡ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_rating_all_em.count_documents({})
            
            unique_funds = await self.col_fund_rating_all_em.distinct('code')
            
            # è¯„çº§åˆ†å¸ƒï¼ˆæŒ‰æ‹›å•†è¯åˆ¸è¯„çº§ï¼‰
            pipeline_rating = [
                {'$group': {'_id': '$æ‹›å•†è¯åˆ¸', 'count': {'$sum': 1}}},
                {'$sort': {'_id': -1}}
            ]
            
            rating_distribution = []
            async for doc in self.col_fund_rating_all_em.aggregate(pipeline_rating):
                if doc['_id'] is not None:
                    rating_distribution.append({
                        'rating': doc['_id'],
                        'count': doc['count']
                    })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'rating_distribution': rating_distribution
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘è¯„çº§æ€»æ±‡ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_rating_sh_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜ä¸Šæµ·è¯åˆ¸è¯„çº§åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰ä¸Šæµ·è¯åˆ¸è¯„çº§æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡ä¸Šæµ·è¯åˆ¸è¯„çº§æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_code = str(doc.get('ä»£ç ', ''))
                    date_str = str(doc.get('æ—¥æœŸ', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_rating_sh'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç å’Œæ—¥æœŸä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if fund_code and date_str:
                        ops.append(
                            UpdateOne(
                                {'code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_rating_sh_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡ä¸Šæµ·è¯åˆ¸è¯„çº§æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜ä¸Šæµ·è¯åˆ¸è¯„çº§æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_rating_sh_em_data(self) -> int:
        """æ¸…ç©ºä¸Šæµ·è¯åˆ¸è¯„çº§æ•°æ®"""
        try:
            result = await self.col_fund_rating_sh_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡ä¸Šæµ·è¯åˆ¸è¯„çº§æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºä¸Šæµ·è¯åˆ¸è¯„çº§æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_rating_sh_em_stats(self) -> Dict[str, Any]:
        """è·å–ä¸Šæµ·è¯åˆ¸è¯„çº§ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_rating_sh_em.count_documents({})
            
            unique_funds = await self.col_fund_rating_sh_em.distinct('code')
            
            # 3å¹´æœŸè¯„çº§åˆ†å¸ƒ
            pipeline_rating = [
                {'$group': {'_id': '$3å¹´æœŸè¯„çº§-3å¹´è¯„çº§', 'count': {'$sum': 1}}},
                {'$sort': {'_id': -1}}
            ]
            
            rating_distribution = []
            async for doc in self.col_fund_rating_sh_em.aggregate(pipeline_rating):
                if doc['_id'] is not None:
                    rating_distribution.append({
                        'rating': doc['_id'],
                        'count': doc['count']
                    })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'rating_distribution': rating_distribution
            }
        except Exception as e:
            logger.error(f"è·å–ä¸Šæµ·è¯åˆ¸è¯„çº§ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_rating_zs_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜æ‹›å•†è¯åˆ¸è¯„çº§åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰æ‹›å•†è¯åˆ¸è¯„çº§æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡æ‹›å•†è¯åˆ¸è¯„çº§æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_code = str(doc.get('ä»£ç ', ''))
                    date_str = str(doc.get('æ—¥æœŸ', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_rating_zs'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç å’Œæ—¥æœŸä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if fund_code and date_str:
                        ops.append(
                            UpdateOne(
                                {'code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_rating_zs_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡æ‹›å•†è¯åˆ¸è¯„çº§æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜æ‹›å•†è¯åˆ¸è¯„çº§æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_rating_zs_em_data(self) -> int:
        """æ¸…ç©ºæ‹›å•†è¯åˆ¸è¯„çº§æ•°æ®"""
        try:
            result = await self.col_fund_rating_zs_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡æ‹›å•†è¯åˆ¸è¯„çº§æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºæ‹›å•†è¯åˆ¸è¯„çº§æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_rating_zs_em_stats(self) -> Dict[str, Any]:
        """è·å–æ‹›å•†è¯åˆ¸è¯„çº§ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_rating_zs_em.count_documents({})
            
            unique_funds = await self.col_fund_rating_zs_em.distinct('code')
            
            # 3å¹´æœŸè¯„çº§åˆ†å¸ƒ
            pipeline_rating = [
                {'$group': {'_id': '$3å¹´æœŸè¯„çº§-3å¹´è¯„çº§', 'count': {'$sum': 1}}},
                {'$sort': {'_id': -1}}
            ]
            
            rating_distribution = []
            async for doc in self.col_fund_rating_zs_em.aggregate(pipeline_rating):
                if doc['_id'] is not None:
                    rating_distribution.append({
                        'rating': doc['_id'],
                        'count': doc['count']
                    })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'rating_distribution': rating_distribution
            }
        except Exception as e:
            logger.error(f"è·å–æ‹›å•†è¯åˆ¸è¯„çº§ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_rating_ja_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜æµå®‰é‡‘ä¿¡è¯„çº§åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰æµå®‰é‡‘ä¿¡è¯„çº§æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡æµå®‰é‡‘ä¿¡è¯„çº§æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_code = str(doc.get('ä»£ç ', ''))
                    date_str = str(doc.get('æ—¥æœŸ', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_rating_ja'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç å’Œæ—¥æœŸä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if fund_code and date_str:
                        ops.append(
                            UpdateOne(
                                {'code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_rating_ja_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡æµå®‰é‡‘ä¿¡è¯„çº§æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜æµå®‰é‡‘ä¿¡è¯„çº§æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_rating_ja_em_data(self) -> int:
        """æ¸…ç©ºæµå®‰é‡‘ä¿¡è¯„çº§æ•°æ®"""
        try:
            result = await self.col_fund_rating_ja_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡æµå®‰é‡‘ä¿¡è¯„çº§æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºæµå®‰é‡‘ä¿¡è¯„çº§æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_rating_ja_em_stats(self) -> Dict[str, Any]:
        """è·å–æµå®‰é‡‘ä¿¡è¯„çº§ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_rating_ja_em.count_documents({})
            
            unique_funds = await self.col_fund_rating_ja_em.distinct('code')
            
            # 3å¹´æœŸè¯„çº§åˆ†å¸ƒ
            pipeline_rating = [
                {'$group': {'_id': '$3å¹´æœŸè¯„çº§-3å¹´è¯„çº§', 'count': {'$sum': 1}}},
                {'$sort': {'_id': -1}}
            ]
            
            rating_distribution = []
            async for doc in self.col_fund_rating_ja_em.aggregate(pipeline_rating):
                if doc['_id'] is not None:
                    rating_distribution.append({
                        'rating': doc['_id'],
                        'count': doc['count']
                    })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'rating_distribution': rating_distribution
            }
        except Exception as e:
            logger.error(f"è·å–æµå®‰é‡‘ä¿¡è¯„çº§ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_manager_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘ç»ç†åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘ç»ç†æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘ç»ç†æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    name = str(doc.get('å§“å', ''))
                    fund_codes = str(doc.get('ç°ä»»åŸºé‡‘ä»£ç ', ''))
                    doc['name'] = name
                    doc['fund_codes'] = fund_codes
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_manager_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨å§“åå’Œç°ä»»åŸºé‡‘ä»£ç ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼ˆå¦‚æœåŸºé‡‘ä»£ç ä¸ºç©ºï¼Œå¯èƒ½éœ€è¦å…¶ä»–æ ‡è¯†ï¼Œå¦‚åºå·ï¼‰
                    unique_key = {'name': name, 'fund_codes': fund_codes}
                    
                    # å¦‚æœæœ‰åºå·ï¼Œä¼˜å…ˆä½¿ç”¨åºå·ä½œä¸ºè¾…åŠ©ï¼ˆå‡è®¾åºå·æ˜¯å”¯ä¸€IDï¼‰
                    # ä½†APIè¿”å›çš„åºå·ä¸ä¸€å®šæ˜¯å›ºå®šçš„IDï¼Œå¯èƒ½æ˜¯æœ¬æ¬¡æŸ¥è¯¢çš„åºå·ã€‚
                    # æ‰€ä»¥è¿˜æ˜¯ç”¨ name + fund_codes æ¯”è¾ƒç¨³å¦¥ã€‚
                    
                    ops.append(
                        UpdateOne(
                            unique_key,
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_manager_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘ç»ç†æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘ç»ç†æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_manager_em_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘ç»ç†æ•°æ®"""
        try:
            result = await self.col_fund_manager_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘ç»ç†æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘ç»ç†æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_manager_em_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘ç»ç†ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_manager_em.count_documents({})
            
            unique_companies = await self.col_fund_manager_em.distinct('æ‰€å±å…¬å¸')
            
            # åŸºé‡‘ç»ç†äººæ•°æœ€å¤šçš„å…¬å¸TOP10
            pipeline_company = [
                {'$group': {'_id': '$æ‰€å±å…¬å¸', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}},
                {'$limit': 10}
            ]
            
            company_distribution = []
            async for doc in self.col_fund_manager_em.aggregate(pipeline_company):
                if doc['_id']:
                    company_distribution.append({
                        'company': doc['_id'],
                        'count': doc['count']
                    })
            
            return {
                'total_count': total_count,
                'unique_companies': len(unique_companies),
                'company_distribution': company_distribution
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘ç»ç†ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_new_found_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜æ–°å‘åŸºé‡‘åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰æ–°å‘åŸºé‡‘æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡æ–°å‘åŸºé‡‘æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_new_found_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if fund_code:
                        ops.append(
                            UpdateOne(
                                {'code': fund_code},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_new_found_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡æ–°å‘åŸºé‡‘æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜æ–°å‘åŸºé‡‘æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_new_found_em_data(self) -> int:
        """æ¸…ç©ºæ–°å‘åŸºé‡‘æ•°æ®"""
        try:
            result = await self.col_fund_new_found_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡æ–°å‘åŸºé‡‘æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºæ–°å‘åŸºé‡‘æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_new_found_em_stats(self) -> Dict[str, Any]:
        """è·å–æ–°å‘åŸºé‡‘ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_new_found_em.count_documents({})
            
            unique_funds = await self.col_fund_new_found_em.distinct('code')
            
            # åŸºé‡‘ç±»å‹åˆ†å¸ƒï¼ˆå‡è®¾æœ‰'åŸºé‡‘ç±»å‹'å­—æ®µï¼‰
            pipeline_type = [
                {'$group': {'_id': '$åŸºé‡‘ç±»å‹', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            type_distribution = []
            async for doc in self.col_fund_new_found_em.aggregate(pipeline_type):
                if doc['_id']:
                    type_distribution.append({
                        'type': doc['_id'],
                        'count': doc['count']
                    })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'type_distribution': type_distribution
            }
        except Exception as e:
            logger.error(f"è·å–æ–°å‘åŸºé‡‘ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_scale_open_sina_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜å¼€æ”¾å¼åŸºé‡‘è§„æ¨¡åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰å¼€æ”¾å¼åŸºé‡‘è§„æ¨¡æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡å¼€æ”¾å¼åŸºé‡‘è§„æ¨¡æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    # å‡è®¾æœ‰æ—¥æœŸå­—æ®µï¼Œå¦‚æœæ²¡æœ‰ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
                    date_str = str(doc.get('æ›´æ–°æ—¥æœŸ', '')) or str(doc.get('æˆªæ­¢æ—¥æœŸ', '')) or datetime.now().strftime('%Y-%m-%d')
                    
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_scale_open_sina'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç å’Œæ—¥æœŸä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if fund_code:
                        ops.append(
                            UpdateOne(
                                {'code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_scale_open_sina.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡å¼€æ”¾å¼åŸºé‡‘è§„æ¨¡æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜å¼€æ”¾å¼åŸºé‡‘è§„æ¨¡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_scale_open_sina_data(self) -> int:
        """æ¸…ç©ºå¼€æ”¾å¼åŸºé‡‘è§„æ¨¡æ•°æ®"""
        try:
            result = await self.col_fund_scale_open_sina.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡å¼€æ”¾å¼åŸºé‡‘è§„æ¨¡æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºå¼€æ”¾å¼åŸºé‡‘è§„æ¨¡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_scale_open_sina_stats(self) -> Dict[str, Any]:
        """è·å–å¼€æ”¾å¼åŸºé‡‘è§„æ¨¡ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_scale_open_sina.count_documents({})
            
            unique_funds = await self.col_fund_scale_open_sina.distinct('code')
            
            # è§„æ¨¡TOP10ï¼ˆéœ€è¦æ ¹æ®æœ€æ–°æ—¥æœŸè¿‡æ»¤ï¼Œè¿™é‡Œç®€å•æŒ‰è§„æ¨¡æ’åºï¼Œå‡è®¾è§„æ¨¡å­—æ®µä¸º'æ€»èµ„äº§'æˆ–ç±»ä¼¼ï¼‰
            # å‡è®¾å­—æ®µåä¸º 'èµ„äº§å‡€å€¼' æˆ– 'åŸºé‡‘è§„æ¨¡'
            # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ç»Ÿè®¡ï¼Œä¸éœ€è¦å¤ªå¤æ‚
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds)
            }
        except Exception as e:
            logger.error(f"è·å–å¼€æ”¾å¼åŸºé‡‘è§„æ¨¡ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_scale_close_sina_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜å°é—­å¼åŸºé‡‘è§„æ¨¡åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰å°é—­å¼åŸºé‡‘è§„æ¨¡æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡å°é—­å¼åŸºé‡‘è§„æ¨¡æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    date_str = str(doc.get('æ›´æ–°æ—¥æœŸ', '')) or str(doc.get('æˆªæ­¢æ—¥æœŸ', '')) or datetime.now().strftime('%Y-%m-%d')
                    
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_scale_close_sina'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç å’Œæ—¥æœŸä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if fund_code:
                        ops.append(
                            UpdateOne(
                                {'code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_scale_close_sina.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡å°é—­å¼åŸºé‡‘è§„æ¨¡æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜å°é—­å¼åŸºé‡‘è§„æ¨¡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_scale_close_sina_data(self) -> int:
        """æ¸…ç©ºå°é—­å¼åŸºé‡‘è§„æ¨¡æ•°æ®"""
        try:
            result = await self.col_fund_scale_close_sina.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡å°é—­å¼åŸºé‡‘è§„æ¨¡æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºå°é—­å¼åŸºé‡‘è§„æ¨¡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_scale_close_sina_stats(self) -> Dict[str, Any]:
        """è·å–å°é—­å¼åŸºé‡‘è§„æ¨¡ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_scale_close_sina.count_documents({})
            
            unique_funds = await self.col_fund_scale_close_sina.distinct('code')
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds)
            }
        except Exception as e:
            logger.error(f"è·å–å°é—­å¼åŸºé‡‘è§„æ¨¡ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_scale_structured_sina_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åˆ†çº§å­åŸºé‡‘è§„æ¨¡åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åˆ†çº§å­åŸºé‡‘è§„æ¨¡æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åˆ†çº§å­åŸºé‡‘è§„æ¨¡æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', ''))
                    date_str = str(doc.get('æ›´æ–°æ—¥æœŸ', '')) or str(doc.get('æˆªæ­¢æ—¥æœŸ', '')) or datetime.now().strftime('%Y-%m-%d')
                    
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_scale_structured_sina'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘ä»£ç å’Œæ—¥æœŸä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if fund_code:
                        ops.append(
                            UpdateOne(
                                {'code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_scale_structured_sina.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åˆ†çº§å­åŸºé‡‘è§„æ¨¡æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†çº§å­åŸºé‡‘è§„æ¨¡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_scale_structured_sina_data(self) -> int:
        """æ¸…ç©ºåˆ†çº§å­åŸºé‡‘è§„æ¨¡æ•°æ®"""
        try:
            result = await self.col_fund_scale_structured_sina.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åˆ†çº§å­åŸºé‡‘è§„æ¨¡æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåˆ†çº§å­åŸºé‡‘è§„æ¨¡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_scale_structured_sina_stats(self) -> Dict[str, Any]:
        """è·å–åˆ†çº§å­åŸºé‡‘è§„æ¨¡ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_scale_structured_sina.count_documents({})
            
            unique_funds = await self.col_fund_scale_structured_sina.distinct('code')
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds)
            }
        except Exception as e:
            logger.error(f"è·å–åˆ†çº§å­åŸºé‡‘è§„æ¨¡ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_aum_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘è§„æ¨¡è¯¦æƒ…åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘è§„æ¨¡è¯¦æƒ…æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘è§„æ¨¡è¯¦æƒ…æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_company = str(doc.get('åŸºé‡‘å…¬å¸', ''))
                    # å‡è®¾æœ‰æ›´æ–°æ—¥æœŸå­—æ®µ
                    date_str = str(doc.get('æ›´æ–°æ—¥æœŸ', '')) or str(doc.get('æˆªæ­¢æ—¥æœŸ', '')) or datetime.now().strftime('%Y-%m-%d')
                    
                    doc['company'] = fund_company
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_aum_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘å…¬å¸å’Œæ—¥æœŸä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if fund_company:
                        ops.append(
                            UpdateOne(
                                {'company': fund_company, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_aum_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘è§„æ¨¡è¯¦æƒ…æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘è§„æ¨¡è¯¦æƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_aum_em_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘è§„æ¨¡è¯¦æƒ…æ•°æ®"""
        try:
            result = await self.col_fund_aum_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘è§„æ¨¡è¯¦æƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘è§„æ¨¡è¯¦æƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_aum_em_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘è§„æ¨¡è¯¦æƒ…ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_aum_em.count_documents({})
            
            unique_companies = await self.col_fund_aum_em.distinct('company')
            
            return {
                'total_count': total_count,
                'unique_companies': len(unique_companies)
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘è§„æ¨¡è¯¦æƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_aum_trend_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘è§„æ¨¡èµ°åŠ¿åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘è§„æ¨¡èµ°åŠ¿æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘è§„æ¨¡èµ°åŠ¿æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    date_str = str(doc.get('date', '')) or str(doc.get('æˆªæ­¢æ—¥æœŸ', '')) or datetime.now().strftime('%Y-%m-%d')
                    
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_aum_trend_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨æ—¥æœŸä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if date_str:
                        ops.append(
                            UpdateOne(
                                {'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_aum_trend_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘è§„æ¨¡èµ°åŠ¿æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘è§„æ¨¡èµ°åŠ¿æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_aum_trend_em_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘è§„æ¨¡èµ°åŠ¿æ•°æ®"""
        try:
            result = await self.col_fund_aum_trend_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘è§„æ¨¡èµ°åŠ¿æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘è§„æ¨¡èµ°åŠ¿æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_aum_trend_em_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘è§„æ¨¡èµ°åŠ¿ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_aum_trend_em.count_documents({})
            
            # å‡è®¾æˆ‘ä»¬åªå…³å¿ƒæ€»è®°å½•æ•°
            
            return {
                'total_count': total_count
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘è§„æ¨¡èµ°åŠ¿ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_aum_hist_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘å…¬å¸å†å¹´ç®¡ç†è§„æ¨¡åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘å…¬å¸å†å¹´ç®¡ç†è§„æ¨¡æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘å…¬å¸å†å¹´ç®¡ç†è§„æ¨¡æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    fund_company = str(doc.get('åŸºé‡‘å…¬å¸', ''))
                    date_str = str(doc.get('æ›´æ–°æ—¥æœŸ', '')) or str(doc.get('æˆªæ­¢æ—¥æœŸ', '')) or datetime.now().strftime('%Y-%m-%d')
                    
                    doc['company'] = fund_company
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_aum_hist_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨åŸºé‡‘å…¬å¸å’Œæ—¥æœŸä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if fund_company and date_str:
                        ops.append(
                            UpdateOne(
                                {'company': fund_company, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_aum_hist_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘å…¬å¸å†å¹´ç®¡ç†è§„æ¨¡æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘å…¬å¸å†å¹´ç®¡ç†è§„æ¨¡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_aum_hist_em_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘å…¬å¸å†å¹´ç®¡ç†è§„æ¨¡æ•°æ®"""
        try:
            result = await self.col_fund_aum_hist_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘å…¬å¸å†å¹´ç®¡ç†è§„æ¨¡æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘å…¬å¸å†å¹´ç®¡ç†è§„æ¨¡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_aum_hist_em_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘å…¬å¸å†å¹´ç®¡ç†è§„æ¨¡ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_aum_hist_em.count_documents({})
            
            unique_companies = await self.col_fund_aum_hist_em.distinct('company')
            
            return {
                'total_count': total_count,
                'unique_companies': len(unique_companies)
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘å…¬å¸å†å¹´ç®¡ç†è§„æ¨¡ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_reits_realtime_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜REITså®æ—¶è¡Œæƒ…åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰REITså®æ—¶è¡Œæƒ…æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡REITså®æ—¶è¡Œæƒ…æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    code = str(doc.get('ä»£ç ', ''))
                    date_str = datetime.now().strftime('%Y-%m-%d')
                    
                    doc['code'] = code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'reits_realtime_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨ä»£ç å’Œæ—¥æœŸä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if code:
                        ops.append(
                            UpdateOne(
                                {'code': code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_reits_realtime_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡REITså®æ—¶è¡Œæƒ…æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜REITså®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_reits_realtime_em_data(self) -> int:
        """æ¸…ç©ºREITså®æ—¶è¡Œæƒ…æ•°æ®"""
        try:
            result = await self.col_reits_realtime_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡REITså®æ—¶è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºREITså®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_reits_realtime_em_stats(self) -> Dict[str, Any]:
        """è·å–REITså®æ—¶è¡Œæƒ…ç»Ÿè®¡"""
        try:
            total_count = await self.col_reits_realtime_em.count_documents({})
            
            unique_codes = await self.col_reits_realtime_em.distinct('code')
            
            return {
                'total_count': total_count,
                'unique_codes': len(unique_codes)
            }
        except Exception as e:
            logger.error(f"è·å–REITså®æ—¶è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_reits_hist_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜REITså†å²è¡Œæƒ…åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰REITså†å²è¡Œæƒ…æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡REITså†å²è¡Œæƒ…æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    code = str(doc.get('code', '')) or str(doc.get('ä»£ç ', ''))
                    date_str = str(doc.get('date', '')) or str(doc.get('æ—¥æœŸ', ''))
                    
                    doc['code'] = code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'reits_hist_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨ä»£ç å’Œæ—¥æœŸä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if code and date_str:
                        ops.append(
                            UpdateOne(
                                {'code': code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_reits_hist_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡REITså†å²è¡Œæƒ…æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜REITså†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_reits_hist_em_data(self) -> int:
        """æ¸…ç©ºREITså†å²è¡Œæƒ…æ•°æ®"""
        try:
            result = await self.col_reits_hist_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡REITså†å²è¡Œæƒ…æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºREITså†å²è¡Œæƒ…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_reits_hist_em_stats(self) -> Dict[str, Any]:
        """è·å–REITså†å²è¡Œæƒ…ç»Ÿè®¡"""
        try:
            total_count = await self.col_reits_hist_em.count_documents({})
            
            unique_codes = await self.col_reits_hist_em.distinct('code')
            
            return {
                'total_count': total_count,
                'unique_codes': len(unique_codes)
            }
        except Exception as e:
            logger.error(f"è·å–REITså†å²è¡Œæƒ…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_report_stock_cninfo_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘é‡ä»“è‚¡-å·¨æ½®æ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘é‡ä»“è‚¡æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘é‡ä»“è‚¡æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    # å‡è®¾å­—æ®µåŒ…å«ï¼šåŸºé‡‘ä»£ç ã€è‚¡ç¥¨ä»£ç ã€æˆªæ­¢æ—¥æœŸ/æŠ¥å‘ŠæœŸ
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', '')) or str(doc.get('fund_code', ''))
                    stock_code = str(doc.get('è‚¡ç¥¨ä»£ç ', '')) or str(doc.get('stock_code', ''))
                    date_str = str(doc.get('æŠ¥å‘ŠæœŸ', '')) or str(doc.get('date', '')) or str(doc.get('æˆªæ­¢æ—¥æœŸ', ''))
                    
                    doc['fund_code'] = fund_code
                    doc['stock_code'] = stock_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_report_stock_cninfo'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨ åŸºé‡‘ä»£ç  + è‚¡ç¥¨ä»£ç  + æŠ¥å‘ŠæœŸ ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if fund_code and stock_code and date_str:
                        ops.append(
                            UpdateOne(
                                {'fund_code': fund_code, 'stock_code': stock_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_report_stock_cninfo.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘é‡ä»“è‚¡æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘é‡ä»“è‚¡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_report_stock_cninfo_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘é‡ä»“è‚¡æ•°æ®"""
        try:
            result = await self.col_fund_report_stock_cninfo.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘é‡ä»“è‚¡æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘é‡ä»“è‚¡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_report_stock_cninfo_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘é‡ä»“è‚¡ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_report_stock_cninfo.count_documents({})
            
            return {
                'total_count': total_count
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘é‡ä»“è‚¡ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_report_industry_allocation_cninfo_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘è¡Œä¸šé…ç½®-å·¨æ½®æ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘è¡Œä¸šé…ç½®æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘è¡Œä¸šé…ç½®æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    # å‡è®¾å­—æ®µåŒ…å«ï¼šåŸºé‡‘ä»£ç ã€è¡Œä¸šåç§°ã€è¡Œä¸šç¼–ç ã€æˆªæ­¢æ—¥æœŸ/æŠ¥å‘ŠæœŸ
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', '')) or str(doc.get('fund_code', ''))
                    industry_name = str(doc.get('è¡Œä¸šåç§°', '')) or str(doc.get('industry_name', ''))
                    date_str = str(doc.get('æŠ¥å‘ŠæœŸ', '')) or str(doc.get('date', '')) or str(doc.get('æˆªæ­¢æ—¥æœŸ', ''))
                    
                    doc['fund_code'] = fund_code
                    doc['industry_name'] = industry_name
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_report_industry_allocation_cninfo'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨ åŸºé‡‘ä»£ç  + è¡Œä¸šåç§° + æŠ¥å‘ŠæœŸ ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if fund_code and industry_name and date_str:
                        ops.append(
                            UpdateOne(
                                {'fund_code': fund_code, 'industry_name': industry_name, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_report_industry_allocation_cninfo.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘è¡Œä¸šé…ç½®æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘è¡Œä¸šé…ç½®æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_report_industry_allocation_cninfo_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘è¡Œä¸šé…ç½®æ•°æ®"""
        try:
            result = await self.col_fund_report_industry_allocation_cninfo.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘è¡Œä¸šé…ç½®æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘è¡Œä¸šé…ç½®æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_report_industry_allocation_cninfo_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘è¡Œä¸šé…ç½®ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_report_industry_allocation_cninfo.count_documents({})
            
            return {
                'total_count': total_count
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘è¡Œä¸šé…ç½®ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_report_asset_allocation_cninfo_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘èµ„äº§é…ç½®-å·¨æ½®æ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘èµ„äº§é…ç½®æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘èµ„äº§é…ç½®æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    # å‡è®¾å­—æ®µåŒ…å«ï¼šåŸºé‡‘ä»£ç ã€æˆªæ­¢æ—¥æœŸ/æŠ¥å‘ŠæœŸ
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', '')) or str(doc.get('fund_code', ''))
                    date_str = str(doc.get('æŠ¥å‘ŠæœŸ', '')) or str(doc.get('date', '')) or str(doc.get('æˆªæ­¢æ—¥æœŸ', ''))
                    
                    doc['fund_code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_report_asset_allocation_cninfo'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨ åŸºé‡‘ä»£ç  + æŠ¥å‘ŠæœŸ ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if fund_code and date_str:
                        ops.append(
                            UpdateOne(
                                {'fund_code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_report_asset_allocation_cninfo.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘èµ„äº§é…ç½®æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘èµ„äº§é…ç½®æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_report_asset_allocation_cninfo_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘èµ„äº§é…ç½®æ•°æ®"""
        try:
            result = await self.col_fund_report_asset_allocation_cninfo.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘èµ„äº§é…ç½®æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘èµ„äº§é…ç½®æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_report_asset_allocation_cninfo_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘èµ„äº§é…ç½®ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_report_asset_allocation_cninfo.count_documents({})
            
            return {
                'total_count': total_count
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘èµ„äº§é…ç½®ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_scale_change_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜è§„æ¨¡å˜åŠ¨-ä¸œè´¢æ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰è§„æ¨¡å˜åŠ¨æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡è§„æ¨¡å˜åŠ¨æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    # å‡è®¾å­—æ®µåŒ…å«ï¼šæˆªæ­¢æ—¥æœŸã€å‡€èµ„äº§ã€æœŸé—´ç”³è´­ã€æœŸé—´èµå›ç­‰ï¼Œéœ€è¦code
                    # ç”±äºæ¥å£æ˜¯æŒ‰codeæŸ¥çš„ï¼Œdfé‡Œå¯èƒ½æ²¡æœ‰codeï¼Œéœ€è¦ç¡®ä¿ä¼ å…¥å‰åŠ ä¸Š
                    fund_code = str(doc.get('code', '')) or str(doc.get('fund_code', ''))
                    date_str = str(doc.get('æˆªæ­¢æ—¥æœŸ', '')) or str(doc.get('date', ''))
                    
                    doc['fund_code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_scale_change_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨ åŸºé‡‘ä»£ç  + æˆªæ­¢æ—¥æœŸ ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if fund_code and date_str:
                        ops.append(
                            UpdateOne(
                                {'fund_code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_scale_change_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡è§„æ¨¡å˜åŠ¨æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜è§„æ¨¡å˜åŠ¨æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_scale_change_em_data(self) -> int:
        """æ¸…ç©ºè§„æ¨¡å˜åŠ¨æ•°æ®"""
        try:
            result = await self.col_fund_scale_change_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡è§„æ¨¡å˜åŠ¨æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºè§„æ¨¡å˜åŠ¨æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_scale_change_em_stats(self) -> Dict[str, Any]:
        """è·å–è§„æ¨¡å˜åŠ¨ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_scale_change_em.count_documents({})
            unique_funds = await self.col_fund_scale_change_em.distinct('fund_code')
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds)
            }
        except Exception as e:
            logger.error(f"è·å–è§„æ¨¡å˜åŠ¨ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_hold_structure_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜æŒæœ‰äººç»“æ„-ä¸œè´¢æ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰æŒæœ‰äººç»“æ„æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡æŒæœ‰äººç»“æ„æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    # å‡è®¾å­—æ®µåŒ…å«ï¼šæˆªæ­¢æ—¥æœŸã€æœºæ„æŒæœ‰æ¯”ä¾‹ã€ä¸ªäººæŒæœ‰æ¯”ä¾‹ã€å†…éƒ¨æŒæœ‰æ¯”ä¾‹ã€æ€»ä»½é¢ç­‰
                    # éœ€è¦code
                    fund_code = str(doc.get('code', '')) or str(doc.get('fund_code', ''))
                    date_str = str(doc.get('æˆªæ­¢æ—¥æœŸ', '')) or str(doc.get('date', ''))
                    
                    doc['fund_code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_hold_structure_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨ åŸºé‡‘ä»£ç  + æˆªæ­¢æ—¥æœŸ ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if fund_code and date_str:
                        ops.append(
                            UpdateOne(
                                {'fund_code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_hold_structure_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡æŒæœ‰äººç»“æ„æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜æŒæœ‰äººç»“æ„æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_hold_structure_em_data(self) -> int:
        """æ¸…ç©ºæŒæœ‰äººç»“æ„æ•°æ®"""
        try:
            result = await self.col_fund_hold_structure_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡æŒæœ‰äººç»“æ„æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºæŒæœ‰äººç»“æ„æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_hold_structure_em_stats(self) -> Dict[str, Any]:
        """è·å–æŒæœ‰äººç»“æ„ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_hold_structure_em.count_documents({})
            unique_funds = await self.col_fund_hold_structure_em.distinct('fund_code')
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds)
            }
        except Exception as e:
            logger.error(f"è·å–æŒæœ‰äººç»“æ„ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_stock_position_lg_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜è‚¡ç¥¨å‹åŸºé‡‘ä»“ä½-ä¹å’•ä¹è‚¡æ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰è‚¡ç¥¨å‹åŸºé‡‘ä»“ä½æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡è‚¡ç¥¨å‹åŸºé‡‘ä»“ä½æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    # å‡è®¾å­—æ®µåŒ…å«ï¼šdate, ä»“ä½
                    date_str = str(doc.get('date', '')) or str(doc.get('æ—¥æœŸ', ''))
                    
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_stock_position_lg'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨ date ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if date_str:
                        ops.append(
                            UpdateOne(
                                {'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_stock_position_lg.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡è‚¡ç¥¨å‹åŸºé‡‘ä»“ä½æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜è‚¡ç¥¨å‹åŸºé‡‘ä»“ä½æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_stock_position_lg_data(self) -> int:
        """æ¸…ç©ºè‚¡ç¥¨å‹åŸºé‡‘ä»“ä½æ•°æ®"""
        try:
            result = await self.col_fund_stock_position_lg.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡è‚¡ç¥¨å‹åŸºé‡‘ä»“ä½æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºè‚¡ç¥¨å‹åŸºé‡‘ä»“ä½æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_stock_position_lg_stats(self) -> Dict[str, Any]:
        """è·å–è‚¡ç¥¨å‹åŸºé‡‘ä»“ä½ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_stock_position_lg.count_documents({})
            
            return {
                'total_count': total_count
            }
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨å‹åŸºé‡‘ä»“ä½ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_balance_position_lg_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜å¹³è¡¡æ··åˆå‹åŸºé‡‘ä»“ä½-ä¹å’•ä¹è‚¡æ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰å¹³è¡¡æ··åˆå‹åŸºé‡‘ä»“ä½æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡å¹³è¡¡æ··åˆå‹åŸºé‡‘ä»“ä½æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    # å‡è®¾å­—æ®µåŒ…å«ï¼šdate, ä»“ä½
                    date_str = str(doc.get('date', '')) or str(doc.get('æ—¥æœŸ', ''))
                    
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_balance_position_lg'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨ date ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if date_str:
                        ops.append(
                            UpdateOne(
                                {'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_balance_position_lg.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡å¹³è¡¡æ··åˆå‹åŸºé‡‘ä»“ä½æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜å¹³è¡¡æ··åˆå‹åŸºé‡‘ä»“ä½æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_balance_position_lg_data(self) -> int:
        """æ¸…ç©ºå¹³è¡¡æ··åˆå‹åŸºé‡‘ä»“ä½æ•°æ®"""
        try:
            result = await self.col_fund_balance_position_lg.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡å¹³è¡¡æ··åˆå‹åŸºé‡‘ä»“ä½æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºå¹³è¡¡æ··åˆå‹åŸºé‡‘ä»“ä½æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_balance_position_lg_stats(self) -> Dict[str, Any]:
        """è·å–å¹³è¡¡æ··åˆå‹åŸºé‡‘ä»“ä½ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_balance_position_lg.count_documents({})
            
            return {
                'total_count': total_count
            }
        except Exception as e:
            logger.error(f"è·å–å¹³è¡¡æ··åˆå‹åŸºé‡‘ä»“ä½ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_linghuo_position_lg_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜çµæ´»é…ç½®å‹åŸºé‡‘ä»“ä½-ä¹å’•ä¹è‚¡æ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰çµæ´»é…ç½®å‹åŸºé‡‘ä»“ä½æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡çµæ´»é…ç½®å‹åŸºé‡‘ä»“ä½æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    # å‡è®¾å­—æ®µåŒ…å«ï¼šdate, ä»“ä½
                    date_str = str(doc.get('date', '')) or str(doc.get('æ—¥æœŸ', ''))
                    
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_linghuo_position_lg'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ä½¿ç”¨ date ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    if date_str:
                        ops.append(
                            UpdateOne(
                                {'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_linghuo_position_lg.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡çµæ´»é…ç½®å‹åŸºé‡‘ä»“ä½æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜çµæ´»é…ç½®å‹åŸºé‡‘ä»“ä½æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_linghuo_position_lg_data(self) -> int:
        """æ¸…ç©ºçµæ´»é…ç½®å‹åŸºé‡‘ä»“ä½æ•°æ®"""
        try:
            result = await self.col_fund_linghuo_position_lg.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡çµæ´»é…ç½®å‹åŸºé‡‘ä»“ä½æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºçµæ´»é…ç½®å‹åŸºé‡‘ä»“ä½æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_linghuo_position_lg_stats(self) -> Dict[str, Any]:
        """è·å–çµæ´»é…ç½®å‹åŸºé‡‘ä»“ä½ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_linghuo_position_lg.count_documents({})
            
            return {
                'total_count': total_count
            }
        except Exception as e:
            logger.error(f"è·å–çµæ´»é…ç½®å‹åŸºé‡‘ä»“ä½ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_announcement_dividend_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘å…¬å‘Šåˆ†çº¢é…é€-ä¸œè´¢æ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘å…¬å‘Šåˆ†çº¢é…é€æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘å…¬å‘Šåˆ†çº¢é…é€æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    # å‡è®¾å­—æ®µåŒ…å«ï¼šå…¬å‘Šæ—¥æœŸã€å…¬å‘Šæ ‡é¢˜ã€å…¬å‘Šå†…å®¹ç­‰
                    # å”¯ä¸€æ ‡è¯†å¯èƒ½æ˜¯ï¼šåŸºé‡‘ä»£ç  + å…¬å‘Šæ ‡é¢˜ + å…¬å‘Šæ—¥æœŸ
                    
                    fund_code = str(doc.get('code', '')) or str(doc.get('symbol', ''))
                    title = str(doc.get('å…¬å‘Šæ ‡é¢˜', '')) or str(doc.get('title', ''))
                    date_str = str(doc.get('å…¬å‘Šæ—¥æœŸ', '')) or str(doc.get('date', ''))
                    
                    doc['fund_code'] = fund_code
                    doc['title'] = title
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_announcement_dividend_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # å”¯ä¸€æ ‡è¯†ï¼šfund_code + title + date
                    if fund_code and title and date_str:
                        ops.append(
                            UpdateOne(
                                {'fund_code': fund_code, 'title': title, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_announcement_dividend_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘å…¬å‘Šåˆ†çº¢é…é€æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘å…¬å‘Šåˆ†çº¢é…é€æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_announcement_dividend_em_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘å…¬å‘Šåˆ†çº¢é…é€æ•°æ®"""
        try:
            result = await self.col_fund_announcement_dividend_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘å…¬å‘Šåˆ†çº¢é…é€æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘å…¬å‘Šåˆ†çº¢é…é€æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_announcement_dividend_em_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘å…¬å‘Šåˆ†çº¢é…é€ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_announcement_dividend_em.count_documents({})
            unique_funds = await self.col_fund_announcement_dividend_em.distinct('fund_code')
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds)
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘å…¬å‘Šåˆ†çº¢é…é€ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_announcement_report_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘å…¬å‘Šå®šæœŸæŠ¥å‘Š-ä¸œè´¢æ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘å…¬å‘Šå®šæœŸæŠ¥å‘Šæ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘å…¬å‘Šå®šæœŸæŠ¥å‘Šæ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    # å‡è®¾å­—æ®µåŒ…å«ï¼šå…¬å‘Šæ—¥æœŸã€å…¬å‘Šæ ‡é¢˜ã€å…¬å‘Šå†…å®¹ç­‰
                    
                    fund_code = str(doc.get('code', '')) or str(doc.get('symbol', ''))
                    title = str(doc.get('å…¬å‘Šæ ‡é¢˜', '')) or str(doc.get('title', ''))
                    date_str = str(doc.get('å…¬å‘Šæ—¥æœŸ', '')) or str(doc.get('date', ''))
                    
                    doc['fund_code'] = fund_code
                    doc['title'] = title
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_announcement_report_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # å”¯ä¸€æ ‡è¯†ï¼šfund_code + title + date
                    if fund_code and title and date_str:
                        ops.append(
                            UpdateOne(
                                {'fund_code': fund_code, 'title': title, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_announcement_report_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘å…¬å‘Šå®šæœŸæŠ¥å‘Šæ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘å…¬å‘Šå®šæœŸæŠ¥å‘Šæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_announcement_report_em_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘å…¬å‘Šå®šæœŸæŠ¥å‘Šæ•°æ®"""
        try:
            result = await self.col_fund_announcement_report_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘å…¬å‘Šå®šæœŸæŠ¥å‘Šæ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘å…¬å‘Šå®šæœŸæŠ¥å‘Šæ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_announcement_report_em_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘å…¬å‘Šå®šæœŸæŠ¥å‘Šç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_announcement_report_em.count_documents({})
            unique_funds = await self.col_fund_announcement_report_em.distinct('fund_code')
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds)
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘å…¬å‘Šå®šæœŸæŠ¥å‘Šç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

    async def save_fund_announcement_personnel_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """ä¿å­˜åŸºé‡‘å…¬å‘Šäººäº‹è°ƒæ•´-ä¸œè´¢æ•°æ®åˆ°MongoDB"""
        if df is None or df.empty:
            logger.warning("æ²¡æœ‰åŸºé‡‘å…¬å‘Šäººäº‹è°ƒæ•´æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡åŸºé‡‘å…¬å‘Šäººäº‹è°ƒæ•´æ•°æ®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    # å­—æ®µï¼šåŸºé‡‘ä»£ç ã€å…¬å‘Šæ ‡é¢˜ã€åŸºé‡‘åç§°ã€å…¬å‘Šæ—¥æœŸã€æŠ¥å‘ŠID
                    
                    fund_code = str(doc.get('åŸºé‡‘ä»£ç ', '')) or str(doc.get('code', ''))
                    title = str(doc.get('å…¬å‘Šæ ‡é¢˜', '')) or str(doc.get('title', ''))
                    fund_name = str(doc.get('åŸºé‡‘åç§°', '')) or str(doc.get('name', ''))
                    date_str = str(doc.get('å…¬å‘Šæ—¥æœŸ', '')) or str(doc.get('date', ''))
                    report_id = str(doc.get('æŠ¥å‘ŠID', '')) or str(doc.get('report_id', ''))
                    
                    doc['fund_code'] = fund_code
                    doc['title'] = title
                    doc['fund_name'] = fund_name
                    doc['date'] = date_str
                    doc['report_id'] = report_id
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_announcement_personnel_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # å”¯ä¸€æ ‡è¯†ï¼šfund_code + report_idï¼ˆæŠ¥å‘ŠIDæ˜¯å”¯ä¸€çš„ï¼‰
                    if fund_code and report_id:
                        ops.append(
                            UpdateOne(
                                {'fund_code': fund_code, 'report_id': report_id},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_announcement_personnel_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"å·²ä¿å­˜ {end_idx}/{total_count} æ¡æ•°æ® ({progress}%)"
                        )
            
            logger.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å†™å…¥å®Œæˆ: æ€»è®¡ä¿å­˜ {total_saved}/{total_count} æ¡åŸºé‡‘å…¬å‘Šäººäº‹è°ƒæ•´æ•°æ®")
            return total_saved
                
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºé‡‘å…¬å‘Šäººäº‹è°ƒæ•´æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def clear_fund_announcement_personnel_em_data(self) -> int:
        """æ¸…ç©ºåŸºé‡‘å…¬å‘Šäººäº‹è°ƒæ•´æ•°æ®"""
        try:
            result = await self.col_fund_announcement_personnel_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"æˆåŠŸæ¸…ç©º {deleted_count} æ¡åŸºé‡‘å…¬å‘Šäººäº‹è°ƒæ•´æ•°æ®")
            return deleted_count
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŸºé‡‘å…¬å‘Šäººäº‹è°ƒæ•´æ•°æ®å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def get_fund_announcement_personnel_em_stats(self) -> Dict[str, Any]:
        """è·å–åŸºé‡‘å…¬å‘Šäººäº‹è°ƒæ•´ç»Ÿè®¡"""
        try:
            total_count = await self.col_fund_announcement_personnel_em.count_documents({})
            unique_funds = await self.col_fund_announcement_personnel_em.distinct('fund_code')
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds)
            }
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘å…¬å‘Šäººäº‹è°ƒæ•´ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            raise

"""
Âü∫ÈáëÊï∞ÊçÆÊúçÂä°
Ë¥üË¥£‰ªéakshareËé∑ÂèñÂü∫ÈáëÊï∞ÊçÆÂπ∂Â≠òÂÇ®Âà∞MongoDB
"""
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, date
import pandas as pd
from motor.motor_asyncio import AsyncIOMotorDatabase
from pymongo import UpdateOne
import asyncio
import io

logger = logging.getLogger("webapi")


class FundDataService:
    """Âü∫ÈáëÊï∞ÊçÆÊúçÂä°Á±ª"""
    
    def __init__(self, db: AsyncIOMotorDatabase):
        self.db = db
        self.col_fund_name_em = db.get_collection("fund_name_em")
        self.col_fund_basic_info = db.get_collection("fund_basic_info")
        self.col_fund_info_index = db.get_collection("fund_info_index_em")
        self.col_fund_purchase_status = db.get_collection("fund_purchase_status")
        self.col_fund_etf_spot = db.get_collection("fund_etf_spot_em")
        self.col_fund_etf_spot_ths = db.get_collection("fund_etf_spot_ths")
        self.col_fund_lof_spot = db.get_collection("fund_lof_spot_em")
        self.col_fund_spot_sina = db.get_collection("fund_spot_sina")
        self.col_fund_etf_hist_min_em = db.get_collection("fund_etf_hist_min_em")
        self.col_fund_lof_hist_min_em = db.get_collection("fund_lof_hist_min_em")
        self.col_fund_etf_hist_em = db.get_collection("fund_etf_hist_em")
        self.col_fund_lof_hist_em = db.get_collection("fund_lof_hist_em")
        self.col_fund_hist_sina = db.get_collection("fund_hist_sina")
        self.col_fund_open_fund_daily_em = db.get_collection("fund_open_fund_daily_em")
        self.col_fund_open_fund_info_em = db.get_collection("fund_open_fund_info_em")
        self.col_fund_money_fund_daily_em = db.get_collection("fund_money_fund_daily_em")
        self.col_fund_money_fund_info_em = db.get_collection("fund_money_fund_info_em")
        self.col_fund_financial_fund_daily_em = db.get_collection("fund_financial_fund_daily_em")
        self.col_fund_financial_fund_info_em = db.get_collection("fund_financial_fund_info_em")
        self.col_fund_graded_fund_daily_em = db.get_collection("fund_graded_fund_daily_em")
        self.col_fund_graded_fund_info_em = db.get_collection("fund_graded_fund_info_em")
        self.col_fund_etf_fund_daily_em = db.get_collection("fund_etf_fund_daily_em")
        self.col_fund_hk_hist_em = db.get_collection("fund_hk_hist_em")
        self.col_fund_etf_fund_info_em = db.get_collection("fund_etf_fund_info_em")
        self.col_fund_etf_dividend_sina = db.get_collection("fund_etf_dividend_sina")
        self.col_fund_fh_em = db.get_collection("fund_fh_em")
        self.col_fund_cf_em = db.get_collection("fund_cf_em")
        self.col_fund_fh_rank_em = db.get_collection("fund_fh_rank_em")
        self.col_fund_open_fund_rank_em = db.get_collection("fund_open_fund_rank_em")
        self.col_fund_exchange_rank_em = db.get_collection("fund_exchange_rank_em")
        self.col_fund_money_rank_em = db.get_collection("fund_money_rank_em")
        self.col_fund_lcx_rank_em = db.get_collection("fund_lcx_rank_em")
        self.col_fund_hk_rank_em = db.get_collection("fund_hk_rank_em")
        self.col_fund_individual_achievement_xq = db.get_collection("fund_individual_achievement_xq")
        self.col_fund_value_estimation_em = db.get_collection("fund_value_estimation_em")
        self.col_fund_individual_analysis_xq = db.get_collection("fund_individual_analysis_xq")
        self.col_fund_individual_profit_probability_xq = db.get_collection("fund_individual_profit_probability_xq")
        self.col_fund_individual_detail_hold_xq = db.get_collection("fund_individual_detail_hold_xq")
        self.col_fund_overview_em = db.get_collection("fund_overview_em")
        self.col_fund_fee_em = db.get_collection("fund_fee_em")
        self.col_fund_individual_detail_info_xq = db.get_collection("fund_individual_detail_info_xq")
        self.col_fund_portfolio_hold_em = db.get_collection("fund_portfolio_hold_em")
        self.col_fund_portfolio_bond_hold_em = db.get_collection("fund_portfolio_bond_hold_em")
        self.col_fund_portfolio_change_em = db.get_collection("fund_portfolio_change_em")
        self.col_fund_rating_all_em = db.get_collection("fund_rating_all_em")
        self.col_fund_rating_sh_em = db.get_collection("fund_rating_sh_em")
        self.col_fund_rating_zs_em = db.get_collection("fund_rating_zs_em")
        self.col_fund_rating_ja_em = db.get_collection("fund_rating_ja_em")
        self.col_fund_manager_em = db.get_collection("fund_manager_em")
        self.col_fund_new_found_em = db.get_collection("fund_new_found_em")
        self.col_fund_scale_open_sina = db.get_collection("fund_scale_open_sina")
        self.col_fund_scale_close_sina = db.get_collection("fund_scale_close_sina")
        self.col_fund_scale_structured_sina = db.get_collection("fund_scale_structured_sina")
        self.col_fund_aum_em = db.get_collection("fund_aum_em")
        self.col_fund_aum_trend_em = db.get_collection("fund_aum_trend_em")
        self.col_fund_aum_hist_em = db.get_collection("fund_aum_hist_em")
        self.col_reits_realtime_em = db.get_collection("reits_realtime_em")
        self.col_reits_hist_em = db.get_collection("reits_hist_em")
        self.col_fund_report_stock_cninfo = db.get_collection("fund_report_stock_cninfo")
        self.col_fund_report_industry_allocation_cninfo = db.get_collection("fund_report_industry_allocation_cninfo")
        self.col_fund_report_asset_allocation_cninfo = db.get_collection("fund_report_asset_allocation_cninfo")
        self.col_fund_scale_change_em = db.get_collection("fund_scale_change_em")
        self.col_fund_hold_structure_em = db.get_collection("fund_hold_structure_em")
        self.col_fund_stock_position_lg = db.get_collection("fund_stock_position_lg")
        self.col_fund_balance_position_lg = db.get_collection("fund_balance_position_lg")
        self.col_fund_linghuo_position_lg = db.get_collection("fund_linghuo_position_lg")
        self.col_fund_announcement_dividend_em = db.get_collection("fund_announcement_dividend_em")
        self.col_fund_announcement_report_em = db.get_collection("fund_announcement_report_em")
        self.col_fund_announcement_personnel_em = db.get_collection("fund_announcement_personnel_em")
    
    async def save_fund_name_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ‰øùÂ≠òÂü∫ÈáëÂü∫Êú¨‰ø°ÊÅØÊï∞ÊçÆÂà∞MongoDB
        
        Args:
            df: ÂåÖÂê´Âü∫ÈáëÂü∫Êú¨‰ø°ÊÅØÁöÑDataFrame
            
        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÂü∫Êú¨‰ø°ÊÅØÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            # Ê∏ÖÁêÜÊó†ÊïàÁöÑÊµÆÁÇπÊï∞ÂÄºÔºàNaN, InfinityÁ≠âÔºâÔºåÈò≤Ê≠¢JSONÂ∫èÂàóÂåñÈîôËØØ
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÊï∞ÊçÆ...")
            
            # ÂàÜÊâπÂ§ÑÁêÜÔºåÊØèÊâπ500Êù°
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"üì¶ Â∞ÜÂàÜ {total_batches} ÊâπÊ¨°Â§ÑÁêÜÔºåÊØèÊâπ {batch_size} Êù°")
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"üìù Â§ÑÁêÜÁ¨¨ {batch_idx + 1}/{total_batches} ÊâπÔºåËÆ∞ÂΩïËåÉÂõ¥: {start_idx + 1}-{end_idx}")
                
                # ÊûÑÂª∫ÊâπÈáèÊìç‰Ωú
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # Ê∏ÖÁêÜNaN/InfinityÂÄºÔºàto_dict()ÂèØËÉΩ‰ºöÈáçÊñ∞ÂºïÂÖ•NaNÔºâ
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        # ËΩ¨Êç¢ datetime.date ÂØπË±°‰∏∫Â≠óÁ¨¶‰∏≤
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        # ËΩ¨Êç¢ datetime.datetime ÂØπË±°‰∏∫Â≠óÁ¨¶‰∏≤
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_name_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫Èáë‰ª£Á†Å‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'endpoint': 'fund_name_em'},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # ÊâßË°åÊâπÈáèÂÜôÂÖ•
                if ops:
                    result = await self.col_fund_name_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"‚úÖ Á¨¨ {batch_idx + 1}/{total_batches} ÊâπÂÜôÂÖ•ÂÆåÊàê: "
                        f"Êñ∞Â¢û={result.upserted_count}, Êõ¥Êñ∞={result.matched_count}, "
                        f"Êú¨Êâπ‰øùÂ≠ò={batch_saved}, Á¥ØËÆ°={total_saved}/{total_count}"
                    )
                    
                    # Ë∞ÉÁî®ËøõÂ∫¶ÂõûË∞ÉÔºàÂ¶ÇÊûúÊèê‰æõÔºâ
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÂü∫Êú¨‰ø°ÊÅØÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_name_em_data(self) -> int:
        """
        Ê∏ÖÁ©∫Âü∫ÈáëÂü∫Êú¨‰ø°ÊÅØÊï∞ÊçÆ
        
        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_name_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÂü∫Êú¨‰ø°ÊÅØÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÂü∫Êú¨‰ø°ÊÅØÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_name_em_stats(self) -> Dict[str, Any]:
        """
        Ëé∑ÂèñÂü∫ÈáëÂü∫Êú¨‰ø°ÊÅØÁªüËÆ°
        
        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_count = await self.col_fund_name_em.count_documents({})
            
            # ÊåâÂü∫ÈáëÁ±ªÂûãÁªüËÆ°
            pipeline = [
                {
                    '$group': {
                        '_id': '$Âü∫ÈáëÁ±ªÂûã',
                        'count': {'$sum': 1}
                    }
                },
                {
                    '$sort': {'count': -1}
                }
            ]
            
            type_stats = []
            async for doc in self.col_fund_name_em.aggregate(pipeline):
                type_stats.append({
                    'type': doc['_id'],
                    'count': doc['count']
                })
            
            return {
                'total_count': total_count,
                'type_stats': type_stats
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÂü∫Êú¨‰ø°ÊÅØÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_basic_info_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ‰øùÂ≠òÂü∫ÈáëÂü∫Êú¨‰ø°ÊÅØÊï∞ÊçÆÂà∞fund_basic_infoÈõÜÂêà
        ‰ΩøÁî® fund_individual_basic_info_xq Êï∞ÊçÆÊ∫ê
        
        Args:
            df: ÂåÖÂê´Âü∫ÈáëÂü∫Êú¨‰ø°ÊÅØÁöÑDataFrame
            
        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÂü∫Êú¨‰ø°ÊÅØÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            # Ê∏ÖÁêÜÊó†ÊïàÁöÑÊµÆÁÇπÊï∞ÂÄºÔºàNaN, InfinityÁ≠âÔºâÔºåÈò≤Ê≠¢JSONÂ∫èÂàóÂåñÈîôËØØ
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÊï∞ÊçÆÂà∞fund_basic_infoÈõÜÂêà...")
            
            # ÂàÜÊâπÂ§ÑÁêÜÔºåÊØèÊâπ500Êù°
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"üì¶ Â∞ÜÂàÜ {total_batches} ÊâπÊ¨°Â§ÑÁêÜÔºåÊØèÊâπ {batch_size} Êù°")
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"üìù Â§ÑÁêÜÁ¨¨ {batch_idx + 1}/{total_batches} ÊâπÔºåËÆ∞ÂΩïËåÉÂõ¥: {start_idx + 1}-{end_idx}")
                
                # ÊûÑÂª∫ÊâπÈáèÊìç‰Ωú
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # Ê∏ÖÁêÜNaN/InfinityÂÄºÔºàto_dict()ÂèØËÉΩ‰ºöÈáçÊñ∞ÂºïÂÖ•NaNÔºâ
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        # ËΩ¨Êç¢ datetime.date ÂØπË±°‰∏∫Â≠óÁ¨¶‰∏≤
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        # ËΩ¨Êç¢ datetime.datetime ÂØπË±°‰∏∫Â≠óÁ¨¶‰∏≤
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_individual_basic_info_xq'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫Èáë‰ª£Á†Å‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    ops.append(
                        UpdateOne(
                            {'code': fund_code},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # ÊâßË°åÊâπÈáèÂÜôÂÖ•
                if ops:
                    result = await self.col_fund_basic_info.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"‚úÖ Á¨¨ {batch_idx + 1}/{total_batches} ÊâπÂÜôÂÖ•fund_basic_infoÂÆåÊàê: "
                        f"Êñ∞Â¢û={result.upserted_count}, Êõ¥Êñ∞={result.matched_count}, "
                        f"Êú¨Êâπ‰øùÂ≠ò={batch_saved}, Á¥ØËÆ°={total_saved}/{total_count}"
                    )
                    
                    # Ë∞ÉÁî®ËøõÂ∫¶ÂõûË∞ÉÔºàÂ¶ÇÊûúÊèê‰æõÔºâ
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆÂà∞fund_basic_info ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•fund_basic_infoÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÂü∫Êú¨‰ø°ÊÅØÊï∞ÊçÆÂà∞fund_basic_infoÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_basic_info_data(self) -> int:
        """
        Ê∏ÖÁ©∫fund_basic_infoÂü∫ÈáëÊï∞ÊçÆ
        
        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_basic_info.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫fund_basic_info {deleted_count} Êù°Êï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫fund_basic_infoÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_basic_info_stats(self) -> Dict[str, Any]:
        """
        Ëé∑Âèñfund_basic_infoÈõÜÂêàÁªüËÆ°
        
        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_count = await self.col_fund_basic_info.count_documents({})
            
            # ÊåâÂü∫ÈáëÁ±ªÂûãÁªüËÆ°
            pipeline = [
                {
                    '$group': {
                        '_id': '$Âü∫ÈáëÁ±ªÂûã',
                        'count': {'$sum': 1}
                    }
                },
                {
                    '$sort': {'count': -1}
                }
            ]
            
            type_stats = []
            async for doc in self.col_fund_basic_info.aggregate(pipeline):
                type_stats.append({
                    'type': doc['_id'],
                    'count': doc['count']
                })
            
            return {
                'total_count': total_count,
                'type_stats': type_stats
            }
        except Exception as e:
            logger.error(f"Ëé∑Âèñfund_basic_infoÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_info_index_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÊåáÊï∞ÂûãÂü∫ÈáëÂü∫Êú¨‰ø°ÊÅØÊï∞ÊçÆÂà∞ fund_info_index_em ÈõÜÂêà„ÄÇ

        ‰ΩøÁî® akshare fund_info_index_em Êé•Âè£Êï∞ÊçÆÔºå
        ‰ª• Âü∫Èáë‰ª£Á†Å + Êó•Êúü ‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ„ÄÇ
        """
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÊåáÊï∞ÂûãÂü∫ÈáëÂü∫Êú¨‰ø°ÊÅØÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0

        try:
            # Ê∏ÖÁêÜÊó†ÊïàÁöÑÊµÆÁÇπÊï∞ÂÄºÔºàNaN, InfinityÁ≠âÔºâÔºåÈò≤Ê≠¢JSONÂ∫èÂàóÂåñÈîôËØØ
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)  # ÊõøÊç¢Êó†Á©∑Â§ß‰∏∫None
            df = df.where(pd.notna(df), None)  # ÊõøÊç¢NaN‰∏∫None
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°ÊåáÊï∞ÂûãÂü∫ÈáëÊï∞ÊçÆÂà∞ fund_info_index_em ÈõÜÂêà...")

            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size

            logger.info(f"üì¶ Â∞ÜÂàÜ {total_batches} ÊâπÊ¨°Â§ÑÁêÜÔºåÊØèÊâπ {batch_size} Êù°")

            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]

                logger.info(f"üìù Â§ÑÁêÜÁ¨¨ {batch_idx + 1}/{total_batches} ÊâπÔºåËÆ∞ÂΩïËåÉÂõ¥: {start_idx + 1}-{end_idx}")

                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # ÂÜçÊ¨°Ê∏ÖÁêÜNaN/InfinityÂÄºÔºàto_dict()ÂèØËÉΩ‰ºöÈáçÊñ∞ÂºïÂÖ•NaNÔºâ
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        # ËΩ¨Êç¢ datetime.date ÂØπË±°‰∏∫Â≠óÁ¨¶‰∏≤
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        # ËΩ¨Êç¢ datetime.datetime ÂØπË±°‰∏∫Â≠óÁ¨¶‰∏≤
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')

                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', '')).strip()
                    date_str = str(doc.get('Êó•Êúü', '')).strip()
                    tracking_target = str(doc.get('Ë∑üË∏™Ê†áÁöÑ', '')).strip()
                    
                    if not fund_code or not date_str or not tracking_target:
                        continue

                    # ÂÖÉÊï∞ÊçÆ
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_info_index_em'
                    doc['updated_at'] = datetime.now().isoformat()

                    # ‰ΩøÁî® Êó•Êúü + Âü∫Èáë‰ª£Á†Å + Ë∑üË∏™Ê†áÁöÑ ‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    ops.append(
                        UpdateOne(
                            {
                                'Êó•Êúü': date_str,
                                'code': fund_code,
                                'Ë∑üË∏™Ê†áÁöÑ': tracking_target
                            },
                            {'$set': doc},
                            upsert=True
                        )
                    )

                if ops:
                    result = await self.col_fund_info_index.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved

                    logger.info(
                        f"‚úÖ Á¨¨ {batch_idx + 1}/{total_batches} ÊâπÂÜôÂÖ• fund_info_index_em ÂÆåÊàê: "
                        f"Êñ∞Â¢û={result.upserted_count}, Êõ¥Êñ∞={result.matched_count}, "
                        f"Êú¨Êâπ‰øùÂ≠ò={batch_saved}, Á¥ØËÆ°={total_saved}/{total_count}"
                    )

                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆÂà∞ fund_info_index_em ({progress}%)"
                        )

            logger.info(
                f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ• fund_info_index_em ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°ÊåáÊï∞ÂûãÂü∫ÈáëÊï∞ÊçÆ"
            )
            return total_saved
        except Exception as e:
            logger.error(f"‰øùÂ≠òÊåáÊï∞ÂûãÂü∫ÈáëÂü∫Êú¨‰ø°ÊÅØÊï∞ÊçÆÂà∞ fund_info_index_em Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def clear_fund_info_index_data(self) -> int:
        """Ê∏ÖÁ©∫ fund_info_index_em ÊåáÊï∞ÂûãÂü∫ÈáëÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_info_index.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ fund_info_index_em {deleted_count} Êù°Êï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ fund_info_index_em Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def get_fund_info_index_stats(self) -> Dict[str, Any]:
        """Ëé∑Âèñ fund_info_index_em ÈõÜÂêàÁªüËÆ°‰ø°ÊÅØ"""
        try:
            total_count = await self.col_fund_info_index.count_documents({})

            # ÊåâË∑üË∏™Ê†áÁöÑÁªüËÆ°
            pipeline_type = [
                {
                    '$group': {
                        '_id': '$Ë∑üË∏™Ê†áÁöÑ',
                        'count': {'$sum': 1}
                    }
                },
                {
                    '$sort': {'count': -1}
                }
            ]

            type_stats: List[Dict[str, Any]] = []
            async for doc in self.col_fund_info_index.aggregate(pipeline_type):
                type_stats.append({
                    'type': doc['_id'],
                    'count': doc['count']
                })

            # ËÆ°ÁÆóÊúÄÊó©ÂíåÊúÄÊôöÊó•Êúü
            earliest_date = None
            latest_date = None
            pipeline_date = [
                {
                    '$group': {
                        '_id': None,
                        'earliest': {'$min': '$Êó•Êúü'},
                        'latest': {'$max': '$Êó•Êúü'}
                    }
                }
            ]

            async for doc in self.col_fund_info_index.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')

            return {
                'total_count': total_count,
                'type_stats': type_stats,
                'earliest_date': earliest_date,
                'latest_date': latest_date
            }
        except Exception as e:
            logger.error(f"Ëé∑Âèñ fund_info_index_em ÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def import_data_from_file(self, collection_name: str, content: bytes, filename: str) -> Dict[str, Any]:
        """‰ªéÊñá‰ª∂ÂØºÂÖ•Êï∞ÊçÆ"""
        try:
            if filename.endswith('.csv'):
                df = pd.read_csv(io.BytesIO(content))
            else:
                df = pd.read_excel(io.BytesIO(content))
            
            if df.empty:
                return {"imported_count": 0, "message": "Êñá‰ª∂‰∏∫Á©∫"}
                
            count = 0
            if collection_name == 'fund_name_em':
                count = await self.save_fund_name_em_data(df)
            elif collection_name == 'fund_basic_info':
                count = await self.save_fund_basic_info_data(df)
            elif collection_name == 'fund_info_index_em':
                count = await self.save_fund_info_index_data(df)
            elif collection_name == 'fund_purchase_status':
                count = await self.save_fund_purchase_status_data(df)
            elif collection_name == 'fund_etf_spot_em':
                count = await self.save_fund_etf_spot_data(df)
            elif collection_name == 'fund_etf_spot_ths':
                count = await self.save_fund_etf_spot_ths_data(df)
            elif collection_name == 'fund_lof_spot_em':
                count = await self.save_fund_lof_spot_data(df)
            elif collection_name == 'fund_spot_sina':
                count = await self.save_fund_spot_sina_data(df)
            elif collection_name == 'fund_etf_hist_min_em':
                count = await self.save_fund_etf_hist_min_data(df)
            elif collection_name == 'fund_etf_hist_em':
                count = await self.save_fund_etf_hist_data(df)
            elif collection_name == 'fund_lof_hist_em':
                count = await self.save_fund_lof_hist_data(df)
            elif collection_name == 'fund_hist_sina':
                count = await self.save_fund_hist_sina_data(df)
            elif collection_name == 'fund_open_fund_daily_em':
                count = await self.save_fund_open_fund_daily_data(df)
            elif collection_name == 'fund_open_fund_info_em':
                # Êñá‰ª∂ÂØºÂÖ•ÈúÄË¶ÅÊåáÂÆö fund_code Âíå indicator
                logger.warning("ÂºÄÊîæÂºèÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊñá‰ª∂ÂØºÂÖ•ÈúÄË¶ÅÁâπÊÆäÂ§ÑÁêÜÔºåËØ∑‰ΩøÁî® API Âà∑Êñ∞")
                return {"imported_count": 0, "message": "ËØ•ÈõÜÂêàÈúÄË¶ÅÈÄöËøá API Âà∑Êñ∞"}
            elif collection_name == 'fund_money_fund_daily_em':
                count = await self.save_fund_money_fund_daily_data(df)
            elif collection_name == 'fund_money_fund_info_em':
                logger.warning("Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊñá‰ª∂ÂØºÂÖ•ÈúÄË¶ÅÊåáÂÆöÂü∫Èáë‰ª£Á†Å")
                return {"imported_count": 0, "message": "ËØ•ÈõÜÂêàÈúÄË¶ÅÈÄöËøá API Âà∑Êñ∞"}
            elif collection_name == 'fund_financial_fund_daily_em':
                count = await self.save_fund_financial_fund_daily_data(df)
            elif collection_name == 'fund_financial_fund_info_em':
                logger.warning("ÁêÜË¥¢ÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÅÊñá‰ª∂ÂØºÂÖ•ÈúÄË¶ÅÊåáÂÆöÂü∫Èáë‰ª£Á†Å")
                return {"imported_count": 0, "message": "ËØ•ÈõÜÂêàÈúÄË¶ÅÈÄöËøá API Âà∑Êñ∞"}
            elif collection_name == 'fund_graded_fund_daily_em':
                count = await self.save_fund_graded_fund_daily_data(df)
            elif collection_name == 'fund_graded_fund_info_em':
                logger.warning("ÂàÜÁ∫ßÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆÊñá‰ª∂ÂØºÂÖ•ÈúÄË¶ÅÊåáÂÆöÂü∫Èáë‰ª£Á†Å")
                return {"imported_count": 0, "message": "ËØ•ÈõÜÂêàÈúÄË¶ÅÈÄöËøá API Âà∑Êñ∞"}
            elif collection_name == 'fund_etf_fund_daily_em':
                count = await self.save_fund_etf_fund_daily_data(df)
            elif collection_name == 'fund_hk_hist_em':
                count = await self.save_fund_hk_hist_em_data(df)
            elif collection_name == 'fund_cf_em':
                count = await self.save_fund_cf_em_data(df)
            elif collection_name == 'fund_fh_rank_em':
                count = await self.save_fund_fh_rank_em_data(df)
            elif collection_name == 'fund_open_fund_rank_em':
                count = await self.save_fund_open_fund_rank_em_data(df)
            elif collection_name == 'fund_exchange_rank_em':
                count = await self.save_fund_exchange_rank_em_data(df)
            elif collection_name == 'fund_money_rank_em':
                count = await self.save_fund_money_rank_em_data(df)
            elif collection_name == 'fund_lcx_rank_em':
                count = await self.save_fund_lcx_rank_em_data(df)
            elif collection_name == 'fund_hk_rank_em':
                count = await self.save_fund_hk_rank_em_data(df)
            elif collection_name == 'fund_individual_achievement_xq':
                count = await self.save_fund_individual_achievement_xq_data(df)
            elif collection_name == 'fund_value_estimation_em':
                count = await self.save_fund_value_estimation_em_data(df)
            elif collection_name == 'fund_individual_analysis_xq':
                count = await self.save_fund_individual_analysis_xq_data(df)
            elif collection_name == 'fund_individual_profit_probability_xq':
                count = await self.save_fund_individual_profit_probability_xq_data(df)
            elif collection_name == 'fund_individual_detail_hold_xq':
                count = await self.save_fund_individual_detail_hold_xq_data(df)
            elif collection_name == 'fund_overview_em':
                count = await self.save_fund_overview_em_data(df)
            elif collection_name == 'fund_fee_em':
                count = await self.save_fund_fee_em_data(df)
            elif collection_name == 'fund_individual_detail_info_xq':
                count = await self.save_fund_individual_detail_info_xq_data(df)
            elif collection_name == 'fund_portfolio_hold_em':
                count = await self.save_fund_portfolio_hold_em_data(df)
            elif collection_name == 'fund_portfolio_bond_hold_em':
                count = await self.save_fund_portfolio_bond_hold_em_data(df)
            elif collection_name == 'fund_portfolio_industry_allocation_em':
                count = await self.save_fund_portfolio_industry_allocation_em_data(df)
            elif collection_name == 'fund_portfolio_change_em':
                count = await self.save_fund_portfolio_change_em_data(df)
            elif collection_name == 'fund_rating_all_em':
                count = await self.save_fund_rating_all_em_data(df)
            elif collection_name == 'fund_rating_sh_em':
                count = await self.save_fund_rating_sh_em_data(df)
            elif collection_name == 'fund_rating_zs_em':
                count = await self.save_fund_rating_zs_em_data(df)
            elif collection_name == 'fund_rating_ja_em':
                count = await self.save_fund_rating_ja_em_data(df)
            elif collection_name == 'fund_manager_em':
                count = await self.save_fund_manager_em_data(df)
            elif collection_name == 'fund_new_found_em':
                count = await self.save_fund_new_found_em_data(df)
            elif collection_name == 'fund_scale_open_sina':
                count = await self.save_fund_scale_open_sina_data(df)
            elif collection_name == 'fund_scale_close_sina':
                count = await self.save_fund_scale_close_sina_data(df)
            elif collection_name == 'fund_scale_structured_sina':
                count = await self.save_fund_scale_structured_sina_data(df)
            elif collection_name == 'fund_aum_em':
                count = await self.save_fund_aum_em_data(df)
            elif collection_name == 'fund_aum_trend_em':
                count = await self.save_fund_aum_trend_em_data(df)
            elif collection_name == 'fund_aum_hist_em':
                count = await self.save_fund_aum_hist_em_data(df)
            elif collection_name == 'reits_realtime_em':
                count = await self.save_reits_realtime_em_data(df)
            elif collection_name == 'reits_hist_em':
                count = await self.save_reits_hist_em_data(df)
            elif collection_name == 'fund_report_stock_cninfo':
                count = await self.save_fund_report_stock_cninfo_data(df)
            elif collection_name == 'fund_report_industry_allocation_cninfo':
                count = await self.save_fund_report_industry_allocation_cninfo_data(df)
            elif collection_name == 'fund_report_asset_allocation_cninfo':
                count = await self.save_fund_report_asset_allocation_cninfo_data(df)
            elif collection_name == 'fund_scale_change_em':
                count = await self.save_fund_scale_change_em_data(df)
            elif collection_name == 'fund_hold_structure_em':
                count = await self.save_fund_hold_structure_em_data(df)
            elif collection_name == 'fund_stock_position_lg':
                count = await self.save_fund_stock_position_lg_data(df)
            elif collection_name == 'fund_balance_position_lg':
                count = await self.save_fund_balance_position_lg_data(df)
            elif collection_name == 'fund_linghuo_position_lg':
                count = await self.save_fund_linghuo_position_lg_data(df)
            elif collection_name == 'fund_announcement_dividend_em':
                count = await self.save_fund_announcement_dividend_em_data(df)
            elif collection_name == 'fund_announcement_report_em':
                count = await self.save_fund_announcement_report_em_data(df)
            elif collection_name == 'fund_announcement_personnel_em':
                count = await self.save_fund_announcement_personnel_em_data(df)
            else:
                raise ValueError(f"‰∏çÊîØÊåÅÁöÑÊñá‰ª∂ÂØºÂÖ•ÈõÜÂêà: {collection_name}")
                
            return {"imported_count": count, "message": f"ÊàêÂäüÂØºÂÖ• {count} Êù°Êï∞ÊçÆ"}
        except Exception as e:
            logger.error(f"ÂØºÂÖ•Êñá‰ª∂Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def sync_data_from_remote(self, collection_name: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """‰ªéËøúÁ®ãÊï∞ÊçÆÂ∫ìÂêåÊ≠•Êï∞ÊçÆ"""
        from motor.motor_asyncio import AsyncIOMotorClient
        try:
            host = config.get('host')
            port = int(config.get('port', 27017))
            username = config.get('username')
            password = config.get('password')
            auth_source = config.get('authSource', 'admin')
            remote_db_name = config.get('database', 'tradingagents') 
            remote_col_name = config.get('collection', collection_name)
            batch_size = int(config.get('batch_size', 1000))
            
            if username and password:
                uri = f"mongodb://{username}:{password}@{host}:{port}/{auth_source}"
            else:
                uri = f"mongodb://{host}:{port}"
            
            if "mongodb://" in host:
                client = AsyncIOMotorClient(host)
            else:
                client = AsyncIOMotorClient(uri)
                
            try:
                if "mongodb://" in host and "/" in host.split("://")[1]:
                     remote_db = client.get_default_database()
                else:
                     remote_db = client[remote_db_name]
            except Exception:
                remote_db = client[remote_db_name]

            remote_col = remote_db[remote_col_name]
            
            cursor = remote_col.find({})
            
            batch = []
            total_synced = 0
            
            async for doc in cursor:
                if '_id' in doc:
                    del doc['_id']
                batch.append(doc)
                
                if len(batch) >= batch_size:
                    df = pd.DataFrame(batch)
                    if collection_name == 'fund_name_em':
                        await self.save_fund_name_em_data(df)
                    elif collection_name == 'fund_basic_info':
                        await self.save_fund_basic_info_data(df)
                    elif collection_name == 'fund_info_index_em':
                        await self.save_fund_info_index_data(df)
                    elif collection_name == 'fund_purchase_status':
                        await self.save_fund_purchase_status_data(df)
                    elif collection_name == 'fund_etf_spot_em':
                        await self.save_fund_etf_spot_data(df)
                    elif collection_name == 'fund_etf_spot_ths':
                        await self.save_fund_etf_spot_ths_data(df)
                    elif collection_name == 'fund_lof_spot_em':
                        await self.save_fund_lof_spot_data(df)
                    elif collection_name == 'fund_spot_sina':
                        await self.save_fund_spot_sina_data(df)
                    elif collection_name == 'fund_etf_hist_min_em':
                        await self.save_fund_etf_hist_min_data(df)
                    elif collection_name == 'fund_etf_hist_em':
                        await self.save_fund_etf_hist_data(df)
                    elif collection_name == 'fund_lof_hist_em':
                        await self.save_fund_lof_hist_data(df)
                    elif collection_name == 'fund_hist_sina':
                        await self.save_fund_hist_sina_data(df)
                    elif collection_name == 'fund_open_fund_daily_em':
                        await self.save_fund_open_fund_daily_data(df)
                    elif collection_name == 'fund_open_fund_info_em':
                        logger.warning("ÂºÄÊîæÂºèÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÈúÄË¶ÅÁâπÊÆäÂ§ÑÁêÜ")
                    elif collection_name == 'fund_money_fund_daily_em':
                        await self.save_fund_money_fund_daily_data(df)
                    elif collection_name == 'fund_money_fund_info_em':
                        logger.warning("Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÈúÄË¶ÅÁâπÊÆäÂ§ÑÁêÜ")
                    elif collection_name == 'fund_hk_hist_em':
                        await self.save_fund_hk_hist_em_data(df)
                    elif collection_name == 'fund_cf_em':
                        await self.save_fund_cf_em_data(df)
                    elif collection_name == 'fund_fh_rank_em':
                        await self.save_fund_fh_rank_em_data(df)
                    elif collection_name == 'fund_open_fund_rank_em':
                        await self.save_fund_open_fund_rank_em_data(df)
                    elif collection_name == 'fund_exchange_rank_em':
                        await self.save_fund_exchange_rank_em_data(df)
                    elif collection_name == 'fund_money_rank_em':
                        await self.save_fund_money_rank_em_data(df)
                    elif collection_name == 'fund_lcx_rank_em':
                        await self.save_fund_lcx_rank_em_data(df)
                    elif collection_name == 'fund_hk_rank_em':
                        await self.save_fund_hk_rank_em_data(df)
                    elif collection_name == 'fund_individual_achievement_xq':
                        await self.save_fund_individual_achievement_xq_data(df)
                    elif collection_name == 'fund_value_estimation_em':
                        await self.save_fund_value_estimation_em_data(df)
                    elif collection_name == 'fund_individual_analysis_xq':
                        await self.save_fund_individual_analysis_xq_data(df)
                    elif collection_name == 'fund_individual_profit_probability_xq':
                        await self.save_fund_individual_profit_probability_xq_data(df)
                    elif collection_name == 'fund_individual_detail_hold_xq':
                        await self.save_fund_individual_detail_hold_xq_data(df)
                    elif collection_name == 'fund_overview_em':
                        await self.save_fund_overview_em_data(df)
                    elif collection_name == 'fund_fee_em':
                        await self.save_fund_fee_em_data(df)
                    elif collection_name == 'fund_individual_detail_info_xq':
                        await self.save_fund_individual_detail_info_xq_data(df)
                    elif collection_name == 'fund_portfolio_hold_em':
                        await self.save_fund_portfolio_hold_em_data(df)
                    elif collection_name == 'fund_portfolio_bond_hold_em':
                        await self.save_fund_portfolio_bond_hold_em_data(df)
                    elif collection_name == 'fund_portfolio_industry_allocation_em':
                        await self.save_fund_portfolio_industry_allocation_em_data(df)
                    elif collection_name == 'fund_portfolio_change_em':
                        await self.save_fund_portfolio_change_em_data(df)
                    elif collection_name == 'fund_rating_all_em':
                        await self.save_fund_rating_all_em_data(df)
                    elif collection_name == 'fund_rating_sh_em':
                        await self.save_fund_rating_sh_em_data(df)
                    elif collection_name == 'fund_rating_zs_em':
                        await self.save_fund_rating_zs_em_data(df)
                    elif collection_name == 'fund_rating_ja_em':
                        await self.save_fund_rating_ja_em_data(df)
                    elif collection_name == 'fund_manager_em':
                        await self.save_fund_manager_em_data(df)
                    elif collection_name == 'fund_new_found_em':
                        await self.save_fund_new_found_em_data(df)
                    elif collection_name == 'fund_scale_open_sina':
                        await self.save_fund_scale_open_sina_data(df)
                    elif collection_name == 'fund_scale_close_sina':
                        await self.save_fund_scale_close_sina_data(df)
                    elif collection_name == 'fund_scale_structured_sina':
                        await self.save_fund_scale_structured_sina_data(df)
                    elif collection_name == 'fund_aum_em':
                        await self.save_fund_aum_em_data(df)
                    elif collection_name == 'fund_aum_trend_em':
                        await self.save_fund_aum_trend_em_data(df)
                    elif collection_name == 'fund_aum_hist_em':
                        await self.save_fund_aum_hist_em_data(df)
                    elif collection_name == 'reits_realtime_em':
                        await self.save_reits_realtime_em_data(df)
                    elif collection_name == 'reits_hist_em':
                        await self.save_reits_hist_em_data(df)
                    elif collection_name == 'fund_report_stock_cninfo':
                        await self.save_fund_report_stock_cninfo_data(df)
                    elif collection_name == 'fund_report_industry_allocation_cninfo':
                        await self.save_fund_report_industry_allocation_cninfo_data(df)
                    elif collection_name == 'fund_report_asset_allocation_cninfo':
                        await self.save_fund_report_asset_allocation_cninfo_data(df)
                    elif collection_name == 'fund_scale_change_em':
                        await self.save_fund_scale_change_em_data(df)
                    elif collection_name == 'fund_hold_structure_em':
                        await self.save_fund_hold_structure_em_data(df)
                    elif collection_name == 'fund_stock_position_lg':
                        await self.save_fund_stock_position_lg_data(df)
                    elif collection_name == 'fund_balance_position_lg':
                        await self.save_fund_balance_position_lg_data(df)
                    elif collection_name == 'fund_linghuo_position_lg':
                        await self.save_fund_linghuo_position_lg_data(df)
                    elif collection_name == 'fund_announcement_dividend_em':
                        await self.save_fund_announcement_dividend_em_data(df)
                    elif collection_name == 'fund_announcement_report_em':
                        await self.save_fund_announcement_report_em_data(df)
                    elif collection_name == 'fund_announcement_personnel_em':
                        await self.save_fund_announcement_personnel_em_data(df)
                    total_synced += len(batch)
                    batch = []
            
            if batch:
                df = pd.DataFrame(batch)
                if collection_name == 'fund_name_em':
                    await self.save_fund_name_em_data(df)
                elif collection_name == 'fund_basic_info':
                    await self.save_fund_basic_info_data(df)
                elif collection_name == 'fund_info_index_em':
                    await self.save_fund_info_index_data(df)
                elif collection_name == 'fund_purchase_status':
                    await self.save_fund_purchase_status_data(df)
                elif collection_name == 'fund_etf_spot_em':
                    await self.save_fund_etf_spot_data(df)
                elif collection_name == 'fund_etf_spot_ths':
                    await self.save_fund_etf_spot_ths_data(df)
                elif collection_name == 'fund_lof_spot_em':
                    await self.save_fund_lof_spot_data(df)
                elif collection_name == 'fund_spot_sina':
                    await self.save_fund_spot_sina_data(df)
                elif collection_name == 'fund_etf_hist_min_em':
                    await self.save_fund_etf_hist_min_data(df)
                elif collection_name == 'fund_etf_hist_em':
                    await self.save_fund_etf_hist_data(df)
                elif collection_name == 'fund_lof_hist_em':
                    await self.save_fund_lof_hist_data(df)
                elif collection_name == 'fund_hist_sina':
                    await self.save_fund_hist_sina_data(df)
                elif collection_name == 'fund_open_fund_daily_em':
                    await self.save_fund_open_fund_daily_data(df)
                elif collection_name == 'fund_open_fund_info_em':
                    logger.warning("ÂºÄÊîæÂºèÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÈúÄË¶ÅÁâπÊÆäÂ§ÑÁêÜ")
                elif collection_name == 'fund_money_fund_daily_em':
                    await self.save_fund_money_fund_daily_data(df)
                elif collection_name == 'fund_money_fund_info_em':
                    logger.warning("Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÈúÄË¶ÅÁâπÊÆäÂ§ÑÁêÜ")
                elif collection_name == 'fund_hk_hist_em':
                    await self.save_fund_hk_hist_em_data(df)
                elif collection_name == 'fund_cf_em':
                    await self.save_fund_cf_em_data(df)
                elif collection_name == 'fund_fh_rank_em':
                    await self.save_fund_fh_rank_em_data(df)
                elif collection_name == 'fund_open_fund_rank_em':
                    await self.save_fund_open_fund_rank_em_data(df)
                elif collection_name == 'fund_exchange_rank_em':
                    await self.save_fund_exchange_rank_em_data(df)
                elif collection_name == 'fund_money_rank_em':
                    await self.save_fund_money_rank_em_data(df)
                elif collection_name == 'fund_lcx_rank_em':
                    await self.save_fund_lcx_rank_em_data(df)
                elif collection_name == 'fund_hk_rank_em':
                    await self.save_fund_hk_rank_em_data(df)
                elif collection_name == 'fund_individual_achievement_xq':
                    await self.save_fund_individual_achievement_xq_data(df)
                elif collection_name == 'fund_value_estimation_em':
                    await self.save_fund_value_estimation_em_data(df)
                elif collection_name == 'fund_individual_analysis_xq':
                    await self.save_fund_individual_analysis_xq_data(df)
                elif collection_name == 'fund_individual_profit_probability_xq':
                    await self.save_fund_individual_profit_probability_xq_data(df)
                elif collection_name == 'fund_individual_detail_hold_xq':
                    await self.save_fund_individual_detail_hold_xq_data(df)
                elif collection_name == 'fund_overview_em':
                    await self.save_fund_overview_em_data(df)
                elif collection_name == 'fund_fee_em':
                    await self.save_fund_fee_em_data(df)
                elif collection_name == 'fund_individual_detail_info_xq':
                    await self.save_fund_individual_detail_info_xq_data(df)
                elif collection_name == 'fund_portfolio_hold_em':
                    await self.save_fund_portfolio_hold_em_data(df)
                elif collection_name == 'fund_portfolio_bond_hold_em':
                    await self.save_fund_portfolio_bond_hold_em_data(df)
                elif collection_name == 'fund_portfolio_industry_allocation_em':
                    await self.save_fund_portfolio_industry_allocation_em_data(df)
                elif collection_name == 'fund_portfolio_change_em':
                    await self.save_fund_portfolio_change_em_data(df)
                elif collection_name == 'fund_rating_all_em':
                    await self.save_fund_rating_all_em_data(df)
                elif collection_name == 'fund_rating_sh_em':
                    await self.save_fund_rating_sh_em_data(df)
                elif collection_name == 'fund_rating_zs_em':
                    await self.save_fund_rating_zs_em_data(df)
                elif collection_name == 'fund_rating_ja_em':
                    await self.save_fund_rating_ja_em_data(df)
                elif collection_name == 'fund_manager_em':
                    await self.save_fund_manager_em_data(df)
                elif collection_name == 'fund_new_found_em':
                    await self.save_fund_new_found_em_data(df)
                elif collection_name == 'fund_scale_open_sina':
                    await self.save_fund_scale_open_sina_data(df)
                elif collection_name == 'fund_scale_close_sina':
                    await self.save_fund_scale_close_sina_data(df)
                elif collection_name == 'fund_scale_structured_sina':
                    await self.save_fund_scale_structured_sina_data(df)
                elif collection_name == 'fund_aum_em':
                    await self.save_fund_aum_em_data(df)
                elif collection_name == 'fund_aum_trend_em':
                    await self.save_fund_aum_trend_em_data(df)
                elif collection_name == 'fund_aum_hist_em':
                    await self.save_fund_aum_hist_em_data(df)
                elif collection_name == 'reits_realtime_em':
                    await self.save_reits_realtime_em_data(df)
                elif collection_name == 'reits_hist_em':
                    await self.save_reits_hist_em_data(df)
                elif collection_name == 'fund_report_stock_cninfo':
                    await self.save_fund_report_stock_cninfo_data(df)
                elif collection_name == 'fund_report_industry_allocation_cninfo':
                    await self.save_fund_report_industry_allocation_cninfo_data(df)
                elif collection_name == 'fund_report_asset_allocation_cninfo':
                    await self.save_fund_report_asset_allocation_cninfo_data(df)
                elif collection_name == 'fund_scale_change_em':
                    await self.save_fund_scale_change_em_data(df)
                elif collection_name == 'fund_hold_structure_em':
                    await self.save_fund_hold_structure_em_data(df)
                elif collection_name == 'fund_stock_position_lg':
                    await self.save_fund_stock_position_lg_data(df)
                elif collection_name == 'fund_balance_position_lg':
                    await self.save_fund_balance_position_lg_data(df)
                elif collection_name == 'fund_linghuo_position_lg':
                    await self.save_fund_linghuo_position_lg_data(df)
                elif collection_name == 'fund_announcement_dividend_em':
                    await self.save_fund_announcement_dividend_em_data(df)
                elif collection_name == 'fund_announcement_report_em':
                    await self.save_fund_announcement_report_em_data(df)
                elif collection_name == 'fund_announcement_personnel_em':
                    await self.save_fund_announcement_personnel_em_data(df)
                total_synced += len(batch)
                
            client.close()
            
            return {
                "synced_count": total_synced, 
                "remote_total": total_synced,
                "message": f"ÊàêÂäüÂêåÊ≠• {total_synced} Êù°Êï∞ÊçÆ"
            }
            
        except Exception as e:
            logger.error(f"ËøúÁ®ãÂêåÊ≠•Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_purchase_status_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ‰øùÂ≠òÂü∫ÈáëÁî≥Ë¥≠Áä∂ÊÄÅÊï∞ÊçÆÂà∞fund_purchase_statusÈõÜÂêà
        
        Args:
            df: ÂåÖÂê´Âü∫ÈáëÁî≥Ë¥≠Áä∂ÊÄÅÁöÑDataFrame
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞
            
        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÁî≥Ë¥≠Áä∂ÊÄÅÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            # Ê∏ÖÁêÜÊó†ÊïàÁöÑÊµÆÁÇπÊï∞ÂÄº
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÁî≥Ë¥≠Áä∂ÊÄÅÊï∞ÊçÆ...")
            
            # ÂàÜÊâπÂ§ÑÁêÜ
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"üì¶ Â∞ÜÂàÜ {total_batches} ÊâπÊ¨°Â§ÑÁêÜÔºåÊØèÊâπ {batch_size} Êù°")
            
            # Ëé∑ÂèñÂΩìÂâçÊó•Êúü‰Ωú‰∏∫Êï∞ÊçÆÊó•Êúü
            current_date = datetime.now().strftime('%Y-%m-%d')
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"üìù Â§ÑÁêÜÁ¨¨ {batch_idx + 1}/{total_batches} ÊâπÔºåËÆ∞ÂΩïËåÉÂõ¥: {start_idx + 1}-{end_idx}")
                
                # ÊûÑÂª∫ÊâπÈáèÊìç‰Ωú
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # Ê∏ÖÁêÜNaN/InfinityÂÄº
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        # ËΩ¨Êç¢ datetime.date ÂØπË±°‰∏∫Â≠óÁ¨¶‰∏≤
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        # ËΩ¨Êç¢ datetime.datetime ÂØπË±°‰∏∫Â≠óÁ¨¶‰∏≤
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ëé∑ÂèñÂü∫Èáë‰ª£Á†ÅÂíåÊä•ÂëäÊó∂Èó¥
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    report_time = str(doc.get('ÊúÄÊñ∞ÂáÄÂÄº/‰∏á‰ªΩÊî∂Áõä-Êä•ÂëäÊó∂Èó¥', current_date))
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    doc['code'] = fund_code
                    doc['date'] = report_time
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_purchase_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫Èáë‰ª£Á†ÅÂíåÊó•Êúü‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜÔºàÂ¶ÇÈúÄÊ±ÇÊâÄËø∞Ôºâ
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'date': report_time},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # ÊâßË°åÊâπÈáèÂÜôÂÖ•
                if ops:
                    result = await self.col_fund_purchase_status.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"‚úÖ Á¨¨ {batch_idx + 1}/{total_batches} ÊâπÂÜôÂÖ•ÂÆåÊàê: "
                        f"Êñ∞Â¢û={result.upserted_count}, Êõ¥Êñ∞={result.matched_count}, "
                        f"Êú¨Êâπ‰øùÂ≠ò={batch_saved}, Á¥ØËÆ°={total_saved}/{total_count}"
                    )
                    
                    # Ë∞ÉÁî®ËøõÂ∫¶ÂõûË∞É
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Âü∫ÈáëÁî≥Ë¥≠Áä∂ÊÄÅÊï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëÁî≥Ë¥≠Áä∂ÊÄÅÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÁî≥Ë¥≠Áä∂ÊÄÅÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_purchase_status_data(self) -> int:
        """
        Ê∏ÖÁ©∫Âü∫ÈáëÁî≥Ë¥≠Áä∂ÊÄÅÊï∞ÊçÆ
        
        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_purchase_status.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÁî≥Ë¥≠Áä∂ÊÄÅÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÁî≥Ë¥≠Áä∂ÊÄÅÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_purchase_status_stats(self) -> Dict[str, Any]:
        """
        Ëé∑ÂèñÂü∫ÈáëÁî≥Ë¥≠Áä∂ÊÄÅÁªüËÆ°‰ø°ÊÅØ
        
        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_count = await self.col_fund_purchase_status.count_documents({})
            
            # ÊåâÂü∫ÈáëÁ±ªÂûãÁªüËÆ°
            pipeline_type = [
                {
                    '$group': {
                        '_id': '$Âü∫ÈáëÁ±ªÂûã',
                        'count': {'$sum': 1}
                    }
                },
                {
                    '$sort': {'count': -1}
                }
            ]
            
            type_stats: List[Dict[str, Any]] = []
            async for doc in self.col_fund_purchase_status.aggregate(pipeline_type):
                type_stats.append({
                    'type': doc['_id'],
                    'count': doc['count']
                })
            
            # ÊåâÁî≥Ë¥≠Áä∂ÊÄÅÁªüËÆ°
            pipeline_purchase = [
                {
                    '$group': {
                        '_id': '$Áî≥Ë¥≠Áä∂ÊÄÅ',
                        'count': {'$sum': 1}
                    }
                },
                {
                    '$sort': {'count': -1}
                }
            ]
            
            purchase_status_stats: List[Dict[str, Any]] = []
            async for doc in self.col_fund_purchase_status.aggregate(pipeline_purchase):
                purchase_status_stats.append({
                    'status': doc['_id'],
                    'count': doc['count']
                })
            
            # ÊåâËµéÂõûÁä∂ÊÄÅÁªüËÆ°
            pipeline_redeem = [
                {
                    '$group': {
                        '_id': '$ËµéÂõûÁä∂ÊÄÅ',
                        'count': {'$sum': 1}
                    }
                },
                {
                    '$sort': {'count': -1}
                }
            ]
            
            redeem_status_stats: List[Dict[str, Any]] = []
            async for doc in self.col_fund_purchase_status.aggregate(pipeline_redeem):
                redeem_status_stats.append({
                    'status': doc['_id'],
                    'count': doc['count']
                })
            
            # ËÆ°ÁÆóÊúÄÊó©ÂíåÊúÄÊôöÊó•Êúü
            earliest_date = None
            latest_date = None
            pipeline_date = [
                {
                    '$group': {
                        '_id': None,
                        'earliest': {'$min': '$date'},
                        'latest': {'$max': '$date'}
                    }
                }
            ]
            
            async for doc in self.col_fund_purchase_status.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            return {
                'total_count': total_count,
                'type_stats': type_stats,
                'purchase_status_stats': purchase_status_stats,
                'redeem_status_stats': redeem_status_stats,
                'earliest_date': earliest_date,
                'latest_date': latest_date
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÁî≥Ë¥≠Áä∂ÊÄÅÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_etf_spot_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ‰øùÂ≠òETFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂà∞fund_etf_spot_emÈõÜÂêà
        
        Args:
            df: ÂåÖÂê´ETFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁöÑDataFrame
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞
            
        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâETFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            # Ê∏ÖÁêÜÊó†ÊïàÁöÑÊµÆÁÇπÊï∞ÂÄº
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°ETFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ...")
            
            # ÂàÜÊâπÂ§ÑÁêÜ
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"üì¶ Â∞ÜÂàÜ {total_batches} ÊâπÊ¨°Â§ÑÁêÜÔºåÊØèÊâπ {batch_size} Êù°")
            
            # Ëé∑ÂèñÂΩìÂâçÊó•Êúü‰Ωú‰∏∫Êï∞ÊçÆÊó•ÊúüÔºàÂ¶ÇÊûúÊï∞ÊçÆ‰∏≠Ê≤°ÊúâÊó•ÊúüÂ≠óÊÆµÔºâ
            current_date = datetime.now().strftime('%Y-%m-%d')
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"üìù Â§ÑÁêÜÁ¨¨ {batch_idx + 1}/{total_batches} ÊâπÔºåËÆ∞ÂΩïËåÉÂõ¥: {start_idx + 1}-{end_idx}")
                
                # ÊûÑÂª∫ÊâπÈáèÊìç‰Ωú
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # Ê∏ÖÁêÜNaN/InfinityÂÄºÂíåËΩ¨Êç¢Êó•ÊúüÁ±ªÂûã
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        # ËΩ¨Êç¢ datetime.date ÂØπË±°‰∏∫Â≠óÁ¨¶‰∏≤
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        # ËΩ¨Êç¢ datetime.datetime ÂØπË±°‰∏∫Â≠óÁ¨¶‰∏≤
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d %H:%M:%S')
                    
                    # Ëé∑ÂèñÂü∫Èáë‰ª£Á†ÅÂíåÊï∞ÊçÆÊó•Êúü
                    fund_code = str(doc.get('‰ª£Á†Å', ''))
                    data_date = str(doc.get('Êï∞ÊçÆÊó•Êúü', current_date))
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    doc['code'] = fund_code
                    doc['date'] = data_date
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_etf_spot_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫Èáë‰ª£Á†ÅÂíåÊó•Êúü‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'date': data_date},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # ÊâßË°åÊâπÈáèÂÜôÂÖ•
                if ops:
                    result = await self.col_fund_etf_spot.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"‚úÖ Á¨¨ {batch_idx + 1}/{total_batches} ÊâπÂÜôÂÖ•ÂÆåÊàê: "
                        f"Êñ∞Â¢û={result.upserted_count}, Êõ¥Êñ∞={result.matched_count}, "
                        f"Êú¨Êâπ‰øùÂ≠ò={batch_saved}, Á¥ØËÆ°={total_saved}/{total_count}"
                    )
                    
                    # Ë∞ÉÁî®ËøõÂ∫¶ÂõûË∞É
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°ETFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òETFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_etf_spot_data(self) -> int:
        """
        Ê∏ÖÁ©∫ETFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ
        
        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_etf_spot.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"üóëÔ∏è  Â∑≤Ê∏ÖÁ©∫ {deleted_count} Êù°ETFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ETFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_etf_spot_stats(self) -> Dict[str, Any]:
        """
        Ëé∑ÂèñETFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°‰ø°ÊÅØ
        
        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_count = await self.col_fund_etf_spot.count_documents({})
            
            # ÁªüËÆ°Ê∂®Ë∑åÊï∞Èáè
            rise_count = await self.col_fund_etf_spot.count_documents({'Ê∂®Ë∑åÂπÖ': {'$gt': 0}})
            fall_count = await self.col_fund_etf_spot.count_documents({'Ê∂®Ë∑åÂπÖ': {'$lt': 0}})
            flat_count = total_count - rise_count - fall_count
            
            # ÁªüËÆ°Êàê‰∫§È¢ùTOP10
            pipeline_volume = [
                {
                    '$sort': {'Êàê‰∫§È¢ù': -1}
                },
                {
                    '$limit': 10
                },
                {
                    '$project': {
                        'name': '$ÂêçÁß∞',
                        'code': '$‰ª£Á†Å',
                        'volume': '$Êàê‰∫§È¢ù',
                        'price': '$ÊúÄÊñ∞‰ª∑',
                        'change_pct': '$Ê∂®Ë∑åÂπÖ'
                    }
                }
            ]
            
            top_volume: List[Dict[str, Any]] = []
            async for doc in self.col_fund_etf_spot.aggregate(pipeline_volume):
                top_volume.append({
                    'name': doc.get('name'),
                    'code': doc.get('code'),
                    'volume': doc.get('volume'),
                    'price': doc.get('price'),
                    'change_pct': doc.get('change_pct')
                })
            
            # ÁªüËÆ°Ê∂®Ë∑åÂπÖTOP10
            pipeline_rise = [
                {
                    '$sort': {'Ê∂®Ë∑åÂπÖ': -1}
                },
                {
                    '$limit': 10
                },
                {
                    '$project': {
                        'name': '$ÂêçÁß∞',
                        'code': '$‰ª£Á†Å',
                        'change_pct': '$Ê∂®Ë∑åÂπÖ',
                        'price': '$ÊúÄÊñ∞‰ª∑',
                        'volume': '$Êàê‰∫§È¢ù'
                    }
                }
            ]
            
            top_gainers: List[Dict[str, Any]] = []
            async for doc in self.col_fund_etf_spot.aggregate(pipeline_rise):
                top_gainers.append({
                    'name': doc.get('name'),
                    'code': doc.get('code'),
                    'change_pct': doc.get('change_pct'),
                    'price': doc.get('price'),
                    'volume': doc.get('volume')
                })
            
            # ËÆ°ÁÆóÊúÄÊñ∞Êó•Êúü
            pipeline_date = [
                {
                    '$group': {
                        '_id': None,
                        'latest': {'$max': '$date'}
                    }
                }
            ]
            
            latest_date = None
            async for doc in self.col_fund_etf_spot.aggregate(pipeline_date):
                latest_date = doc.get('latest')
            
            # ÁªüËÆ°Âü∫ÈáëÁ±ªÂûãÂàÜÂ∏ÉÔºàÂü∫‰∫éÂêçÁß∞ÂÖ≥ÈîÆËØçÂàÜÁ±ªÔºâ
            type_keywords = {
                'Ë°å‰∏öETF': ['ËäØÁâá', 'ÂçäÂØº‰Ωì', 'ÂåªËçØ', 'Ê∂àË¥π', 'ÈáëËûç', 'Âú∞‰∫ß', 'ËÉΩÊ∫ê', 'ÂåñÂ∑•', 'ÂÜõÂ∑•', 'Ê±ΩËΩ¶', 'ÈÄö‰ø°', '‰º†Â™í', 'ÁîµÂ≠ê', 'ËÆ°ÁÆóÊú∫', 'Êú∫Ê¢∞', 'ÁîµÊ∞î', 'Âª∫Á≠ë', 'Èí¢ÈìÅ', 'ÊúâËâ≤', 'ÁÖ§ÁÇ≠', 'Áü≥Ê≤π', 'Èì∂Ë°å', 'ËØÅÂà∏', '‰øùÈô©'],
                'ÂÆΩÂü∫ETF': ['Ê≤™Ê∑±300', '‰∏≠ËØÅ500', 'Âàõ‰∏öÊùø', 'ÁßëÂàõ50', '‰∏äËØÅ50', '‰∏≠ËØÅ1000', 'Á∫¢Âà©', '‰ª∑ÂÄº', 'ÊàêÈïø', 'Ë¥®Èáè', '‰ΩéÊ≥¢'],
                '‰∏ªÈ¢òETF': ['Êñ∞ËÉΩÊ∫ê', 'ÁßëÊäÄ', 'Á¢≥‰∏≠Âíå', 'Êï∞Â≠óÁªèÊµé', 'Â§ßÊï∞ÊçÆ', '‰∫∫Â∑•Êô∫ËÉΩ', '5G', 'Áâ©ËÅîÁΩë', '‰∫ëËÆ°ÁÆó', 'Êô∫ËÉΩ', 'ÂàõÊñ∞', 'ËΩ¨Âûã'],
                'Ë°å‰∏öÊåáÊï∞ETF': ['ËØÅÂà∏ÂÖ¨Âè∏', 'ÈùûÈì∂ÈáëËûç', 'ÊàøÂú∞‰∫ß', 'ÂõΩÈò≤ÂÜõÂ∑•', 'È£üÂìÅÈ•ÆÊñô', 'ÂÆ∂Áî®ÁîµÂô®', 'Á∫∫ÁªáÊúçË£Ö', 'ÂÜúÊûóÁâßÊ∏î'],
                'Ê∏ØËÇ°ETF': ['Ê∏ØËÇ°', 'ÊÅíÁîü', 'È¶ôÊ∏Ø', 'HËÇ°', 'HKEX'],
                'ÂÄ∫Âà∏ETF': ['ÂÄ∫', 'ÂõΩÂÄ∫', 'Âú∞ÊñπÂÄ∫', '‰ºÅ‰∏öÂÄ∫', 'ÂèØËΩ¨ÂÄ∫', '‰ø°Áî®ÂÄ∫'],
                'ÂïÜÂìÅETF': ['ÈªÑÈáë', 'ÁôΩÈì∂', 'ÂéüÊ≤π', 'ÂïÜÂìÅ', 'ÊúâËâ≤ÈáëÂ±û', 'Ë¥µÈáëÂ±û'],
                'Ë∑®Â¢ÉETF': ['ÁæéËÇ°', 'Á∫≥ÊñØËææÂÖã', 'Ê†áÊôÆ', 'Âæ∑ÂõΩ', 'Ê≥ïÂõΩ', 'Êó•Êú¨', 'Âç∞Â∫¶', 'Ë∂äÂçó', 'ÂÖ®ÁêÉ'],
            }
            
            type_counts: Dict[str, int] = {}
            
            # Ëé∑ÂèñÊâÄÊúâÂü∫ÈáëÂêçÁß∞Âπ∂ÂàÜÁ±ª
            async for doc in self.col_fund_etf_spot.find({}, {'ÂêçÁß∞': 1}):
                name = doc.get('ÂêçÁß∞', '')
                classified = False
                
                # ÊåâÂÖ≥ÈîÆËØçÂåπÈÖçÁ±ªÂûã
                for fund_type, keywords in type_keywords.items():
                    if any(keyword in name for keyword in keywords):
                        type_counts[fund_type] = type_counts.get(fund_type, 0) + 1
                        classified = True
                        break
                
                # Êú™ÂåπÈÖçÁöÑÂΩí‰∏∫ÂÖ∂‰ªñÁ±ªÂûã
                if not classified:
                    type_counts['ÂÖ∂‰ªñETF'] = type_counts.get('ÂÖ∂‰ªñETF', 0) + 1
            
            # ËΩ¨Êç¢‰∏∫ÂàóË°®Ê†ºÂºè
            type_stats = [
                {'type': fund_type, 'count': count}
                for fund_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True)
            ]
            
            return {
                'total_count': total_count,
                'rise_count': rise_count,
                'fall_count': fall_count,
                'flat_count': flat_count,
                'top_volume': top_volume,
                'top_gainers': top_gainers,
                'latest_date': latest_date,
                'type_stats': type_stats
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñETFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_etf_spot_ths_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ‰øùÂ≠òÂêåËä±È°∫ETFÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂà∞MongoDB
        
        Args:
            df: ÂåÖÂê´ETFÂÆûÊó∂Ë°åÊÉÖÁöÑDataFrame
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞(current, total, percentage, message)
            
        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("DataFrame‰∏∫Á©∫ÔºåÊó†Êï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            # Ê∏ÖÁêÜÊï∞ÊçÆÔºöÊõøÊç¢Êó†ÊïàÂÄº
            df = df.replace([float('inf'), float('-inf')], None)
            df = df.where(pd.notna(df), None)
            
            # ÂáÜÂ§áÊâπÈáèÊìç‰Ωú
            ops = []
            total_count = len(df)
            batch_size = 500
            
            for idx, row in df.iterrows():
                # Ëé∑ÂèñÂü∫Èáë‰ª£Á†ÅÂíåÊü•ËØ¢Êó•Êúü‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                fund_code = str(row['Âü∫Èáë‰ª£Á†Å']).strip()
                query_date = str(row['Êü•ËØ¢Êó•Êúü']).strip() if pd.notna(row.get('Êü•ËØ¢Êó•Êúü')) else ''
                
                if not fund_code or not query_date:
                    continue
                
                # ÊûÑÂª∫ÊñáÊ°£
                doc = {
                    'Â∫èÂè∑': int(row['Â∫èÂè∑']) if pd.notna(row.get('Â∫èÂè∑')) else None,
                    'Âü∫Èáë‰ª£Á†Å': fund_code,
                    'Âü∫ÈáëÂêçÁß∞': str(row['Âü∫ÈáëÂêçÁß∞']).strip() if pd.notna(row.get('Âü∫ÈáëÂêçÁß∞')) else '',
                    'ÂΩìÂâç-Âçï‰ΩçÂáÄÂÄº': float(row['ÂΩìÂâç-Âçï‰ΩçÂáÄÂÄº']) if pd.notna(row.get('ÂΩìÂâç-Âçï‰ΩçÂáÄÂÄº')) else None,
                    'ÂΩìÂâç-Á¥ØËÆ°ÂáÄÂÄº': float(row['ÂΩìÂâç-Á¥ØËÆ°ÂáÄÂÄº']) if pd.notna(row.get('ÂΩìÂâç-Á¥ØËÆ°ÂáÄÂÄº')) else None,
                    'Ââç‰∏ÄÊó•-Âçï‰ΩçÂáÄÂÄº': float(row['Ââç‰∏ÄÊó•-Âçï‰ΩçÂáÄÂÄº']) if pd.notna(row.get('Ââç‰∏ÄÊó•-Âçï‰ΩçÂáÄÂÄº')) else None,
                    'Ââç‰∏ÄÊó•-Á¥ØËÆ°ÂáÄÂÄº': float(row['Ââç‰∏ÄÊó•-Á¥ØËÆ°ÂáÄÂÄº']) if pd.notna(row.get('Ââç‰∏ÄÊó•-Á¥ØËÆ°ÂáÄÂÄº')) else None,
                    'Â¢ûÈïøÂÄº': float(row['Â¢ûÈïøÂÄº']) if pd.notna(row.get('Â¢ûÈïøÂÄº')) else None,
                    'Â¢ûÈïøÁéá': float(row['Â¢ûÈïøÁéá']) if pd.notna(row.get('Â¢ûÈïøÁéá')) else None,
                    'ËµéÂõûÁä∂ÊÄÅ': str(row['ËµéÂõûÁä∂ÊÄÅ']).strip() if pd.notna(row.get('ËµéÂõûÁä∂ÊÄÅ')) else '',
                    'Áî≥Ë¥≠Áä∂ÊÄÅ': str(row['Áî≥Ë¥≠Áä∂ÊÄÅ']).strip() if pd.notna(row.get('Áî≥Ë¥≠Áä∂ÊÄÅ')) else '',
                    'ÊúÄÊñ∞-‰∫§ÊòìÊó•': str(row['ÊúÄÊñ∞-‰∫§ÊòìÊó•']).strip() if pd.notna(row.get('ÊúÄÊñ∞-‰∫§ÊòìÊó•')) else '',
                    'ÊúÄÊñ∞-Âçï‰ΩçÂáÄÂÄº': float(row['ÊúÄÊñ∞-Âçï‰ΩçÂáÄÂÄº']) if pd.notna(row.get('ÊúÄÊñ∞-Âçï‰ΩçÂáÄÂÄº')) else None,
                    'ÊúÄÊñ∞-Á¥ØËÆ°ÂáÄÂÄº': float(row['ÊúÄÊñ∞-Á¥ØËÆ°ÂáÄÂÄº']) if pd.notna(row.get('ÊúÄÊñ∞-Á¥ØËÆ°ÂáÄÂÄº')) else None,
                    'Âü∫ÈáëÁ±ªÂûã': str(row['Âü∫ÈáëÁ±ªÂûã']).strip() if pd.notna(row.get('Âü∫ÈáëÁ±ªÂûã')) else '',
                    'Êü•ËØ¢Êó•Êúü': query_date,
                    'code': fund_code,
                    'date': query_date,
                    'source': 'akshare',
                    'endpoint': 'fund_etf_spot_ths',
                    'updated_at': datetime.now()
                }
                
                # Â§ÑÁêÜÊó•ÊúüÁ±ªÂûãÂ≠óÊÆµ
                for field in ['Êü•ËØ¢Êó•Êúü', 'ÊúÄÊñ∞-‰∫§ÊòìÊó•', 'date']:
                    if field in doc and doc[field] and isinstance(doc[field], (date, datetime)):
                        doc[field] = doc[field].isoformat() if hasattr(doc[field], 'isoformat') else str(doc[field])
                
                # Ê∑ªÂä†Âà∞ÊâπÈáèÊìç‰Ωú
                ops.append(
                    UpdateOne(
                        {'code': fund_code, 'date': query_date},
                        {'$set': doc},
                        upsert=True
                    )
                )
                
                # ÊâπÈáèÊâßË°å
                if len(ops) >= batch_size:
                    result = await self.col_fund_etf_spot_ths.bulk_write(ops, ordered=False)
                    
                    if progress_callback:
                        current = idx + 1
                        percentage = int((current / total_count) * 100)
                        progress_callback(current, total_count, percentage, f"Â∑≤‰øùÂ≠ò {current}/{total_count} Êù°Êï∞ÊçÆ")
                    
                    ops = []
            
            # ÊâßË°åÂâ©‰ΩôÊìç‰Ωú
            saved_count = 0
            if ops:
                result = await self.col_fund_etf_spot_ths.bulk_write(ops, ordered=False)
                saved_count = result.upserted_count + result.modified_count
            
            if progress_callback:
                progress_callback(total_count, total_count, 100, f"ÂÆåÊàêÔºÅÂÖ±‰øùÂ≠ò {total_count} Êù°Êï∞ÊçÆ")
            
            logger.info(f"ÊàêÂäü‰øùÂ≠ò {total_count} Êù°ÂêåËä±È°∫ETFÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return total_count
            
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂêåËä±È°∫ETFÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_etf_spot_ths_data(self) -> int:
        """
        Ê∏ÖÁ©∫ÂêåËä±È°∫ETFÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ
        
        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_etf_spot_ths.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÂêåËä±È°∫ETFÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÂêåËä±È°∫ETFÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_etf_spot_ths_stats(self) -> Dict[str, Any]:
        """
        Ëé∑ÂèñÂêåËä±È°∫ETFÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°‰ø°ÊÅØ
        
        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            # ÊÄªËÆ∞ÂΩïÊï∞
            total_count = await self.col_fund_etf_spot_ths.count_documents({})
            
            # Ê∂®Ë∑åÁªüËÆ°
            rise_count = await self.col_fund_etf_spot_ths.count_documents({'Â¢ûÈïøÁéá': {'$gt': 0}})
            fall_count = await self.col_fund_etf_spot_ths.count_documents({'Â¢ûÈïøÁéá': {'$lt': 0}})
            flat_count = await self.col_fund_etf_spot_ths.count_documents({'Â¢ûÈïøÁéá': 0})
            
            # Âü∫ÈáëÁ±ªÂûãÂàÜÂ∏É
            type_pipeline = [
                {'$group': {'_id': '$Âü∫ÈáëÁ±ªÂûã', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            type_stats = []
            async for doc in self.col_fund_etf_spot_ths.aggregate(type_pipeline):
                if doc['_id']:
                    type_stats.append({
                        'type': doc['_id'],
                        'count': doc['count']
                    })
            
            # Ê∂®ÂπÖTOP10
            top_gainers = []
            cursor = self.col_fund_etf_spot_ths.find(
                {'Â¢ûÈïøÁéá': {'$ne': None, '$gt': 0}},
                {'Âü∫Èáë‰ª£Á†Å': 1, 'Âü∫ÈáëÂêçÁß∞': 1, 'Â¢ûÈïøÁéá': 1, '_id': 0}
            ).sort('Â¢ûÈïøÁéá', -1).limit(10)
            
            async for doc in cursor:
                top_gainers.append({
                    'code': doc.get('Âü∫Èáë‰ª£Á†Å'),
                    'name': doc.get('Âü∫ÈáëÂêçÁß∞'),
                    'rate': doc.get('Â¢ûÈïøÁéá')
                })
            
            # Ë∑åÂπÖTOP10
            top_losers = []
            cursor = self.col_fund_etf_spot_ths.find(
                {'Â¢ûÈïøÁéá': {'$ne': None, '$lt': 0}},
                {'Âü∫Èáë‰ª£Á†Å': 1, 'Âü∫ÈáëÂêçÁß∞': 1, 'Â¢ûÈïøÁéá': 1, '_id': 0}
            ).sort('Â¢ûÈïøÁéá', 1).limit(10)
            
            async for doc in cursor:
                top_losers.append({
                    'code': doc.get('Âü∫Èáë‰ª£Á†Å'),
                    'name': doc.get('Âü∫ÈáëÂêçÁß∞'),
                    'rate': doc.get('Â¢ûÈïøÁéá')
                })
            
            # Áî≥ËµéÁä∂ÊÄÅÁªüËÆ°
            purchase_pipeline = [
                {'$group': {'_id': '$Áî≥Ë¥≠Áä∂ÊÄÅ', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            purchase_stats = []
            async for doc in self.col_fund_etf_spot_ths.aggregate(purchase_pipeline):
                if doc['_id']:
                    purchase_stats.append({
                        'status': doc['_id'],
                        'count': doc['count']
                    })
            
            redeem_pipeline = [
                {'$group': {'_id': '$ËµéÂõûÁä∂ÊÄÅ', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            redeem_stats = []
            async for doc in self.col_fund_etf_spot_ths.aggregate(redeem_pipeline):
                if doc['_id']:
                    redeem_stats.append({
                        'status': doc['_id'],
                        'count': doc['count']
                    })
            
            # ÊúÄÊñ∞Êó•Êúü
            latest_date = None
            cursor = self.col_fund_etf_spot_ths.find(
                {'Êü•ËØ¢Êó•Êúü': {'$ne': None}},
                {'Êü•ËØ¢Êó•Êúü': 1, '_id': 0}
            ).sort('Êü•ËØ¢Êó•Êúü', -1).limit(1)
            
            async for doc in cursor:
                latest_date = doc.get('Êü•ËØ¢Êó•Êúü')
            
            return {
                'total_count': total_count,
                'rise_count': rise_count,
                'fall_count': fall_count,
                'flat_count': flat_count,
                'type_stats': type_stats,
                'top_gainers': top_gainers,
                'top_losers': top_losers,
                'purchase_status_stats': purchase_stats,
                'redeem_status_stats': redeem_stats,
                'latest_date': latest_date
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂêåËä±È°∫ETFÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_lof_spot_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ‰øùÂ≠òLOFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂà∞MongoDB
        
        Args:
            df: ÂåÖÂê´LOFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁöÑDataFrame
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞(current, total, percentage, message)
            
        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("DataFrame‰∏∫Á©∫ÔºåÊó†Êï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            # Ê∏ÖÁêÜÊï∞ÊçÆÔºöÊõøÊç¢Êó†ÊïàÂÄº
            df = df.replace([float('inf'), float('-inf')], None)
            df = df.where(pd.notna(df), None)
            
            # ÂáÜÂ§áÊâπÈáèÊìç‰Ωú
            ops = []
            total_count = len(df)
            batch_size = 500
            
            # Ê∑ªÂä†Êï∞ÊçÆÊó•Êúü
            data_date = datetime.now().strftime('%Y-%m-%d')
            
            for idx, row in df.iterrows():
                # Ëé∑ÂèñÂü∫Èáë‰ª£Á†Å‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                fund_code = str(row['‰ª£Á†Å']).strip()
                
                if not fund_code:
                    continue
                
                # ÊûÑÂª∫ÊñáÊ°£
                doc = {
                    '‰ª£Á†Å': fund_code,
                    'ÂêçÁß∞': str(row['ÂêçÁß∞']).strip() if pd.notna(row.get('ÂêçÁß∞')) else '',
                    'ÊúÄÊñ∞‰ª∑': float(row['ÊúÄÊñ∞‰ª∑']) if pd.notna(row.get('ÊúÄÊñ∞‰ª∑')) else None,
                    'Ê∂®Ë∑åÈ¢ù': float(row['Ê∂®Ë∑åÈ¢ù']) if pd.notna(row.get('Ê∂®Ë∑åÈ¢ù')) else None,
                    'Ê∂®Ë∑åÂπÖ': float(row['Ê∂®Ë∑åÂπÖ']) if pd.notna(row.get('Ê∂®Ë∑åÂπÖ')) else None,
                    'Êàê‰∫§Èáè': float(row['Êàê‰∫§Èáè']) if pd.notna(row.get('Êàê‰∫§Èáè')) else None,
                    'Êàê‰∫§È¢ù': float(row['Êàê‰∫§È¢ù']) if pd.notna(row.get('Êàê‰∫§È¢ù')) else None,
                    'ÂºÄÁõò‰ª∑': float(row['ÂºÄÁõò‰ª∑']) if pd.notna(row.get('ÂºÄÁõò‰ª∑')) else None,
                    'ÊúÄÈ´ò‰ª∑': float(row['ÊúÄÈ´ò‰ª∑']) if pd.notna(row.get('ÊúÄÈ´ò‰ª∑')) else None,
                    'ÊúÄ‰Ωé‰ª∑': float(row['ÊúÄ‰Ωé‰ª∑']) if pd.notna(row.get('ÊúÄ‰Ωé‰ª∑')) else None,
                    'Êò®Êî∂': float(row['Êò®Êî∂']) if pd.notna(row.get('Êò®Êî∂')) else None,
                    'Êç¢ÊâãÁéá': float(row['Êç¢ÊâãÁéá']) if pd.notna(row.get('Êç¢ÊâãÁéá')) else None,
                    'ÊµÅÈÄöÂ∏ÇÂÄº': int(row['ÊµÅÈÄöÂ∏ÇÂÄº']) if pd.notna(row.get('ÊµÅÈÄöÂ∏ÇÂÄº')) else None,
                    'ÊÄªÂ∏ÇÂÄº': int(row['ÊÄªÂ∏ÇÂÄº']) if pd.notna(row.get('ÊÄªÂ∏ÇÂÄº')) else None,
                    'Êï∞ÊçÆÊó•Êúü': data_date,
                    'code': fund_code,
                    'date': data_date,
                    'source': 'akshare',
                    'endpoint': 'fund_lof_spot_em',
                    'updated_at': datetime.now()
                }
                
                # Ê∑ªÂä†Âà∞ÊâπÈáèÊìç‰Ωú
                ops.append(
                    UpdateOne(
                        {'code': fund_code, 'date': data_date},
                        {'$set': doc},
                        upsert=True
                    )
                )
                
                # ÊâπÈáèÊâßË°å
                if len(ops) >= batch_size:
                    result = await self.col_fund_lof_spot.bulk_write(ops, ordered=False)
                    
                    if progress_callback:
                        current = idx + 1
                        percentage = int((current / total_count) * 100)
                        progress_callback(current, total_count, percentage, f"Â∑≤‰øùÂ≠ò {current}/{total_count} Êù°Êï∞ÊçÆ")
                    
                    ops = []
            
            # ÊâßË°åÂâ©‰ΩôÊìç‰Ωú
            saved_count = 0
            if ops:
                result = await self.col_fund_lof_spot.bulk_write(ops, ordered=False)
                saved_count = result.upserted_count + result.modified_count
            
            if progress_callback:
                progress_callback(total_count, total_count, 100, f"ÂÆåÊàêÔºÅÂÖ±‰øùÂ≠ò {total_count} Êù°Êï∞ÊçÆ")
            
            logger.info(f"ÊàêÂäü‰øùÂ≠ò {total_count} Êù°LOFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return total_count
            
        except Exception as e:
            logger.error(f"‰øùÂ≠òLOFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_lof_spot_data(self) -> int:
        """
        Ê∏ÖÁ©∫LOFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ
        
        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_lof_spot.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°LOFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫LOFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_lof_spot_stats(self) -> Dict[str, Any]:
        """
        Ëé∑ÂèñLOFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°‰ø°ÊÅØ
        
        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            # ÊÄªËÆ∞ÂΩïÊï∞
            total_count = await self.col_fund_lof_spot.count_documents({})
            
            # Ê∂®Ë∑åÁªüËÆ°
            rise_count = await self.col_fund_lof_spot.count_documents({'Ê∂®Ë∑åÂπÖ': {'$gt': 0}})
            fall_count = await self.col_fund_lof_spot.count_documents({'Ê∂®Ë∑åÂπÖ': {'$lt': 0}})
            flat_count = await self.col_fund_lof_spot.count_documents({'Ê∂®Ë∑åÂπÖ': 0})
            
            # Êàê‰∫§È¢ùTOP10
            top_volume = []
            cursor = self.col_fund_lof_spot.find(
                {'Êàê‰∫§È¢ù': {'$ne': None}},
                {'‰ª£Á†Å': 1, 'ÂêçÁß∞': 1, 'Êàê‰∫§È¢ù': 1, '_id': 0}
            ).sort('Êàê‰∫§È¢ù', -1).limit(10)
            
            async for doc in cursor:
                top_volume.append({
                    'code': doc.get('‰ª£Á†Å'),
                    'name': doc.get('ÂêçÁß∞'),
                    'amount': doc.get('Êàê‰∫§È¢ù')
                })
            
            # Ê∂®ÂπÖTOP10
            top_gainers = []
            cursor = self.col_fund_lof_spot.find(
                {'Ê∂®Ë∑åÂπÖ': {'$ne': None, '$gt': 0}},
                {'‰ª£Á†Å': 1, 'ÂêçÁß∞': 1, 'Ê∂®Ë∑åÂπÖ': 1, '_id': 0}
            ).sort('Ê∂®Ë∑åÂπÖ', -1).limit(10)
            
            async for doc in cursor:
                top_gainers.append({
                    'code': doc.get('‰ª£Á†Å'),
                    'name': doc.get('ÂêçÁß∞'),
                    'rate': doc.get('Ê∂®Ë∑åÂπÖ')
                })
            
            # Ë∑åÂπÖTOP10
            top_losers = []
            cursor = self.col_fund_lof_spot.find(
                {'Ê∂®Ë∑åÂπÖ': {'$ne': None, '$lt': 0}},
                {'‰ª£Á†Å': 1, 'ÂêçÁß∞': 1, 'Ê∂®Ë∑åÂπÖ': 1, '_id': 0}
            ).sort('Ê∂®Ë∑åÂπÖ', 1).limit(10)
            
            async for doc in cursor:
                top_losers.append({
                    'code': doc.get('‰ª£Á†Å'),
                    'name': doc.get('ÂêçÁß∞'),
                    'rate': doc.get('Ê∂®Ë∑åÂπÖ')
                })
            
            # Â∏ÇÂÄºÂàÜÂ∏ÉÁªüËÆ°ÔºàÊåâÂ∏ÇÂÄºËåÉÂõ¥ÂàÜÁªÑÔºâ
            market_cap_ranges = [
                {'name': '10‰∫ø‰ª•‰∏ã', 'min': 0, 'max': 1000000000},
                {'name': '10-50‰∫ø', 'min': 1000000000, 'max': 5000000000},
                {'name': '50-100‰∫ø', 'min': 5000000000, 'max': 10000000000},
                {'name': '100‰∫ø‰ª•‰∏ä', 'min': 10000000000, 'max': float('inf')}
            ]
            
            market_cap_stats = []
            for range_item in market_cap_ranges:
                count = await self.col_fund_lof_spot.count_documents({
                    'ÊÄªÂ∏ÇÂÄº': {
                        '$gte': range_item['min'],
                        '$lt': range_item['max'] if range_item['max'] != float('inf') else 999999999999
                    }
                })
                if count > 0:
                    market_cap_stats.append({
                        'range': range_item['name'],
                        'count': count
                    })
            
            # ÊúÄÊñ∞Êó•Êúü
            latest_date = None
            cursor = self.col_fund_lof_spot.find(
                {'Êï∞ÊçÆÊó•Êúü': {'$ne': None}},
                {'Êï∞ÊçÆÊó•Êúü': 1, '_id': 0}
            ).sort('Êï∞ÊçÆÊó•Êúü', -1).limit(1)
            
            async for doc in cursor:
                latest_date = doc.get('Êï∞ÊçÆÊó•Êúü')
            
            return {
                'total_count': total_count,
                'rise_count': rise_count,
                'fall_count': fall_count,
                'flat_count': flat_count,
                'top_volume': top_volume,
                'top_gainers': top_gainers,
                'top_losers': top_losers,
                'market_cap_stats': market_cap_stats,
                'latest_date': latest_date
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñLOFÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_spot_sina_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖ-Êñ∞Êµ™Êï∞ÊçÆÂà∞ MongoDB„ÄÇ

        ‰ΩøÁî® `code + date` ‰Ωú‰∏∫ÂîØ‰∏ÄÈîÆËøõË°å upsertÔºåÁªìÊûÑ‰∏é LOF ÂÆûÊó∂Ë°åÊÉÖ‰øùÊåÅ‰∏ÄËá¥Ôºå
        Âπ∂ÈÄöËøá `UpdateOne` ÊûÑÈÄ†ÂêàÊ≥ïÁöÑ bulk_write ËØ∑Ê±Ç„ÄÇ

        Args:
            df: ÂåÖÂê´Âü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁöÑ DataFrame
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞ (current, total, percentage, message)

        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("Âü∫ÈáëÂÆûÊó∂Ë°åÊÉÖ-Êñ∞Êµ™Êï∞ÊçÆ‰∏∫Á©∫")
            return 0

        try:
            logger.info(f"ÂºÄÂßã‰øùÂ≠òÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖ-Êñ∞Êµ™Êï∞ÊçÆÔºåÂÖ± {len(df)} Êù°")

            # Êã∑Ë¥ù‰∏Ä‰ªΩÔºåÈÅøÂÖç‰øÆÊîπÂéüÂßãÊï∞ÊçÆ
            df = df.copy()

            # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
            current_date = datetime.now().strftime("%Y-%m-%d")
            df["Êï∞ÊçÆÊó•Êúü"] = current_date
            # ÈÉ®ÂàÜÂú∫ÊôØ‰∏ãÂèØËÉΩÊ≤°Êúâ code ÂàóÔºåËøôÈáåÊòæÂºè‰ªé ‰ª£Á†Å Ë°çÁîü
            if "code" not in df.columns:
                df["code"] = df["‰ª£Á†Å"].astype(str)
            df["date"] = current_date
            df["source"] = "akshare"
            df["endpoint"] = "fund_etf_category_sina"
            df["updated_at"] = datetime.now()

            # Ê∏ÖÁêÜÊï∞ÊçÆÔºöÂ∞Ü NaN Âíå Infinity ÊõøÊç¢‰∏∫ NoneÔºåÈÅøÂÖç JSON Â∫èÂàóÂåñÈóÆÈ¢ò
            df = df.replace([float("inf"), float("-inf")], None)
            df = df.where(pd.notnull(df), None)

            batch_size = 500
            total_count = len(df)
            total_saved = 0

            for batch_start in range(0, total_count, batch_size):
                batch_end = min(batch_start + batch_size, total_count)
                batch_df = df.iloc[batch_start:batch_end]

                ops = []
                for _, row in batch_df.iterrows():
                    record = row.to_dict()

                    code = str(record.get("code") or record.get("‰ª£Á†Å") or "").strip()
                    if not code:
                        continue

                    date_value = record.get("date") or current_date
                    record["code"] = code
                    record["date"] = date_value

                    ops.append(
                        UpdateOne(
                            {"code": code, "date": date_value},
                            {"$set": record},
                            upsert=True,
                        )
                    )

                if ops:
                    result = await self.col_fund_spot_sina.bulk_write(ops, ordered=False)
                    saved_count = (result.upserted_count or 0) + (result.modified_count or 0)
                    total_saved += saved_count

                # Êõ¥Êñ∞ËøõÂ∫¶
                if progress_callback:
                    current = batch_end
                    percentage = int((current / total_count) * 100)
                    progress_callback(current, total_count, percentage, f"Â∑≤‰øùÂ≠ò {current}/{total_count} Êù°Êï∞ÊçÆ")

            logger.info(f"Âü∫ÈáëÂÆûÊó∂Ë°åÊÉÖ-Êñ∞Êµ™Êï∞ÊçÆ‰øùÂ≠òÂÆåÊàêÔºåÂÖ±‰øùÂ≠ò {total_saved} Êù°")
            return total_saved

        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖ-Êñ∞Êµ™Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_spot_sina_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëÂÆûÊó∂Ë°åÊÉÖ-Êñ∞Êµ™Êï∞ÊçÆ"""
        try:
            result = await self.col_fund_spot_sina.delete_many({})
            logger.info(f"Ê∏ÖÁ©∫Âü∫ÈáëÂÆûÊó∂Ë°åÊÉÖ-Êñ∞Êµ™Êï∞ÊçÆÊàêÂäüÔºåÂà†Èô§ {result.deleted_count} Êù°")
            return result.deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÂÆûÊó∂Ë°åÊÉÖ-Êñ∞Êµ™Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_spot_sina_stats(self) -> dict:
        """
        Ëé∑ÂèñÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖ-Êñ∞Êµ™ÁªüËÆ°‰ø°ÊÅØ
        
        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            # ÊÄªÊï∞
            total_count = await self.col_fund_spot_sina.count_documents({})
            
            # Ê∂®Ë∑åÁªüËÆ°
            rise_count = await self.col_fund_spot_sina.count_documents({
                "Ê∂®Ë∑åÂπÖ": {"$gt": 0}
            })
            fall_count = await self.col_fund_spot_sina.count_documents({
                "Ê∂®Ë∑åÂπÖ": {"$lt": 0}
            })
            flat_count = await self.col_fund_spot_sina.count_documents({
                "Ê∂®Ë∑åÂπÖ": 0
            })
            
            # Âü∫ÈáëÁ±ªÂûãÂàÜÂ∏ÉÁªüËÆ°
            type_pipeline = [
                {
                    "$group": {
                        "_id": "$Âü∫ÈáëÁ±ªÂûã",
                        "count": {"$sum": 1}
                    }
                },
                {
                    "$project": {
                        "_id": 0,
                        "type": "$_id",
                        "count": 1
                    }
                },
                {"$sort": {"count": -1}}
            ]
            type_stats = await self.col_fund_spot_sina.aggregate(type_pipeline).to_list(None)
            
            # Êàê‰∫§È¢ùTOP10
            volume_pipeline = [
                {"$match": {"Êàê‰∫§È¢ù": {"$ne": None}}},
                {"$sort": {"Êàê‰∫§È¢ù": -1}},
                {"$limit": 10},
                {
                    "$project": {
                        "_id": 0,
                        "code": "$‰ª£Á†Å",
                        "name": "$ÂêçÁß∞",
                        "amount": "$Êàê‰∫§È¢ù",
                        "type": "$Âü∫ÈáëÁ±ªÂûã"
                    }
                }
            ]
            top_volume = await self.col_fund_spot_sina.aggregate(volume_pipeline).to_list(None)
            
            # Ê∂®ÂπÖTOP10
            gainers_pipeline = [
                {"$match": {"Ê∂®Ë∑åÂπÖ": {"$ne": None, "$gt": 0}}},
                {"$sort": {"Ê∂®Ë∑åÂπÖ": -1}},
                {"$limit": 10},
                {
                    "$project": {
                        "_id": 0,
                        "code": "$‰ª£Á†Å",
                        "name": "$ÂêçÁß∞",
                        "rate": "$Ê∂®Ë∑åÂπÖ",
                        "type": "$Âü∫ÈáëÁ±ªÂûã"
                    }
                }
            ]
            top_gainers = await self.col_fund_spot_sina.aggregate(gainers_pipeline).to_list(None)
            
            # Ë∑åÂπÖTOP10
            losers_pipeline = [
                {"$match": {"Ê∂®Ë∑åÂπÖ": {"$ne": None, "$lt": 0}}},
                {"$sort": {"Ê∂®Ë∑åÂπÖ": 1}},
                {"$limit": 10},
                {
                    "$project": {
                        "_id": 0,
                        "code": "$‰ª£Á†Å",
                        "name": "$ÂêçÁß∞",
                        "rate": "$Ê∂®Ë∑åÂπÖ",
                        "type": "$Âü∫ÈáëÁ±ªÂûã"
                    }
                }
            ]
            top_losers = await self.col_fund_spot_sina.aggregate(losers_pipeline).to_list(None)
            
            # ÊúÄÊñ∞Êï∞ÊçÆÊó•Êúü
            latest_doc = await self.col_fund_spot_sina.find_one(
                {},
                sort=[("updated_at", -1)]
            )
            latest_date = latest_doc.get("Êï∞ÊçÆÊó•Êúü") if latest_doc else None
            
            return {
                'total_count': total_count,
                'rise_count': rise_count,
                'fall_count': fall_count,
                'flat_count': flat_count,
                'type_stats': type_stats,
                'top_volume': top_volume,
                'top_gainers': top_gainers,
                'top_losers': top_losers,
                'latest_date': latest_date
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖ-Êñ∞Êµ™ÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                'total_count': 0,
                'rise_count': 0,
                'fall_count': 0,
                'flat_count': 0,
                'latest_date': None
            }

    async def save_fund_etf_hist_min_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠ò ETF Âü∫ÈáëÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆÂà∞ fund_etf_hist_min_em ÈõÜÂêà„ÄÇ

        ‰ΩøÁî® `code + time + period + adjust` ‰Ωú‰∏∫ÂîØ‰∏ÄÈîÆËøõË°å upsert„ÄÇ

        Args:
            df: ÂåÖÂê´ ETF ÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆÁöÑ DataFrameÔºåËá≥Â∞ëÈúÄÂåÖÂê´ `‰ª£Á†Å` Âíå `Êó∂Èó¥` Âàó„ÄÇ
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞ (current, total, percentage, message)

        Returns:
            ÂÆûÈôÖÂÜôÂÖ•(Êñ∞Â¢û+Êõ¥Êñ∞)ÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("ETF ÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆ‰∏∫Á©∫ÔºåÊó†ÈúÄ‰øùÂ≠ò")
            return 0

        try:
            # Êã∑Ë¥ù‰∏Ä‰ªΩÔºåÈÅøÂÖç‰øÆÊîπÂ§ñÈÉ® DataFrame
            df = df.copy()

            # Áªü‰∏ÄÊ∏ÖÁêÜÊó†ÊïàÊï∞ÂÄº
            df = df.replace([float("inf"), float("-inf")], None)
            df = df.where(pd.notnull(df), None)

            total_count = len(df)
            batch_size = 1000  # ËææÂà∞1000Êù°‰øùÂ≠ò‰∏ÄÊ¨°ÔºåÈÄÄÂá∫Êó∂‰∏çË∂≥1000Êù°‰πü‰øùÂ≠ò
            total_saved = 0

            for batch_start in range(0, total_count, batch_size):
                batch_end = min(batch_start + batch_size, total_count)
                batch_df = df.iloc[batch_start:batch_end]

                ops: List[UpdateOne] = []

                for _, row in batch_df.iterrows():
                    record = row.to_dict()

                    # ‰ª£Á†Å
                    code = str(record.get("‰ª£Á†Å") or record.get("code") or "").strip()
                    if not code:
                        continue

                    # Êó∂Èó¥ -> Áªü‰∏Ä‰∏∫Â≠óÁ¨¶‰∏≤ÔºåÂπ∂Ê¥æÁîü date
                    time_val = record.get("Êó∂Èó¥") or record.get("time") or record.get("datetime")
                    if time_val is None or (isinstance(time_val, float) and pd.isna(time_val)):
                        continue

                    if isinstance(time_val, pd.Timestamp):
                        time_str = time_val.strftime("%Y-%m-%d %H:%M:%S")
                    else:
                        time_str = str(time_val).strip()

                    if not time_str:
                        continue

                    date_str = time_str[:10]

                    # Âë®Êúü‰∏éÂ§çÊùÉÊñπÂºèÔºàÈªòËÆ§ 5 ÂàÜÈíü„ÄÅÂêéÂ§çÊùÉÔºâ
                    period = str(record.get("period") or "5")
                    adjust = str(record.get("adjust") or "hfq")

                    # ÂÜôÂõûËßÑËåÉÂåñÂ≠óÊÆµ
                    record["‰ª£Á†Å"] = code
                    record["Êó∂Èó¥"] = time_str
                    record["code"] = code
                    record["time"] = time_str
                    record["date"] = date_str
                    record["period"] = period
                    record["adjust"] = adjust
                    record["source"] = record.get("source") or "akshare"
                    record["endpoint"] = record.get("endpoint") or "fund_etf_hist_min_em"
                    record["updated_at"] = datetime.now()

                    # ÊûÑÈÄ† upsert Êìç‰Ωú
                    ops.append(
                        UpdateOne(
                            {"code": code, "time": time_str, "period": period, "adjust": adjust},
                            {"$set": record},
                            upsert=True,
                        )
                    )

                if ops:
                    result = await self.col_fund_etf_hist_min_em.bulk_write(ops, ordered=False)
                    saved_count = (result.upserted_count or 0) + (result.modified_count or 0)
                    total_saved += saved_count

                # ËøõÂ∫¶ÂõûË∞É
                if progress_callback:
                    current = batch_end
                    percentage = int((current / total_count) * 100)
                    progress_callback(
                        current,
                        total_count,
                        percentage,
                        f"Â∑≤‰øùÂ≠ò {current}/{total_count} Êù° ETF ÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆ",
                    )

            logger.info(f"ÊàêÂäü‰øùÂ≠ò {total_saved}/{total_count} Êù° ETF ÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return total_saved

        except Exception as e:
            logger.error(f"‰øùÂ≠ò ETF ÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def clear_fund_etf_hist_min_data(self) -> int:
        """Ê∏ÖÁ©∫ ETF ÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆÈõÜÂêà„ÄÇ"""
        try:
            result = await self.col_fund_etf_hist_min_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù° ETF ÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ ETF ÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def get_fund_etf_hist_min_stats(self) -> Dict[str, Any]:
        """Ëé∑Âèñ ETF ÂàÜÊó∂Ë°åÊÉÖÁªüËÆ°‰ø°ÊÅØ„ÄÇ

        ÂΩìÂâçÊèê‰æõÔºö
        - total_count: ÊÄªËÆ∞ÂΩïÊï∞
        - code_stats: Êåâ‰ª£Á†ÅÂàÜÁªÑÁöÑËÆ∞ÂΩïÊï∞
        - earliest_time / latest_time: ÊúÄÊó©ÂíåÊúÄÊôöÁöÑÊó∂Èó¥Êà≥
        """
        try:
            total_count = await self.col_fund_etf_hist_min_em.count_documents({})

            # Êåâ‰ª£Á†ÅÂàÜÁªÑÁªüËÆ°Êï∞Èáè
            code_stats: List[Dict[str, Any]] = []
            pipeline_codes = [
                {"$group": {"_id": "$code", "count": {"$sum": 1}}},
                {"$sort": {"count": -1}},
            ]
            async for doc in self.col_fund_etf_hist_min_em.aggregate(pipeline_codes):
                if doc.get("_id"):
                    code_stats.append({"code": doc["_id"], "count": doc["count"]})

            # ËÆ°ÁÆóÊó∂Èó¥ËåÉÂõ¥
            earliest_time = None
            latest_time = None
            pipeline_time = [
                {
                    "$group": {
                        "_id": None,
                        "earliest": {"$min": "$time"},
                        "latest": {"$max": "$time"},
                    }
                }
            ]

            async for doc in self.col_fund_etf_hist_min_em.aggregate(pipeline_time):
                earliest_time = doc.get("earliest")
                latest_time = doc.get("latest")

            return {
                "total_count": total_count,
                "code_stats": code_stats,
                "earliest_time": earliest_time,
                "latest_time": latest_time,
            }
        except Exception as e:
            logger.error(f"Ëé∑Âèñ ETF ÂàÜÊó∂Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "code_stats": [],
                "earliest_time": None,
                "latest_time": None,
            }

    async def save_fund_lof_hist_min_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠ò LOF Âü∫ÈáëÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆÂà∞ fund_lof_hist_min_em ÈõÜÂêà„ÄÇ

        ‰ΩøÁî® `code + time + period + adjust` ‰Ωú‰∏∫ÂîØ‰∏ÄÈîÆËøõË°å upsert„ÄÇ

        Args:
            df: ÂåÖÂê´ LOF ÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆÁöÑ DataFrameÔºåËá≥Â∞ëÈúÄÂåÖÂê´ `‰ª£Á†Å` Âíå `Êó∂Èó¥` Âàó„ÄÇ
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞ (current, total, percentage, message)

        Returns:
            ÂÆûÈôÖÂÜôÂÖ•(Êñ∞Â¢û+Êõ¥Êñ∞)ÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("LOF ÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆ‰∏∫Á©∫ÔºåÊó†ÈúÄ‰øùÂ≠ò")
            return 0

        try:
            # Êã∑Ë¥ù‰∏Ä‰ªΩÔºåÈÅøÂÖç‰øÆÊîπÂ§ñÈÉ® DataFrame
            df = df.copy()

            # Áªü‰∏ÄÊ∏ÖÁêÜÊó†ÊïàÊï∞ÂÄº
            df = df.replace([float("inf"), float("-inf")], None)
            df = df.where(pd.notnull(df), None)

            total_count = len(df)
            batch_size = 1000  # ËææÂà∞1000Êù°‰øùÂ≠ò‰∏ÄÊ¨°ÔºåÈÄÄÂá∫Êó∂‰∏çË∂≥1000Êù°‰πü‰øùÂ≠ò
            total_saved = 0

            for batch_start in range(0, total_count, batch_size):
                batch_end = min(batch_start + batch_size, total_count)
                batch_df = df.iloc[batch_start:batch_end]

                ops: List[UpdateOne] = []

                for _, row in batch_df.iterrows():
                    record = row.to_dict()

                    # ‰ª£Á†Å
                    code = str(record.get("‰ª£Á†Å") or record.get("code") or "").strip()
                    if not code:
                        continue

                    # Êó∂Èó¥ -> Áªü‰∏Ä‰∏∫Â≠óÁ¨¶‰∏≤ÔºåÂπ∂Ê¥æÁîü date
                    time_val = record.get("Êó∂Èó¥") or record.get("time") or record.get("datetime")
                    if time_val is None or (isinstance(time_val, float) and pd.isna(time_val)):
                        continue

                    if isinstance(time_val, pd.Timestamp):
                        time_str = time_val.strftime("%Y-%m-%d %H:%M:%S")
                    else:
                        time_str = str(time_val).strip()

                    if not time_str:
                        continue

                    date_str = time_str[:10]

                    # Âë®Êúü‰∏éÂ§çÊùÉÊñπÂºèÔºàÈªòËÆ§ 5 ÂàÜÈíü„ÄÅÂêéÂ§çÊùÉÔºâ
                    period = str(record.get("period") or "5")
                    adjust = str(record.get("adjust") or "hfq")

                    # ÂÜôÂõûËßÑËåÉÂåñÂ≠óÊÆµ
                    record["‰ª£Á†Å"] = code
                    record["Êó∂Èó¥"] = time_str
                    record["code"] = code
                    record["time"] = time_str
                    record["date"] = date_str
                    record["period"] = period
                    record["adjust"] = adjust
                    record["source"] = record.get("source") or "akshare"
                    record["endpoint"] = record.get("endpoint") or "fund_lof_hist_min_em"
                    record["updated_at"] = datetime.now()

                    # ÊûÑÈÄ† upsert Êìç‰Ωú
                    ops.append(
                        UpdateOne(
                            {"code": code, "time": time_str, "period": period, "adjust": adjust},
                            {"$set": record},
                            upsert=True,
                        )
                    )

                if ops:
                    result = await self.col_fund_lof_hist_min_em.bulk_write(ops, ordered=False)
                    saved_count = (result.upserted_count or 0) + (result.modified_count or 0)
                    total_saved += saved_count

                # ËøõÂ∫¶ÂõûË∞É
                if progress_callback:
                    current = batch_end
                    percentage = int((current / total_count) * 100)
                    progress_callback(
                        current,
                        total_count,
                        percentage,
                        f"Â∑≤‰øùÂ≠ò {current}/{total_count} Êù° LOF ÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆ",
                    )

            logger.info(f"ÊàêÂäü‰øùÂ≠ò {total_saved}/{total_count} Êù° LOF ÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return total_saved

        except Exception as e:
            logger.error(f"‰øùÂ≠ò LOF ÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def clear_fund_lof_hist_min_data(self) -> int:
        """Ê∏ÖÁ©∫ LOF ÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆÈõÜÂêà„ÄÇ"""
        try:
            result = await self.col_fund_lof_hist_min_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù° LOF ÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ LOF ÂàÜÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def get_fund_lof_hist_min_stats(self) -> Dict[str, Any]:
        """Ëé∑Âèñ LOF ÂàÜÊó∂Ë°åÊÉÖÁªüËÆ°‰ø°ÊÅØ„ÄÇ

        ÂΩìÂâçÊèê‰æõÔºö
        - total_count: ÊÄªËÆ∞ÂΩïÊï∞
        - code_stats: Êåâ‰ª£Á†ÅÂàÜÁªÑÁöÑËÆ∞ÂΩïÊï∞
        - earliest_time / latest_time: ÊúÄÊó©ÂíåÊúÄÊôöÁöÑÊó∂Èó¥Êà≥
        """
        try:
            total_count = await self.col_fund_lof_hist_min_em.count_documents({})

            # Êåâ‰ª£Á†ÅÂàÜÁªÑÁªüËÆ°Êï∞Èáè
            code_stats: List[Dict[str, Any]] = []
            pipeline_codes = [
                {"$group": {"_id": "$code", "count": {"$sum": 1}}},
                {"$sort": {"count": -1}},
            ]
            async for doc in self.col_fund_lof_hist_min_em.aggregate(pipeline_codes):
                if doc.get("_id"):
                    code_stats.append({"code": doc["_id"], "count": doc["count"]})

            # ËÆ°ÁÆóÊó∂Èó¥ËåÉÂõ¥
            earliest_time = None
            latest_time = None
            pipeline_time = [
                {
                    "$group": {
                        "_id": None,
                        "earliest": {"$min": "$time"},
                        "latest": {"$max": "$time"},
                    }
                }
            ]

            async for doc in self.col_fund_lof_hist_min_em.aggregate(pipeline_time):
                earliest_time = doc.get("earliest")
                latest_time = doc.get("latest")

            return {
                "total_count": total_count,
                "code_stats": code_stats,
                "earliest_time": earliest_time,
                "latest_time": latest_time,
            }
        except Exception as e:
            logger.error(f"Ëé∑Âèñ LOF ÂàÜÊó∂Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "code_stats": [],
                "earliest_time": None,
                "latest_time": None,
            }

    async def save_fund_etf_hist_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠ò ETF Âü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂà∞ fund_etf_hist_em ÈõÜÂêà„ÄÇ

        ‰ΩøÁî® `code + date + period + adjust` ‰Ωú‰∏∫ÂîØ‰∏ÄÈîÆËøõË°å upsert„ÄÇ

        Args:
            df: ÂåÖÂê´ ETF ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÁöÑ DataFrameÔºåËá≥Â∞ëÈúÄÂåÖÂê´ `‰ª£Á†Å` Âíå `Êó•Êúü` Âàó„ÄÇ
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞ (current, total, percentage, message)

        Returns:
            ÂÆûÈôÖÂÜôÂÖ•(Êñ∞Â¢û+Êõ¥Êñ∞)ÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("ETF ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ‰∏∫Á©∫ÔºåÊó†ÈúÄ‰øùÂ≠ò")
            return 0

        try:
            # Êã∑Ë¥ù‰∏Ä‰ªΩÔºåÈÅøÂÖç‰øÆÊîπÂ§ñÈÉ® DataFrame
            df = df.copy()

            # Áªü‰∏ÄÊ∏ÖÁêÜÊó†ÊïàÊï∞ÂÄº
            df = df.replace([float("inf"), float("-inf")], None)
            df = df.where(pd.notnull(df), None)

            total_count = len(df)
            batch_size = 500
            total_saved = 0

            for batch_start in range(0, total_count, batch_size):
                batch_end = min(batch_start + batch_size, total_count)
                batch_df = df.iloc[batch_start:batch_end]

                ops: List[UpdateOne] = []

                for _, row in batch_df.iterrows():
                    record = row.to_dict()

                    # ‰ª£Á†Å
                    code = str(record.get("‰ª£Á†Å") or record.get("code") or "").strip()
                    if not code:
                        continue

                    # Êó•Êúü -> Áªü‰∏Ä‰∏∫Â≠óÁ¨¶‰∏≤
                    date_val = record.get("Êó•Êúü") or record.get("date")
                    if date_val is None or (isinstance(date_val, float) and pd.isna(date_val)):
                        continue

                    if isinstance(date_val, pd.Timestamp):
                        date_str = date_val.strftime("%Y-%m-%d")
                    else:
                        date_str = str(date_val).strip()[:10]

                    if not date_str:
                        continue

                    # Âë®Êúü‰∏éÂ§çÊùÉÊñπÂºèÔºàÈªòËÆ§ daily„ÄÅÂêéÂ§çÊùÉÔºâ
                    period = str(record.get("period") or "daily")
                    adjust = str(record.get("adjust") or "hfq")

                    # ÂÜôÂõûËßÑËåÉÂåñÂ≠óÊÆµ
                    record["‰ª£Á†Å"] = code
                    record["Êó•Êúü"] = date_str
                    record["code"] = code
                    record["date"] = date_str
                    record["period"] = period
                    record["adjust"] = adjust
                    record["source"] = record.get("source") or "akshare"
                    record["endpoint"] = record.get("endpoint") or "fund_etf_hist_em"
                    record["updated_at"] = datetime.now()

                    # ÊûÑÈÄ† upsert Êìç‰Ωú
                    ops.append(
                        UpdateOne(
                            {"code": code, "date": date_str, "period": period, "adjust": adjust},
                            {"$set": record},
                            upsert=True,
                        )
                    )

                if ops:
                    result = await self.col_fund_etf_hist_em.bulk_write(ops, ordered=False)
                    saved_count = (result.upserted_count or 0) + (result.modified_count or 0)
                    total_saved += saved_count

                # ËøõÂ∫¶ÂõûË∞É
                if progress_callback:
                    current = batch_end
                    percentage = int((current / total_count) * 100)
                    progress_callback(
                        current,
                        total_count,
                        percentage,
                        f"Â∑≤‰øùÂ≠ò {current}/{total_count} Êù° ETF ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ",
                    )

            logger.info(f"ÊàêÂäü‰øùÂ≠ò {total_saved}/{total_count} Êù° ETF ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ")
            return total_saved

        except Exception as e:
            logger.error(f"‰øùÂ≠ò ETF ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def clear_fund_etf_hist_data(self) -> int:
        """Ê∏ÖÁ©∫ ETF ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÈõÜÂêà„ÄÇ"""
        try:
            result = await self.col_fund_etf_hist_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù° ETF ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ ETF ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def get_fund_etf_hist_stats(self) -> Dict[str, Any]:
        """Ëé∑Âèñ ETF ÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°‰ø°ÊÅØ„ÄÇ

        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏ÔºåÂåÖÂê´Ôºö
            - total_count: ÊÄªËÆ∞ÂΩïÊï∞
            - code_stats: ÂêÑ‰ª£Á†ÅÁöÑÁªüËÆ° [{code, count}, ...]
            - earliest_date: ÊúÄÊó©Êó•Êúü
            - latest_date: ÊúÄÊñ∞Êó•Êúü
        """
        try:
            # ÊÄªËÆ∞ÂΩïÊï∞
            total_count = await self.col_fund_etf_hist_em.count_documents({})

            # ÂêÑ‰ª£Á†ÅÁªüËÆ°
            code_pipeline = [
                {"$group": {"_id": "$code", "count": {"$sum": 1}}},
                {"$project": {"code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 100},
            ]
            code_stats = await self.col_fund_etf_hist_em.aggregate(code_pipeline).to_list(100)

            # ÊúÄÊó©ÂíåÊúÄÊñ∞Êó•Êúü
            earliest_date = None
            latest_date = None

            if total_count > 0:
                earliest_doc = (
                    await self.col_fund_etf_hist_em.find({}, {"date": 1})
                    .sort("date", 1)
                    .limit(1)
                    .to_list(1)
                )
                if earliest_doc:
                    earliest_date = earliest_doc[0].get("date")

                latest_doc = (
                    await self.col_fund_etf_hist_em.find({}, {"date": 1})
                    .sort("date", -1)
                    .limit(1)
                    .to_list(1)
                )
                if latest_doc:
                    latest_date = latest_doc[0].get("date")

            result = {
                "total_count": total_count,
                "code_stats": code_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            logger.debug(f"ETF ÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°: {result}")
            return result

        except Exception as e:
            logger.error(f"Ëé∑Âèñ ETF ÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_lof_hist_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠ò LOF Âü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂà∞ fund_lof_hist_em ÈõÜÂêà„ÄÇ

        ‰ΩøÁî® `code + date + period + adjust` ‰Ωú‰∏∫ÂîØ‰∏ÄÈîÆËøõË°å upsert„ÄÇ

        Args:
            df: ÂåÖÂê´ LOF ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÁöÑ DataFrameÔºåËá≥Â∞ëÈúÄÂåÖÂê´ `‰ª£Á†Å` Âíå `Êó•Êúü` Âàó„ÄÇ
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞ (current, total, percentage, message)

        Returns:
            ÂÆûÈôÖÂÜôÂÖ•(Êñ∞Â¢û+Êõ¥Êñ∞)ÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("LOF ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ‰∏∫Á©∫ÔºåÊó†ÈúÄ‰øùÂ≠ò")
            return 0

        try:
            # Êã∑Ë¥ù‰∏Ä‰ªΩÔºåÈÅøÂÖç‰øÆÊîπÂ§ñÈÉ® DataFrame
            df = df.copy()

            # Áªü‰∏ÄÊ∏ÖÁêÜÊó†ÊïàÊï∞ÂÄº
            df = df.replace([float("inf"), float("-inf")], None)
            df = df.where(pd.notnull(df), None)

            total_count = len(df)
            batch_size = 500
            total_saved = 0

            for batch_start in range(0, total_count, batch_size):
                batch_end = min(batch_start + batch_size, total_count)
                batch_df = df.iloc[batch_start:batch_end]

                ops: List[UpdateOne] = []

                for _, row in batch_df.iterrows():
                    record = row.to_dict()

                    # ‰ª£Á†Å
                    code = str(record.get("‰ª£Á†Å") or record.get("code") or "").strip()
                    if not code:
                        continue

                    # Êó•Êúü -> Áªü‰∏Ä‰∏∫Â≠óÁ¨¶‰∏≤
                    date_val = record.get("Êó•Êúü") or record.get("date")
                    if date_val is None or (isinstance(date_val, float) and pd.isna(date_val)):
                        continue

                    if isinstance(date_val, pd.Timestamp):
                        date_str = date_val.strftime("%Y-%m-%d")
                    else:
                        date_str = str(date_val).strip()[:10]

                    if not date_str:
                        continue

                    # Âë®Êúü‰∏éÂ§çÊùÉÊñπÂºèÔºàÈªòËÆ§ daily„ÄÅÂêéÂ§çÊùÉÔºâ
                    period = str(record.get("period") or "daily")
                    adjust = str(record.get("adjust") or "hfq")

                    # ÂÜôÂõûËßÑËåÉÂåñÂ≠óÊÆµ
                    record["‰ª£Á†Å"] = code
                    record["Êó•Êúü"] = date_str
                    record["code"] = code
                    record["date"] = date_str
                    record["period"] = period
                    record["adjust"] = adjust
                    record["source"] = record.get("source") or "akshare"
                    record["endpoint"] = record.get("endpoint") or "fund_lof_hist_em"
                    record["updated_at"] = datetime.now()

                    # ÊûÑÈÄ† upsert Êìç‰Ωú
                    ops.append(
                        UpdateOne(
                            {"code": code, "date": date_str, "period": period, "adjust": adjust},
                            {"$set": record},
                            upsert=True,
                        )
                    )

                if ops:
                    result = await self.col_fund_lof_hist_em.bulk_write(ops, ordered=False)
                    saved_count = (result.upserted_count or 0) + (result.modified_count or 0)
                    total_saved += saved_count

                # ËøõÂ∫¶ÂõûË∞É
                if progress_callback:
                    current = batch_end
                    percentage = int((current / total_count) * 100)
                    progress_callback(
                        current,
                        total_count,
                        percentage,
                        f"Â∑≤‰øùÂ≠ò {current}/{total_count} Êù° LOF ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ",
                    )

            logger.info(f"ÊàêÂäü‰øùÂ≠ò {total_saved}/{total_count} Êù° LOF ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ")
            return total_saved

        except Exception as e:
            logger.error(f"‰øùÂ≠ò LOF ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def clear_fund_lof_hist_data(self) -> int:
        """Ê∏ÖÁ©∫ LOF ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÈõÜÂêà„ÄÇ"""
        try:
            result = await self.col_fund_lof_hist_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù° LOF ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ LOF ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def get_fund_lof_hist_stats(self) -> Dict[str, Any]:
        """Ëé∑Âèñ LOF ÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°‰ø°ÊÅØ„ÄÇ

        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏ÔºåÂåÖÂê´Ôºö
            - total_count: ÊÄªËÆ∞ÂΩïÊï∞
            - code_stats: ÂêÑ‰ª£Á†ÅÁöÑÁªüËÆ° [{code, count}, ...]
            - earliest_date: ÊúÄÊó©Êó•Êúü
            - latest_date: ÊúÄÊñ∞Êó•Êúü
        """
        try:
            # ÊÄªËÆ∞ÂΩïÊï∞
            total_count = await self.col_fund_lof_hist_em.count_documents({})

            # ÂêÑ‰ª£Á†ÅÁªüËÆ°
            code_pipeline = [
                {"$group": {"_id": "$code", "count": {"$sum": 1}}},
                {"$project": {"code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 100},
            ]
            code_stats = await self.col_fund_lof_hist_em.aggregate(code_pipeline).to_list(100)

            # ÊúÄÊó©ÂíåÊúÄÊñ∞Êó•Êúü
            earliest_date = None
            latest_date = None

            if total_count > 0:
                earliest_doc = (
                    await self.col_fund_lof_hist_em.find({}, {"date": 1})
                    .sort("date", 1)
                    .limit(1)
                    .to_list(1)
                )
                if earliest_doc:
                    earliest_date = earliest_doc[0].get("date")

                latest_doc = (
                    await self.col_fund_lof_hist_em.find({}, {"date": 1})
                    .sort("date", -1)
                    .limit(1)
                    .to_list(1)
                )
                if latest_doc:
                    latest_date = latest_doc[0].get("date")

            result = {
                "total_count": total_count,
                "code_stats": code_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            logger.debug(f"LOF ÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°: {result}")
            return result

        except Exception as e:
            logger.error(f"Ëé∑Âèñ LOF ÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_hist_sina_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÊñ∞Êµ™Âü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ

        Args:
            df: ÂåÖÂê´ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÁöÑ DataFrameÔºåÂøÖÈ°ªÂåÖÂê´ date, open, high, low, close, volume Âíå‰ª£Á†ÅÂ≠óÊÆµ
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞

        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("Êñ∞Êµ™Âü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ‰∏∫Á©∫ÔºåÊó†ÈúÄ‰øùÂ≠ò")
            return 0

        try:
            df = df.copy()

            # Â≠óÊÆµÊò†Â∞ÑÂíåËßÑËåÉÂåñ
            field_mapping = {
                "date": "date",
                "Êó•Êúü": "date",
                "open": "open",
                "ÂºÄÁõò": "open",
                "high": "high",
                "ÊúÄÈ´ò": "high",
                "low": "low",
                "ÊúÄ‰Ωé": "low",
                "close": "close",
                "Êî∂Áõò": "close",
                "volume": "volume",
                "Êàê‰∫§Èáè": "volume",
                "‰ª£Á†Å": "code",
                "code": "code",
            }

            # ÈáçÂëΩÂêçÂàó
            df = df.rename(columns=field_mapping)

            # Ê£ÄÊü•ÂøÖÈúÄÂ≠óÊÆµ
            required_fields = ["date", "open", "high", "low", "close", "volume", "code"]
            missing = [f for f in required_fields if f not in df.columns]
            if missing:
                logger.error(f"Áº∫Â∞ëÂøÖÈúÄÂ≠óÊÆµ: {missing}")
                return 0

            # Êï∞ÊçÆÊ∏ÖÁêÜÔºöÂ§ÑÁêÜÊó†ÊïàÊï∞ÂÄºÔºàinf„ÄÅNaNÔºâ
            numeric_fields = ["open", "high", "low", "close", "volume"]
            for field in numeric_fields:
                if field in df.columns:
                    df[field] = pd.to_numeric(df[field], errors="coerce")
                    df[field] = df[field].replace([float("inf"), float("-inf")], None)

            # Âà†Èô§ÂÖ≥ÈîÆÂ≠óÊÆµ‰∏∫Á©∫ÁöÑË°å
            df = df.dropna(subset=["date", "code"])

            # Êó•ÊúüÊ†ºÂºèËΩ¨Êç¢
            if df["date"].dtype == "object":
                try:
                    df["date"] = pd.to_datetime(df["date"]).dt.strftime("%Y-%m-%d")
                except Exception as e:
                    logger.warning(f"Êó•ÊúüÊ†ºÂºèËΩ¨Êç¢Â§±Ë¥•: {e}")

            ops = []
            total = len(df)

            for idx, row in df.iterrows():
                code = str(row.get("code", "")).strip()
                date_str = str(row.get("date", "")).strip()

                if not code or not date_str:
                    continue

                record = {
                    "code": code,
                    "date": date_str,
                    "open": float(row["open"]) if pd.notna(row.get("open")) else None,
                    "high": float(row["high"]) if pd.notna(row.get("high")) else None,
                    "low": float(row["low"]) if pd.notna(row.get("low")) else None,
                    "close": float(row["close"]) if pd.notna(row.get("close")) else None,
                    "volume": int(row["volume"]) if pd.notna(row.get("volume")) else None,
                }

                # ÂîØ‰∏ÄÈîÆÔºöcode + date
                ops.append(
                    UpdateOne(
                        {"code": code, "date": date_str},
                        {"$set": record},
                        upsert=True,
                    )
                )

                # ËøõÂ∫¶ÂõûË∞É
                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            if not ops:
                logger.warning("Ê≤°ÊúâÊúâÊïàÊï∞ÊçÆÂèØ‰øùÂ≠ò")
                return 0

            result = await self.col_fund_hist_sina.bulk_write(ops, ordered=False)
            saved_count = (result.upserted_count or 0) + (result.modified_count or 0)

            logger.info(f"ÊàêÂäü‰øùÂ≠ò {saved_count} Êù°Êñ∞Êµ™Âü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ")
            return saved_count

        except Exception as e:
            logger.error(f"‰øùÂ≠òÊñ∞Êµ™Âü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def clear_fund_hist_sina_data(self) -> int:
        """Ê∏ÖÁ©∫Êñ∞Êµ™Âü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ

        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_hist_sina.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Êñ∞Êµ™Âü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Êñ∞Êµ™Âü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def get_fund_hist_sina_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÊñ∞Êµ™Âü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°‰ø°ÊÅØ

        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏ÔºåÂåÖÊã¨ÊÄªÊï∞„ÄÅÂêÑ‰ª£Á†ÅËÆ∞ÂΩïÊï∞„ÄÅÊúÄÊó©/ÊúÄÊñ∞Êó•Êúü
        """
        try:
            total_count = await self.col_fund_hist_sina.count_documents({})

            # Êåâ‰ª£Á†ÅÁªüËÆ°ËÆ∞ÂΩïÊï∞ÔºàTop 100Ôºâ
            code_pipeline = [
                {"$group": {"_id": "$code", "count": {"$sum": 1}}},
                {"$project": {"code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 100},
            ]
            code_stats = await self.col_fund_hist_sina.aggregate(code_pipeline).to_list(100)

            # Ëé∑ÂèñÊúÄÊó©ÂíåÊúÄÊñ∞Êó•Êúü
            earliest_doc = (
                await self.col_fund_hist_sina.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_hist_sina.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            earliest_date = earliest_doc[0]["date"] if earliest_doc else None
            latest_date = latest_doc[0]["date"] if latest_doc else None

            result = {
                "total_count": total_count,
                "code_stats": code_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            logger.debug(f"Êñ∞Êµ™Âü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°: {result}")
            return result

        except Exception as e:
            logger.error(f"Ëé∑ÂèñÊñ∞Êµ™Âü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_open_fund_daily_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂºÄÊîæÂºèÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ

        Args:
            df: ÂåÖÂê´ÂºÄÊîæÂºèÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁöÑ DataFrame
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞

        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("ÂºÄÊîæÂºèÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ‰∏∫Á©∫ÔºåÊó†ÈúÄ‰øùÂ≠ò")
            return 0

        try:
            df = df.copy()

            # ‰ªéÂàóÂêç‰∏≠ÊèêÂèñÊó•ÊúüÔºàÂàóÂêçÊ†ºÂºèÂ¶Ç "2024-01-01-Âçï‰ΩçÂáÄÂÄº"Ôºâ
            date_str = None
            for col in df.columns:
                if "-Âçï‰ΩçÂáÄÂÄº" in col:
                    # ÊèêÂèñÊó•ÊúüÈÉ®ÂàÜ
                    date_str = col.split("-Âçï‰ΩçÂáÄÂÄº")[0]
                    break

            if not date_str:
                # Â¶ÇÊûúÊ≤°ÊúâÊâæÂà∞Êó•ÊúüÔºå‰ΩøÁî®ÂΩìÂâçÊó•Êúü
                from datetime import datetime

                date_str = datetime.now().strftime("%Y-%m-%d")
                logger.warning(f"Êú™‰ªéÂàóÂêç‰∏≠ÊâæÂà∞Êó•ÊúüÔºå‰ΩøÁî®ÂΩìÂâçÊó•Êúü: {date_str}")

            # Â≠óÊÆµÊò†Â∞Ñ
            field_mapping = {
                "Âü∫Èáë‰ª£Á†Å": "fund_code",
                "Âü∫ÈáëÁÆÄÁß∞": "fund_name",
                f"{date_str}-Âçï‰ΩçÂáÄÂÄº": "unit_net_value",
                f"{date_str}-Á¥ØËÆ°ÂáÄÂÄº": "cumulative_net_value",
                f"{date_str}-Ââç‰∫§ÊòìÊó•-Âçï‰ΩçÂáÄÂÄº": "prev_unit_net_value",
                f"{date_str}-Ââç‰∫§ÊòìÊó•-Á¥ØËÆ°ÂáÄÂÄº": "prev_cumulative_net_value",
                "Êó•Â¢ûÈïøÂÄº": "daily_growth_value",
                "Êó•Â¢ûÈïøÁéá": "daily_growth_rate",
                "Áî≥Ë¥≠Áä∂ÊÄÅ": "purchase_status",
                "ËµéÂõûÁä∂ÊÄÅ": "redemption_status",
                "ÊâãÁª≠Ë¥π": "fee",
            }

            # ÈáçÂëΩÂêçÂàó
            df = df.rename(columns=field_mapping)

            # Ê£ÄÊü•ÂøÖÈúÄÂ≠óÊÆµ
            required_fields = ["fund_code"]
            missing = [f for f in required_fields if f not in df.columns]
            if missing:
                logger.error(f"Áº∫Â∞ëÂøÖÈúÄÂ≠óÊÆµ: {missing}")
                return 0

            # Êï∞ÊçÆÊ∏ÖÁêÜÔºöÂ§ÑÁêÜÊó†ÊïàÊï∞ÂÄºÔºàinf„ÄÅNaNÔºâ
            numeric_fields = [
                "unit_net_value",
                "cumulative_net_value",
                "prev_unit_net_value",
                "prev_cumulative_net_value",
                "daily_growth_value",
                "daily_growth_rate",
            ]
            for field in numeric_fields:
                if field in df.columns:
                    df[field] = pd.to_numeric(df[field], errors="coerce")
                    df[field] = df[field].replace([float("inf"), float("-inf")], None)

            # Âà†Èô§ÂÖ≥ÈîÆÂ≠óÊÆµ‰∏∫Á©∫ÁöÑË°å
            df = df.dropna(subset=["fund_code"])

            ops = []
            total = len(df)
            batch_size = 1000  # ÊØèÊâπÂ§ÑÁêÜ1000Êù°
            total_saved = 0

            for idx, row in df.iterrows():
                fund_code = str(row.get("fund_code", "")).strip()

                if not fund_code:
                    continue

                record = {
                    "fund_code": fund_code,
                    "date": date_str,
                    "fund_name": str(row.get("fund_name", "")).strip() if pd.notna(row.get("fund_name")) else None,
                    "unit_net_value": float(row["unit_net_value"]) if pd.notna(row.get("unit_net_value")) else None,
                    "cumulative_net_value": float(row["cumulative_net_value"]) if pd.notna(row.get("cumulative_net_value")) else None,
                    "prev_unit_net_value": float(row["prev_unit_net_value"]) if pd.notna(row.get("prev_unit_net_value")) else None,
                    "prev_cumulative_net_value": float(row["prev_cumulative_net_value"]) if pd.notna(row.get("prev_cumulative_net_value")) else None,
                    "daily_growth_value": float(row["daily_growth_value"]) if pd.notna(row.get("daily_growth_value")) else None,
                    "daily_growth_rate": float(row["daily_growth_rate"]) if pd.notna(row.get("daily_growth_rate")) else None,
                    "purchase_status": str(row.get("purchase_status", "")).strip() if pd.notna(row.get("purchase_status")) else None,
                    "redemption_status": str(row.get("redemption_status", "")).strip() if pd.notna(row.get("redemption_status")) else None,
                    "fee": str(row.get("fee", "")).strip() if pd.notna(row.get("fee")) else None,
                }

                # ÂîØ‰∏ÄÈîÆÔºöfund_code + date
                ops.append(
                    UpdateOne(
                        {"fund_code": fund_code, "date": date_str},
                        {"$set": record},
                        upsert=True,
                    )
                )

                # ÊâπÈáè‰øùÂ≠òÔºöÊØè1000Êù°‰øùÂ≠ò‰∏ÄÊ¨°
                if len(ops) >= batch_size:
                    result = await self.col_fund_open_fund_daily_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.modified_count or 0)
                    total_saved += batch_saved
                    logger.info(f"Â∑≤‰øùÂ≠ò {len(ops)} Êù°Êï∞ÊçÆÔºåÁ¥ØËÆ°‰øùÂ≠ò {total_saved} Êù°")
                    ops = []  # Ê∏ÖÁ©∫Â∑≤Â§ÑÁêÜÁöÑÊìç‰Ωú

                # ËøõÂ∫¶ÂõûË∞É
                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            # ‰øùÂ≠òÂâ©‰ΩôÊï∞ÊçÆ
            if ops:
                result = await self.col_fund_open_fund_daily_em.bulk_write(ops, ordered=False)
                batch_saved = (result.upserted_count or 0) + (result.modified_count or 0)
                total_saved += batch_saved
                logger.info(f"Â∑≤‰øùÂ≠òÂâ©‰Ωô {len(ops)} Êù°Êï∞ÊçÆ")

            if total_saved == 0:
                logger.warning("Ê≤°ÊúâÊúâÊïàÊï∞ÊçÆÂèØ‰øùÂ≠ò")
                return 0

            logger.info(f"ÊàêÂäü‰øùÂ≠ò {total_saved} Êù°ÂºÄÊîæÂºèÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÔºàÊó•Êúü: {date_str}Ôºâ")
            return total_saved

        except Exception as e:
            logger.error(f"‰øùÂ≠òÂºÄÊîæÂºèÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def clear_fund_open_fund_daily_data(self) -> int:
        """Ê∏ÖÁ©∫ÂºÄÊîæÂºèÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ

        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_open_fund_daily_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÂºÄÊîæÂºèÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÂºÄÊîæÂºèÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def get_fund_open_fund_daily_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂºÄÊîæÂºèÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°‰ø°ÊÅØ

        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏ÔºåÂåÖÊã¨ÊÄªÊï∞„ÄÅÊúÄÊó©/ÊúÄÊñ∞Êó•Êúü
        """
        try:
            total_count = await self.col_fund_open_fund_daily_em.count_documents({})

            # Ëé∑ÂèñÊúÄÊó©ÂíåÊúÄÊñ∞Êó•Êúü
            earliest_doc = (
                await self.col_fund_open_fund_daily_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_open_fund_daily_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            earliest_date = earliest_doc[0]["date"] if earliest_doc else None
            latest_date = latest_doc[0]["date"] if latest_doc else None

            # ÊåâÊó•ÊúüÁªüËÆ°ËÆ∞ÂΩïÊï∞
            date_pipeline = [
                {"$group": {"_id": "$date", "count": {"$sum": 1}}},
                {"$project": {"date": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"date": -1}},
                {"$limit": 30},
            ]
            date_stats = await self.col_fund_open_fund_daily_em.aggregate(date_pipeline).to_list(30)

            result = {
                "total_count": total_count,
                "date_stats": date_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            logger.debug(f"ÂºÄÊîæÂºèÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°: {result}")
            return result

        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂºÄÊîæÂºèÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "date_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_open_fund_info_data(
        self, df: pd.DataFrame, fund_code: str, indicator: str, progress_callback=None
    ) -> int:
        """‰øùÂ≠òÂºÄÊîæÂºèÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÔºàÊîØÊåÅÊâÄÊúâ7‰∏™ÊåáÊ†áÔºâ

        Args:
            df: ÂåÖÂê´ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÁöÑ DataFrame
            fund_code: Âü∫Èáë‰ª£Á†Å
            indicator: ÊåáÊ†áÁ±ªÂûãÔºàÂçï‰ΩçÂáÄÂÄºËµ∞Âäø„ÄÅÁ¥ØËÆ°ÂáÄÂÄºËµ∞Âäø„ÄÅÁ¥ØËÆ°Êî∂ÁõäÁéáËµ∞ÂäøÁ≠âÔºâ
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞

        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning(f"ÂºÄÊîæÂºèÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ‰∏∫Á©∫Ôºà{fund_code}, {indicator}Ôºâ")
            return 0

        try:
            df = df.copy()

            # Ê†πÊçÆ‰∏çÂêåÁöÑ indicator Á°ÆÂÆöÊó•ÊúüÂ≠óÊÆµÂêçÁß∞
            date_field_map = {
                "Âçï‰ΩçÂáÄÂÄºËµ∞Âäø": "ÂáÄÂÄºÊó•Êúü",
                "Á¥ØËÆ°ÂáÄÂÄºËµ∞Âäø": "ÂáÄÂÄºÊó•Êúü",
                "Á¥ØËÆ°Êî∂ÁõäÁéáËµ∞Âäø": "Êó•Êúü",
                "ÂêåÁ±ªÊéíÂêçËµ∞Âäø": "Êä•ÂëäÊó•Êúü",
                "ÂêåÁ±ªÊéíÂêçÁôæÂàÜÊØî": "Êä•ÂëäÊó•Êúü",
                "ÂàÜÁ∫¢ÈÄÅÈÖçËØ¶ÊÉÖ": "ÊùÉÁõäÁôªËÆ∞Êó•",  # ÊàñÈô§ÊÅØÊó•
                "ÊãÜÂàÜËØ¶ÊÉÖ": "ÊãÜÂàÜÊäòÁÆóÊó•",
            }

            source_date_field = date_field_map.get(indicator)
            if not source_date_field or source_date_field not in df.columns:
                # Â∞ùËØïÂÖ∂‰ªñÂèØËÉΩÁöÑÊó•ÊúüÂ≠óÊÆµ
                possible_date_fields = ["ÂáÄÂÄºÊó•Êúü", "Êó•Êúü", "Êä•ÂëäÊó•Êúü", "ÊùÉÁõäÁôªËÆ∞Êó•", "Èô§ÊÅØÊó•", "ÊãÜÂàÜÊäòÁÆóÊó•", "ÂàÜÁ∫¢ÂèëÊîæÊó•"]
                source_date_field = None
                for field in possible_date_fields:
                    if field in df.columns:
                        source_date_field = field
                        break

            if not source_date_field:
                logger.error(f"Êó†Ê≥ïÊâæÂà∞Êó•ÊúüÂ≠óÊÆµ: indicator={indicator}, columns={df.columns.tolist()}")
                return 0

            ops = []
            total = len(df)

            for idx, row in df.iterrows():
                date_value = str(row.get(source_date_field, "")).strip()
                if not date_value or date_value == "nan":
                    continue

                # ÊûÑÂª∫ËÆ∞ÂΩï - Âä®ÊÄÅ‰øùÂ≠òÊâÄÊúâÂ≠óÊÆµ
                record = {
                    "fund_code": fund_code,
                    "indicator": indicator,
                    "date": date_value,
                }

                # ‰øùÂ≠òÊâÄÊúâÂÖ∂‰ªñÂ≠óÊÆµ
                for col in df.columns:
                    if col != source_date_field:  # Êó•ÊúüÂ≠óÊÆµÂ∑≤Áªè‰øùÂ≠ò‰∏∫ date
                        value = row.get(col)
                        if pd.notna(value):
                            # Â∞ùËØïËΩ¨Êç¢Êï∞ÂÄºÁ±ªÂûã
                            if isinstance(value, (int, float)):
                                record[col] = float(value) if not isinstance(value, int) else int(value)
                            else:
                                record[col] = str(value).strip()

                # ÂîØ‰∏ÄÈîÆÔºöfund_code + indicator + date
                ops.append(
                    UpdateOne(
                        {"fund_code": fund_code, "indicator": indicator, "date": date_value},
                        {"$set": record},
                        upsert=True,
                    )
                )

                # ËøõÂ∫¶ÂõûË∞É
                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            if not ops:
                logger.warning("Ê≤°ÊúâÊúâÊïàÊï∞ÊçÆÂèØ‰øùÂ≠ò")
                return 0

            result = await self.col_fund_open_fund_info_em.bulk_write(ops, ordered=False)
            saved_count = (result.upserted_count or 0) + (result.modified_count or 0)

            logger.info(
                f"ÊàêÂäü‰øùÂ≠ò {saved_count} Êù°ÂºÄÊîæÂºèÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÔºà{fund_code}, {indicator}Ôºâ"
            )
            return saved_count

        except Exception as e:
            logger.error(f"‰øùÂ≠òÂºÄÊîæÂºèÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_open_fund_info_merged_data(
        self, df_unit: pd.DataFrame, df_acc: pd.DataFrame, fund_code: str, progress_callback=None
    ) -> int:
        """ÂêàÂπ∂Âçï‰ΩçÂáÄÂÄºËµ∞ÂäøÂíåÁ¥ØËÆ°ÂáÄÂÄºËµ∞ÂäøÔºå‰øùÂ≠òÂà∞Êï∞ÊçÆÂ∫ì
        
        Args:
            df_unit: Âçï‰ΩçÂáÄÂÄºËµ∞ÂäøDataFrame
            df_acc: Á¥ØËÆ°ÂáÄÂÄºËµ∞ÂäøDataFrame
            fund_code: Âü∫Èáë‰ª£Á†Å
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞
            
        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df_unit is None or df_unit.empty or df_acc is None or df_acc.empty:
            logger.warning(f"Âçï‰ΩçÂáÄÂÄºÊàñÁ¥ØËÆ°ÂáÄÂÄºÊï∞ÊçÆ‰∏∫Á©∫Ôºà{fund_code}Ôºâ")
            return 0
            
        try:
            df_unit = df_unit.copy()
            df_acc = df_acc.copy()
            
            # Á°Æ‰øù‰∏§‰∏™DataFrameÈÉΩÊúâÊó•ÊúüÂ≠óÊÆµ
            if "ÂáÄÂÄºÊó•Êúü" not in df_unit.columns or "ÂáÄÂÄºÊó•Êúü" not in df_acc.columns:
                logger.error(f"Êï∞ÊçÆÁº∫Â∞ëÂáÄÂÄºÊó•ÊúüÂ≠óÊÆµ: df_unit columns={df_unit.columns.tolist()}, df_acc columns={df_acc.columns.tolist()}")
                return 0
            
            # ÈáçÂëΩÂêçÂàó‰ª•‰æøÂå∫ÂàÜ
            df_unit = df_unit.rename(columns={"Âçï‰ΩçÂáÄÂÄº": "unit_net_value", "Êó•Â¢ûÈïøÁéá": "daily_growth_rate"})
            df_acc = df_acc.rename(columns={"Á¥ØËÆ°ÂáÄÂÄº": "cumulative_net_value"})
            
            # ÊåâÊó•ÊúüÂêàÂπ∂
            merged_df = pd.merge(
                df_unit[["ÂáÄÂÄºÊó•Êúü", "unit_net_value", "daily_growth_rate"]],
                df_acc[["ÂáÄÂÄºÊó•Êúü", "cumulative_net_value"]],
                on="ÂáÄÂÄºÊó•Êúü",
                how="inner"
            )
            
            if merged_df.empty:
                logger.warning(f"ÂêàÂπ∂ÂêéÊï∞ÊçÆ‰∏∫Á©∫Ôºà{fund_code}Ôºâ")
                return 0
            
            # ÊâπÈáè‰øùÂ≠ò
            ops = []
            total = len(merged_df)
            batch_size = 1000
            total_saved = 0
            
            for idx, row in merged_df.iterrows():
                date_value = str(row.get("ÂáÄÂÄºÊó•Êúü", "")).strip()
                if not date_value or date_value == "nan":
                    continue
                
                record = {
                    "fund_code": fund_code,
                    "date": date_value,
                    "unit_net_value": float(row["unit_net_value"]) if pd.notna(row.get("unit_net_value")) else None,
                    "daily_growth_rate": float(row["daily_growth_rate"]) if pd.notna(row.get("daily_growth_rate")) else None,
                    "cumulative_net_value": float(row["cumulative_net_value"]) if pd.notna(row.get("cumulative_net_value")) else None,
                }
                
                # ÂîØ‰∏ÄÈîÆÔºöfund_code + date
                ops.append(
                    UpdateOne(
                        {"fund_code": fund_code, "date": date_value},
                        {"$set": record},
                        upsert=True,
                    )
                )
                
                # ÊâπÈáè‰øùÂ≠òÔºöÊØè1000Êù°‰øùÂ≠ò‰∏ÄÊ¨°
                if len(ops) >= batch_size:
                    result = await self.col_fund_open_fund_info_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.modified_count or 0)
                    total_saved += batch_saved
                    logger.info(f"Â∑≤‰øùÂ≠ò {len(ops)} Êù°Êï∞ÊçÆÔºåÁ¥ØËÆ°‰øùÂ≠ò {total_saved} Êù°Ôºà{fund_code}Ôºâ")
                    ops = []
                
                # ËøõÂ∫¶ÂõûË∞É
                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)
            
            # ‰øùÂ≠òÂâ©‰ΩôÊï∞ÊçÆ
            if ops:
                result = await self.col_fund_open_fund_info_em.bulk_write(ops, ordered=False)
                batch_saved = (result.upserted_count or 0) + (result.modified_count or 0)
                total_saved += batch_saved
                logger.info(f"Â∑≤‰øùÂ≠òÂâ©‰Ωô {len(ops)} Êù°Êï∞ÊçÆÔºà{fund_code}Ôºâ")
            
            if total_saved == 0:
                logger.warning("Ê≤°ÊúâÊúâÊïàÊï∞ÊçÆÂèØ‰øùÂ≠ò")
                return 0
            
            logger.info(f"ÊàêÂäü‰øùÂ≠ò {total_saved} Êù°ÂºÄÊîæÂºèÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÔºà{fund_code}Ôºâ")
            return total_saved
            
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂêàÂπ∂ÂêéÁöÑÂºÄÊîæÂºèÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def clear_fund_open_fund_info_data(self) -> int:
        """Ê∏ÖÁ©∫ÂºÄÊîæÂºèÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ

        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_open_fund_info_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÂºÄÊîæÂºèÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÂºÄÊîæÂºèÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def get_fund_open_fund_info_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂºÄÊîæÂºèÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°‰ø°ÊÅØ

        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏ÔºåÂåÖÊã¨ÊÄªÊï∞„ÄÅÂêÑÂü∫Èáë‰ª£Á†ÅËÆ∞ÂΩïÊï∞„ÄÅÂêÑÊåáÊ†áËÆ∞ÂΩïÊï∞
        """
        try:
            total_count = await self.col_fund_open_fund_info_em.count_documents({})

            # ÊåâÂü∫Èáë‰ª£Á†ÅÁªüËÆ°ËÆ∞ÂΩïÊï∞ÔºàTop 50Ôºâ
            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 50},
            ]
            fund_code_stats = await self.col_fund_open_fund_info_em.aggregate(
                fund_code_pipeline
            ).to_list(50)

            # ÊåâÊåáÊ†áÁªüËÆ°ËÆ∞ÂΩïÊï∞
            indicator_pipeline = [
                {"$group": {"_id": "$indicator", "count": {"$sum": 1}}},
                {"$project": {"indicator": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
            ]
            indicator_stats = await self.col_fund_open_fund_info_em.aggregate(
                indicator_pipeline
            ).to_list(10)

            # Ëé∑ÂèñÊúÄÊó©ÂíåÊúÄÊñ∞Êó•Êúü
            earliest_doc = (
                await self.col_fund_open_fund_info_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_open_fund_info_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            earliest_date = earliest_doc[0]["date"] if earliest_doc else None
            latest_date = latest_doc[0]["date"] if latest_doc else None

            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "indicator_stats": indicator_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            logger.debug(f"ÂºÄÊîæÂºèÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°: {result}")
            return result

        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂºÄÊîæÂºèÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "indicator_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_money_fund_daily_data(
        self, df: pd.DataFrame, progress_callback=None
    ) -> int:
        """‰øùÂ≠òË¥ßÂ∏ÅÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ

        Args:
            df: ÂåÖÂê´Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÁöÑ DataFrame
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞

        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ‰∏∫Á©∫")
            return 0

        try:
            df = df.copy()

            # Ëé∑ÂèñÂΩìÂâçÊó•Êúü‰Ωú‰∏∫Êï∞ÊçÆÊó•Êúü
            from datetime import datetime
            current_date = datetime.now().strftime("%Y-%m-%d")

            # Ê∏ÖÁêÜÂíåËßÑËåÉÂåñÂàóÂêç
            df.columns = df.columns.str.strip()

            # Â≠óÊÆµÊò†Â∞Ñ
            field_map = {
                "Âü∫Èáë‰ª£Á†Å": "fund_code",
                "Âü∫ÈáëÁÆÄÁß∞": "fund_name",
                "ÂΩìÂâç‰∫§ÊòìÊó•-‰∏á‰ªΩÊî∂Áõä": "current_daily_profit_per_10k",
                "ÂΩìÂâç‰∫§ÊòìÊó•-7Êó•Âπ¥Âåñ%": "current_7day_annual_yield",
                "ÂΩìÂâç‰∫§ÊòìÊó•-Âçï‰ΩçÂáÄÂÄº": "current_unit_net_value",
                "Ââç‰∏Ä‰∫§ÊòìÊó•-‰∏á‰ªΩÊî∂Áõä": "prev_daily_profit_per_10k",
                "Ââç‰∏Ä‰∫§ÊòìÊó•-7Êó•Âπ¥Âåñ%": "prev_7day_annual_yield",
                "Ââç‰∏Ä‰∫§ÊòìÊó•-Âçï‰ΩçÂáÄÂÄº": "prev_unit_net_value",
                "Êó•Ê∂®ÂπÖ": "daily_change",
                "ÊàêÁ´ãÊó•Êúü": "establishment_date",
                "Âü∫ÈáëÁªèÁêÜ": "fund_manager",
                "ÊâãÁª≠Ë¥π": "fee",
                "ÂèØË¥≠ÂÖ®ÈÉ®": "purchasable",
            }

            ops = []
            total = len(df)

            for idx, row in df.iterrows():
                fund_code = str(row.get("Âü∫Èáë‰ª£Á†Å", "")).strip()
                if not fund_code or fund_code == "nan":
                    continue

                # ÊûÑÂª∫ËÆ∞ÂΩï
                record = {
                    "fund_code": fund_code,
                    "date": current_date,
                }

                # Êò†Â∞ÑÂÖ∂‰ªñÂ≠óÊÆµ
                for cn_field, en_field in field_map.items():
                    if cn_field == "Âü∫Èáë‰ª£Á†Å":  # Â∑≤Â§ÑÁêÜ
                        continue
                    
                    value = row.get(cn_field)
                    if pd.notna(value):
                        value_str = str(value).strip()
                        # Ë∑≥Ëøá "---" Á≠âÊó†ÊïàÂÄº
                        if value_str and value_str != "---" and value_str != "nan":
                            # Â§ÑÁêÜÁôæÂàÜÊØîÂ≠óÊÆµ
                            if "%" in cn_field or cn_field == "Êó•Ê∂®ÂπÖ":
                                # ‰øùÁïôÂéüÂßãÂ≠óÁ¨¶‰∏≤Ê†ºÂºè
                                record[en_field] = value_str
                            else:
                                # Â∞ùËØïËΩ¨Êç¢Êï∞ÂÄº
                                try:
                                    if isinstance(value, (int, float)):
                                        record[en_field] = float(value)
                                    else:
                                        record[en_field] = value_str
                                except:
                                    record[en_field] = value_str

                # ÂîØ‰∏ÄÈîÆÔºöfund_code + date
                ops.append(
                    UpdateOne(
                        {"fund_code": fund_code, "date": current_date},
                        {"$set": record},
                        upsert=True,
                    )
                )

                # ËøõÂ∫¶ÂõûË∞É
                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            if not ops:
                logger.warning("Ê≤°ÊúâÊúâÊïàÊï∞ÊçÆÂèØ‰øùÂ≠ò")
                return 0

            result = await self.col_fund_money_fund_daily_em.bulk_write(ops, ordered=False)
            saved_count = (result.upserted_count or 0) + (result.modified_count or 0)

            logger.info(f"ÊàêÂäü‰øùÂ≠ò {saved_count} Êù°Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return saved_count

        except Exception as e:
            logger.error(f"‰øùÂ≠òË¥ßÂ∏ÅÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def clear_fund_money_fund_daily_data(self) -> int:
        """Ê∏ÖÁ©∫Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ

        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_money_fund_daily_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def get_fund_money_fund_daily_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñË¥ßÂ∏ÅÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°‰ø°ÊÅØ

        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏ÔºåÂåÖÊã¨ÊÄªÊï∞„ÄÅÂêÑÂü∫Èáë‰ª£Á†ÅËÆ∞ÂΩïÊï∞„ÄÅÊúÄÊó©/ÊúÄÊñ∞Êó•Êúü
        """
        try:
            total_count = await self.col_fund_money_fund_daily_em.count_documents({})

            # ÊåâÂü∫Èáë‰ª£Á†ÅÁªüËÆ°ËÆ∞ÂΩïÊï∞ÔºàTop 50Ôºâ
            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 50},
            ]
            fund_code_stats = await self.col_fund_money_fund_daily_em.aggregate(
                fund_code_pipeline
            ).to_list(50)

            # ÊåâÊó•ÊúüÁªüËÆ°ËÆ∞ÂΩïÊï∞
            date_pipeline = [
                {"$group": {"_id": "$date", "count": {"$sum": 1}}},
                {"$project": {"date": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"date": -1}},
                {"$limit": 30},
            ]
            date_stats = await self.col_fund_money_fund_daily_em.aggregate(
                date_pipeline
            ).to_list(30)

            # Ëé∑ÂèñÊúÄÊó©ÂíåÊúÄÊñ∞Êó•Êúü
            earliest_doc = (
                await self.col_fund_money_fund_daily_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_money_fund_daily_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            earliest_date = earliest_doc[0]["date"] if earliest_doc else None
            latest_date = latest_doc[0]["date"] if latest_doc else None

            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "date_stats": date_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            logger.debug(f"Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°: {result}")
            return result

        except Exception as e:
            logger.error(f"Ëé∑ÂèñË¥ßÂ∏ÅÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "date_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_money_fund_info_data(
        self, df: pd.DataFrame, fund_code: str, progress_callback=None
    ) -> int:
        """‰øùÂ≠òË¥ßÂ∏ÅÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ

        Args:
            df: ÂåÖÂê´ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÁöÑ DataFrame
            fund_code: Âü∫Èáë‰ª£Á†Å
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞

        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning(f"Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ‰∏∫Á©∫Ôºà{fund_code}Ôºâ")
            return 0

        try:
            df = df.copy()
            df.columns = df.columns.str.strip()

            # Â≠óÊÆµÊò†Â∞Ñ
            field_map = {
                "ÂáÄÂÄºÊó•Êúü": "date",
                "ÊØè‰∏á‰ªΩÊî∂Áõä": "daily_profit_per_10k",
                "7Êó•Âπ¥ÂåñÊî∂ÁõäÁéá": "seven_day_annual_yield",
                "Áî≥Ë¥≠Áä∂ÊÄÅ": "purchase_status",
                "ËµéÂõûÁä∂ÊÄÅ": "redemption_status",
            }

            ops = []
            total = len(df)

            for idx, row in df.iterrows():
                date_value = str(row.get("ÂáÄÂÄºÊó•Êúü", "")).strip()
                if not date_value or date_value == "nan":
                    continue

                record = {
                    "fund_code": fund_code,
                    "date": date_value,
                }

                # Êò†Â∞ÑÂÖ∂‰ªñÂ≠óÊÆµ
                for cn_field, en_field in field_map.items():
                    if cn_field == "ÂáÄÂÄºÊó•Êúü":
                        continue
                    value = row.get(cn_field)
                    if pd.notna(value):
                        if isinstance(value, (int, float)):
                            record[en_field] = float(value)
                        else:
                            record[en_field] = str(value).strip()

                ops.append(
                    UpdateOne(
                        {"fund_code": fund_code, "date": date_value},
                        {"$set": record},
                        upsert=True,
                    )
                )

                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            if not ops:
                logger.warning("Ê≤°ÊúâÊúâÊïàÊï∞ÊçÆÂèØ‰øùÂ≠ò")
                return 0

            result = await self.col_fund_money_fund_info_em.bulk_write(ops, ordered=False)
            saved_count = (result.upserted_count or 0) + (result.modified_count or 0)

            logger.info(f"ÊàêÂäü‰øùÂ≠ò {saved_count} Êù°Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÔºà{fund_code}Ôºâ")
            return saved_count

        except Exception as e:
            logger.error(f"‰øùÂ≠òË¥ßÂ∏ÅÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def clear_fund_money_fund_info_data(self) -> int:
        """Ê∏ÖÁ©∫Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ

        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_money_fund_info_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def get_fund_money_fund_info_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñË¥ßÂ∏ÅÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°‰ø°ÊÅØ

        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_count = await self.col_fund_money_fund_info_em.count_documents({})

            # ÊåâÂü∫Èáë‰ª£Á†ÅÁªüËÆ°ËÆ∞ÂΩïÊï∞ÔºàTop 50Ôºâ
            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 50},
            ]
            fund_code_stats = await self.col_fund_money_fund_info_em.aggregate(
                fund_code_pipeline
            ).to_list(50)

            # Ëé∑ÂèñÊúÄÊó©ÂíåÊúÄÊñ∞Êó•Êúü
            earliest_doc = (
                await self.col_fund_money_fund_info_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_money_fund_info_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            earliest_date = earliest_doc[0]["date"] if earliest_doc else None
            latest_date = latest_doc[0]["date"] if latest_doc else None

            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            return result

        except Exception as e:
            logger.error(f"Ëé∑ÂèñË¥ßÂ∏ÅÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_financial_fund_daily_data(
        self, df: pd.DataFrame, progress_callback=None
    ) -> int:
        """‰øùÂ≠òÁêÜË¥¢ÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ

        Args:
            df: ÂåÖÂê´ÁêÜË¥¢ÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÁöÑ DataFrame
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞

        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("ÁêÜË¥¢ÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ‰∏∫Á©∫")
            return 0

        try:
            df = df.copy()
            from datetime import datetime
            current_date = datetime.now().strftime("%Y-%m-%d")
            df.columns = df.columns.str.strip()

            field_map = {
                "Â∫èÂè∑": "sequence",
                "Âü∫Èáë‰ª£Á†Å": "fund_code",
                "Âü∫ÈáëÁÆÄÁß∞": "fund_name",
                "‰∏ä‰∏ÄÊúüÂπ¥ÂåñÊî∂ÁõäÁéá": "last_period_annual_yield",
                "ÂΩìÂâç‰∫§ÊòìÊó•-‰∏á‰ªΩÊî∂Áõä": "current_daily_profit_per_10k",
                "ÂΩìÂâç‰∫§ÊòìÊó•-7Êó•Âπ¥Âçé": "current_7day_annual_yield",
                "Ââç‰∏Ä‰∏™‰∫§ÊòìÊó•-‰∏á‰ªΩÊî∂Áõä": "prev_daily_profit_per_10k",
                "Ââç‰∏Ä‰∏™‰∫§ÊòìÊó•-7Êó•Âπ¥Âçé": "prev_7day_annual_yield",
                "Â∞ÅÈó≠Êúü": "closed_period",
                "Áî≥Ë¥≠Áä∂ÊÄÅ": "purchase_status",
            }

            ops = []
            total = len(df)

            for idx, row in df.iterrows():
                fund_code = str(row.get("Âü∫Èáë‰ª£Á†Å", "")).strip()
                if not fund_code or fund_code == "nan":
                    continue

                record = {"fund_code": fund_code, "date": current_date}

                for cn_field, en_field in field_map.items():
                    if cn_field == "Âü∫Èáë‰ª£Á†Å":
                        continue
                    
                    value = row.get(cn_field)
                    if pd.notna(value):
                        value_str = str(value).strip()
                        if value_str and value_str != "---" and value_str != "nan":
                            try:
                                if isinstance(value, (int, float)):
                                    record[en_field] = float(value)
                                else:
                                    record[en_field] = value_str
                            except:
                                record[en_field] = value_str

                ops.append(
                    UpdateOne(
                        {"fund_code": fund_code, "date": current_date},
                        {"$set": record},
                        upsert=True,
                    )
                )

                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            if not ops:
                logger.warning("Ê≤°ÊúâÊúâÊïàÊï∞ÊçÆÂèØ‰øùÂ≠ò")
                return 0

            result = await self.col_fund_financial_fund_daily_em.bulk_write(ops, ordered=False)
            saved_count = (result.upserted_count or 0) + (result.modified_count or 0)

            logger.info(f"ÊàêÂäü‰øùÂ≠ò {saved_count} Êù°ÁêÜË¥¢ÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return saved_count

        except Exception as e:
            logger.error(f"‰øùÂ≠òÁêÜË¥¢ÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def clear_fund_financial_fund_daily_data(self) -> int:
        """Ê∏ÖÁ©∫ÁêÜË¥¢ÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ

        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_financial_fund_daily_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÁêÜË¥¢ÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÁêÜË¥¢ÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def get_fund_financial_fund_daily_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÁêÜË¥¢ÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°‰ø°ÊÅØ

        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏ÔºåÂåÖÊã¨ÊÄªÊï∞„ÄÅÂêÑÂü∫Èáë‰ª£Á†ÅËÆ∞ÂΩïÊï∞„ÄÅÊúÄÊó©/ÊúÄÊñ∞Êó•Êúü
        """
        try:
            total_count = await self.col_fund_financial_fund_daily_em.count_documents({})

            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 50},
            ]
            fund_code_stats = await self.col_fund_financial_fund_daily_em.aggregate(
                fund_code_pipeline
            ).to_list(50)

            date_pipeline = [
                {"$group": {"_id": "$date", "count": {"$sum": 1}}},
                {"$project": {"date": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"date": -1}},
                {"$limit": 30},
            ]
            date_stats = await self.col_fund_financial_fund_daily_em.aggregate(
                date_pipeline
            ).to_list(30)

            earliest_doc = (
                await self.col_fund_financial_fund_daily_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_financial_fund_daily_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            earliest_date = earliest_doc[0]["date"] if earliest_doc else None
            latest_date = latest_doc[0]["date"] if latest_doc else None

            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "date_stats": date_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            logger.debug(f"ÁêÜË¥¢ÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°: {result}")
            return result

        except Exception as e:
            logger.error(f"Ëé∑ÂèñÁêÜË¥¢ÂûãÂü∫ÈáëÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "date_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_financial_fund_info_data(
        self, df: pd.DataFrame, fund_code: str, progress_callback=None
    ) -> int:
        """‰øùÂ≠òÁêÜË¥¢ÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ

        Args:
            df: ÂåÖÂê´ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÁöÑ DataFrame
            fund_code: Âü∫Èáë‰ª£Á†Å
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞

        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning(f"ÁêÜË¥¢ÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ‰∏∫Á©∫Ôºà{fund_code}Ôºâ")
            return 0

        try:
            df = df.copy()
            df.columns = df.columns.str.strip()

            # Â≠óÊÆµÊò†Â∞Ñ
            field_map = {
                "ÂáÄÂÄºÊó•Êúü": "date",
                "Âçï‰ΩçÂáÄÂÄº": "unit_net_value",
                "Á¥ØËÆ°ÂáÄÂÄº": "accumulative_net_value",
                "Êó•Â¢ûÈïøÁéá": "daily_growth_rate",
                "Áî≥Ë¥≠Áä∂ÊÄÅ": "purchase_status",
                "ËµéÂõûÁä∂ÊÄÅ": "redemption_status",
                "ÂàÜÁ∫¢ÈÄÅÈÖç": "dividend_distribution",
            }

            ops = []
            total = len(df)

            for idx, row in df.iterrows():
                date_value = str(row.get("ÂáÄÂÄºÊó•Êúü", "")).strip()
                if not date_value or date_value == "nan":
                    continue

                record = {
                    "fund_code": fund_code,
                    "date": date_value,
                }

                # Êò†Â∞ÑÂÖ∂‰ªñÂ≠óÊÆµ
                for cn_field, en_field in field_map.items():
                    if cn_field == "ÂáÄÂÄºÊó•Êúü":
                        continue
                    value = row.get(cn_field)
                    if pd.notna(value):
                        if isinstance(value, (int, float)):
                            record[en_field] = float(value)
                        else:
                            record[en_field] = str(value).strip()

                ops.append(
                    UpdateOne(
                        {"fund_code": fund_code, "date": date_value},
                        {"$set": record},
                        upsert=True,
                    )
                )

                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            if not ops:
                logger.warning("Ê≤°ÊúâÊúâÊïàÊï∞ÊçÆÂèØ‰øùÂ≠ò")
                return 0

            result = await self.col_fund_financial_fund_info_em.bulk_write(ops, ordered=False)
            saved_count = (result.upserted_count or 0) + (result.modified_count or 0)

            logger.info(f"ÊàêÂäü‰øùÂ≠ò {saved_count} Êù°ÁêÜË¥¢ÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÔºà{fund_code}Ôºâ")
            return saved_count

        except Exception as e:
            logger.error(f"‰øùÂ≠òÁêÜË¥¢ÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def clear_fund_financial_fund_info_data(self) -> int:
        """Ê∏ÖÁ©∫ÁêÜË¥¢ÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ

        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_financial_fund_info_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÁêÜË¥¢ÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÁêÜË¥¢ÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def get_fund_financial_fund_info_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÁêÜË¥¢ÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°‰ø°ÊÅØ

        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_count = await self.col_fund_financial_fund_info_em.count_documents({})

            # ÊåâÂü∫Èáë‰ª£Á†ÅÁªüËÆ°ËÆ∞ÂΩïÊï∞ÔºàTop 50Ôºâ
            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 50},
            ]
            fund_code_stats = await self.col_fund_financial_fund_info_em.aggregate(
                fund_code_pipeline
            ).to_list(50)

            # Ëé∑ÂèñÊúÄÊó©ÂíåÊúÄÊñ∞Êó•Êúü
            earliest_doc = (
                await self.col_fund_financial_fund_info_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_financial_fund_info_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            earliest_date = earliest_doc[0]["date"] if earliest_doc else None
            latest_date = latest_doc[0]["date"] if latest_doc else None

            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            return result

        except Exception as e:
            logger.error(f"Ëé∑ÂèñÁêÜË¥¢ÂûãÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_graded_fund_daily_data(
        self, df: pd.DataFrame, progress_callback=None
    ) -> int:
        """‰øùÂ≠òÂàÜÁ∫ßÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆ

        Args:
            df: ÂåÖÂê´ÂàÜÁ∫ßÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆÁöÑ DataFrame
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞

        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("ÂàÜÁ∫ßÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆ‰∏∫Á©∫")
            return 0

        try:
            df = df.copy()
            from datetime import datetime
            current_date = datetime.now().strftime("%Y-%m-%d")
            df.columns = df.columns.str.strip()

            field_map = {
                "Âü∫Èáë‰ª£Á†Å": "fund_code",
                "Âü∫ÈáëÁÆÄÁß∞": "fund_name",
                "Âçï‰ΩçÂáÄÂÄº": "unit_net_value",
                "Á¥ØËÆ°ÂáÄÂÄº": "accumulative_net_value",
                "Ââç‰∫§ÊòìÊó•-Âçï‰ΩçÂáÄÂÄº": "prev_unit_net_value",
                "Ââç‰∫§ÊòìÊó•-Á¥ØËÆ°ÂáÄÂÄº": "prev_accumulative_net_value",
                "Êó•Â¢ûÈïøÂÄº": "daily_growth_value",
                "Êó•Â¢ûÈïøÁéá": "daily_growth_rate",
                "Â∏Ç‰ª∑": "market_price",
                "Êäò‰ª∑Áéá": "discount_rate",
                "ÊâãÁª≠Ë¥π": "fee",
            }

            ops = []
            total = len(df)

            for idx, row in df.iterrows():
                fund_code = str(row.get("Âü∫Èáë‰ª£Á†Å", "")).strip()
                if not fund_code or fund_code == "nan":
                    continue

                record = {"fund_code": fund_code, "date": current_date}

                for cn_field, en_field in field_map.items():
                    if cn_field == "Âü∫Èáë‰ª£Á†Å":
                        continue
                    value = row.get(cn_field)
                    if pd.notna(value):
                        value_str = str(value).strip()
                        if value_str and value_str != "---" and value_str != "nan":
                            try:
                                if isinstance(value, (int, float)):
                                    record[en_field] = float(value)
                                else:
                                    record[en_field] = value_str
                            except:
                                record[en_field] = value_str

                ops.append(
                    UpdateOne(
                        {"fund_code": fund_code, "date": current_date},
                        {"$set": record},
                        upsert=True,
                    )
                )

                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            if not ops:
                return 0

            result = await self.col_fund_graded_fund_daily_em.bulk_write(ops, ordered=False)
            saved_count = (result.upserted_count or 0) + (result.modified_count or 0)
            logger.info(f"ÊàêÂäü‰øùÂ≠ò {saved_count} Êù°ÂàÜÁ∫ßÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆ")
            return saved_count

        except Exception as e:
            logger.error(f"‰øùÂ≠òÂàÜÁ∫ßÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def clear_fund_graded_fund_daily_data(self) -> int:
        """Ê∏ÖÁ©∫ÂàÜÁ∫ßÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆ

        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_graded_fund_daily_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÂàÜÁ∫ßÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÂàÜÁ∫ßÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def get_fund_graded_fund_daily_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂàÜÁ∫ßÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆÁªüËÆ°‰ø°ÊÅØ

        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_count = await self.col_fund_graded_fund_daily_em.count_documents({})

            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 50},
            ]
            fund_code_stats = await self.col_fund_graded_fund_daily_em.aggregate(
                fund_code_pipeline
            ).to_list(50)

            date_pipeline = [
                {"$group": {"_id": "$date", "count": {"$sum": 1}}},
                {"$project": {"date": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"date": -1}},
                {"$limit": 30},
            ]
            date_stats = await self.col_fund_graded_fund_daily_em.aggregate(
                date_pipeline
            ).to_list(30)

            earliest_doc = (
                await self.col_fund_graded_fund_daily_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_graded_fund_daily_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "date_stats": date_stats,
                "earliest_date": earliest_doc[0]["date"] if earliest_doc else None,
                "latest_date": latest_doc[0]["date"] if latest_doc else None,
            }

            return result

        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂàÜÁ∫ßÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "date_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_graded_fund_info_data(
        self, df: pd.DataFrame, fund_code: str, progress_callback=None
    ) -> int:
        """‰øùÂ≠òÂàÜÁ∫ßÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆ

        Args:
            df: ÂåÖÂê´ÂéÜÂè≤Êï∞ÊçÆÁöÑ DataFrame
            fund_code: Âü∫Èáë‰ª£Á†Å
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞

        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning(f"ÂàÜÁ∫ßÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆ‰∏∫Á©∫Ôºà{fund_code}Ôºâ")
            return 0

        try:
            df = df.copy()
            df.columns = df.columns.str.strip()

            # Â≠óÊÆµÊò†Â∞Ñ
            field_map = {
                "ÂáÄÂÄºÊó•Êúü": "date",
                "Âçï‰ΩçÂáÄÂÄº": "unit_net_value",
                "Á¥ØËÆ°ÂáÄÂÄº": "accumulative_net_value",
                "Êó•Â¢ûÈïøÁéá": "daily_growth_rate",
                "Áî≥Ë¥≠Áä∂ÊÄÅ": "purchase_status",
                "ËµéÂõûÁä∂ÊÄÅ": "redemption_status",
            }

            ops = []
            total = len(df)

            for idx, row in df.iterrows():
                date_value = str(row.get("ÂáÄÂÄºÊó•Êúü", "")).strip()
                if not date_value or date_value == "nan":
                    continue

                record = {
                    "fund_code": fund_code,
                    "date": date_value,
                }

                # Êò†Â∞ÑÂÖ∂‰ªñÂ≠óÊÆµ
                for cn_field, en_field in field_map.items():
                    if cn_field == "ÂáÄÂÄºÊó•Êúü":
                        continue
                    value = row.get(cn_field)
                    if pd.notna(value):
                        if isinstance(value, (int, float)):
                            record[en_field] = float(value)
                        else:
                            record[en_field] = str(value).strip()

                ops.append(
                    UpdateOne(
                        {"fund_code": fund_code, "date": date_value},
                        {"$set": record},
                        upsert=True,
                    )
                )

                if progress_callback and (idx + 1) % 100 == 0:
                    await progress_callback(idx + 1, total)

            if not ops:
                logger.warning("Ê≤°ÊúâÊúâÊïàÊï∞ÊçÆÂèØ‰øùÂ≠ò")
                return 0

            result = await self.col_fund_graded_fund_info_em.bulk_write(ops, ordered=False)
            saved_count = (result.upserted_count or 0) + (result.modified_count or 0)

            logger.info(f"ÊàêÂäü‰øùÂ≠ò {saved_count} Êù°ÂàÜÁ∫ßÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆÔºà{fund_code}Ôºâ")
            return saved_count

        except Exception as e:
            logger.error(f"‰øùÂ≠òÂàÜÁ∫ßÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def clear_fund_graded_fund_info_data(self) -> int:
        """Ê∏ÖÁ©∫ÂàÜÁ∫ßÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆ

        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_graded_fund_info_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÂàÜÁ∫ßÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÂàÜÁ∫ßÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def get_fund_graded_fund_info_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂàÜÁ∫ßÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆÁªüËÆ°‰ø°ÊÅØ

        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_count = await self.col_fund_graded_fund_info_em.count_documents({})

            # ÊåâÂü∫Èáë‰ª£Á†ÅÁªüËÆ°ËÆ∞ÂΩïÊï∞ÔºàTop 50Ôºâ
            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 50},
            ]
            fund_code_stats = await self.col_fund_graded_fund_info_em.aggregate(
                fund_code_pipeline
            ).to_list(50)

            # Ëé∑ÂèñÊúÄÊó©ÂíåÊúÄÊñ∞Êó•Êúü
            earliest_doc = (
                await self.col_fund_graded_fund_info_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_graded_fund_info_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            earliest_date = earliest_doc[0]["date"] if earliest_doc else None
            latest_date = latest_doc[0]["date"] if latest_doc else None

            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "earliest_date": earliest_date,
                "latest_date": latest_date,
            }

            return result

        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂàÜÁ∫ßÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }

    async def save_fund_etf_fund_daily_data(
        self, df: pd.DataFrame, progress_callback=None
    ) -> int:
        """‰øùÂ≠òÂú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆ

        Args:
            df: ÂåÖÂê´Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆÁöÑ DataFrame
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞

        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆ‰∏∫Á©∫")
            return 0

        try:
            import numpy as np
            # Ê∏ÖÁêÜÊó†ÊïàÁöÑÊµÆÁÇπÊï∞ÂÄº
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            df = df.copy()
            
            current_date = datetime.now().strftime("%Y-%m-%d")
            df.columns = df.columns.str.strip()

            total = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total} Êù°Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆ...")
            
            # ÂàÜÊâπÂ§ÑÁêÜÔºåÊØèÊâπ500Êù°
            batch_size = 500
            total_saved = 0
            total_batches = (total + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"üìù Â§ÑÁêÜÁ¨¨ {batch_idx + 1}/{total_batches} ÊâπÔºåËÆ∞ÂΩïËåÉÂõ¥: {start_idx + 1}-{end_idx}")
                
                ops = []
                for idx, row in batch_df.iterrows():
                    fund_code = str(row.get("Âü∫Èáë‰ª£Á†Å", "")).strip()
                    if not fund_code or fund_code == "nan":
                        continue

                    record = {
                        "fund_code": fund_code,
                        "date": current_date,
                        "source": "akshare",
                        "endpoint": "fund_etf_fund_daily_em",
                        "updated_at": datetime.now().isoformat()
                    }

                    # ÈùôÊÄÅÂ≠óÊÆµÊò†Â∞Ñ
                    static_fields = {
                        "Âü∫ÈáëÁÆÄÁß∞": "fund_name",
                        "Á±ªÂûã": "fund_type",
                        "Â¢ûÈïøÂÄº": "growth_value",
                        "Â¢ûÈïøÁéá": "growth_rate",
                        "Â∏Ç‰ª∑": "market_price",
                        "Êäò‰ª∑Áéá": "discount_rate",
                    }

                    for cn_field, en_field in static_fields.items():
                        value = row.get(cn_field)
                        if pd.notna(value):
                            value_str = str(value).strip()
                            if value_str and value_str != "---" and value_str != "nan":
                                try:
                                    # Â∞ùËØïËΩ¨Êç¢‰∏∫Êï∞ÂÄºÁ±ªÂûã
                                    if isinstance(value, (int, float)):
                                        record[en_field] = float(value)
                                    else:
                                        record[en_field] = value_str
                                except:
                                    record[en_field] = value_str

                    # Â§ÑÁêÜÂä®ÊÄÅÊó•ÊúüÂ≠óÊÆµ - ÊîπËøõÁâàÊú¨
                    # Êü•ÊâæÂΩìÂâç‰∫§ÊòìÊó•ÂíåÂâç‰∏Ä‰∏™‰∫§ÊòìÊó•ÁöÑÊï∞ÊçÆ
                    current_date_fields = {}
                    prev_date_fields = {}
                    
                    for col in df.columns:
                        col_str = str(col).strip()
                        value = row.get(col)
                        
                        if pd.notna(value) and str(value).strip() not in ["", "---", "nan"]:
                            try:
                                if "-Âçï‰ΩçÂáÄÂÄº" in col_str:
                                    date_part = col_str.split("-")[0]
                                    float_val = float(value)
                                    current_date_fields[f"{date_part}_unit_net_value"] = float_val
                                    # ÂÅáËÆæÊúÄÂêé‰∏Ä‰∏™ÊòØÂΩìÂâç‰∫§ÊòìÊó•
                                    record["current_unit_net_value"] = float_val
                                elif "-Á¥ØËÆ°ÂáÄÂÄº" in col_str:
                                    date_part = col_str.split("-")[0]
                                    float_val = float(value)
                                    current_date_fields[f"{date_part}_accumulative_net_value"] = float_val
                                    record["current_accumulative_net_value"] = float_val
                            except (ValueError, TypeError):
                                pass
                    
                    # Â≠òÂÇ®ÊâÄÊúâÂä®ÊÄÅÊó•ÊúüÂ≠óÊÆµ
                    if current_date_fields:
                        record["date_fields"] = current_date_fields

                    ops.append(
                        UpdateOne(
                            {"fund_code": fund_code, "date": current_date},
                            {"$set": record},
                            upsert=True,
                        )
                    )

                # ÊâßË°åÊâπÈáèÂÜôÂÖ•
                if ops:
                    result = await self.col_fund_etf_fund_daily_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"‚úÖ Á¨¨ {batch_idx + 1}/{total_batches} ÊâπÂÜôÂÖ•ÂÆåÊàê: "
                        f"Êñ∞Â¢û={result.upserted_count}, Êõ¥Êñ∞={result.matched_count}, "
                        f"Êú¨Êâπ‰øùÂ≠ò={batch_saved}, Á¥ØËÆ°={total_saved}/{total}"
                    )
                    
                    if progress_callback:
                        progress = int((end_idx / total) * 100)
                        await progress_callback(
                            current=end_idx,
                            total=total,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total} Êù°Êï∞ÊçÆ ({progress}%)"
                        )

            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total} Êù°Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆ")
            return total_saved

        except Exception as e:
            logger.error(f"‰øùÂ≠òÂú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def clear_fund_etf_fund_daily_data(self) -> int:
        """Ê∏ÖÁ©∫Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆ

        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_etf_fund_daily_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise

    async def get_fund_etf_fund_daily_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆÁªüËÆ°‰ø°ÊÅØ

        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_count = await self.col_fund_etf_fund_daily_em.count_documents({})

            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 50},
            ]
            fund_code_stats = await self.col_fund_etf_fund_daily_em.aggregate(
                fund_code_pipeline
            ).to_list(50)

            date_pipeline = [
                {"$group": {"_id": "$date", "count": {"$sum": 1}}},
                {"$project": {"date": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"date": -1}},
                {"$limit": 30},
            ]
            date_stats = await self.col_fund_etf_fund_daily_em.aggregate(
                date_pipeline
            ).to_list(30)

            earliest_doc = (
                await self.col_fund_etf_fund_daily_em.find({}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_etf_fund_daily_em.find({}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )

            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "date_stats": date_stats,
                "earliest_date": earliest_doc[0]["date"] if earliest_doc else None,
                "latest_date": latest_doc[0]["date"] if latest_doc else None,
            }

            return result

        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂÆûÊó∂Êï∞ÊçÆÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "date_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }
    
    # ========== È¶ôÊ∏ØÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆ ==========
    async def save_fund_hk_hist_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÈ¶ôÊ∏ØÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆÔºàÂéÜÂè≤ÂáÄÂÄºÊòéÁªÜÊàñÂàÜÁ∫¢ÈÄÅÈÖçËØ¶ÊÉÖÔºâ
        
        Args:
            df: ÂåÖÂê´È¶ôÊ∏ØÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆÁöÑDataFrame
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞
            
        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÈ¶ôÊ∏ØÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°È¶ôÊ∏ØÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆ...")
            
            # ÂàÜÊâπÂ§ÑÁêÜÔºåÊØèÊâπ500Êù°
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"üìù Â§ÑÁêÜÁ¨¨ {batch_idx + 1}/{total_batches} ÊâπÔºåËÆ∞ÂΩïËåÉÂõ¥: {start_idx + 1}-{end_idx}")
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # Ê∏ÖÁêÜNaN/InfinityÂÄº
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    fund_code = str(doc.get('code', ''))
                    symbol = str(doc.get('symbol', 'ÂéÜÂè≤ÂáÄÂÄºÊòéÁªÜ'))
                    
                    # Á°ÆÂÆöÊó•ÊúüÂ≠óÊÆµ
                    date_field = None
                    if 'ÂáÄÂÄºÊó•Êúü' in doc:
                        date_field = str(doc.get('ÂáÄÂÄºÊó•Êúü', ''))
                    elif 'Èô§ÊÅØÊó•' in doc:
                        date_field = str(doc.get('Èô§ÊÅØÊó•', ''))
                    
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_hk_fund_hist_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ÊûÑÂª∫ÂîØ‰∏ÄÊ†áËØÜÔºàcode + date + symbolÔºâ
                    filter_query = {'code': fund_code, 'symbol': symbol}
                    if date_field:
                        filter_query['date'] = date_field
                        doc['date'] = date_field
                    
                    ops.append(
                        UpdateOne(
                            filter_query,
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # ÊâßË°åÊâπÈáèÂÜôÂÖ•
                if ops:
                    result = await self.col_fund_hk_hist_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"‚úÖ Á¨¨ {batch_idx + 1}/{total_batches} ÊâπÂÜôÂÖ•ÂÆåÊàê: "
                        f"Êñ∞Â¢û={result.upserted_count}, Êõ¥Êñ∞={result.matched_count}, "
                        f"Êú¨Êâπ‰øùÂ≠ò={batch_saved}, Á¥ØËÆ°={total_saved}/{total_count}"
                    )
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        await progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°È¶ôÊ∏ØÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆ")
            return total_saved
            
        except Exception as e:
            logger.error(f"‰øùÂ≠òÈ¶ôÊ∏ØÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_hk_hist_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÈ¶ôÊ∏ØÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆÁªüËÆ°‰ø°ÊÅØ
        
        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_count = await self.col_fund_hk_hist_em.count_documents({})
            
            # Âü∫ÈáëÊï∞ÈáèÁªüËÆ°
            fund_count_pipeline = [
                {"$group": {"_id": "$code"}},
                {"$count": "count"}
            ]
            fund_count_result = await self.col_fund_hk_hist_em.aggregate(fund_count_pipeline).to_list(1)
            fund_count = fund_count_result[0]["count"] if fund_count_result else 0
            
            # symbolÂàÜÂ∏ÉÁªüËÆ°
            symbol_pipeline = [
                {"$group": {"_id": "$symbol", "count": {"$sum": 1}}},
                {"$project": {"symbol": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}}
            ]
            symbol_stats = await self.col_fund_hk_hist_em.aggregate(symbol_pipeline).to_list(10)
            
            # Êó•ÊúüËåÉÂõ¥ÁªüËÆ°
            earliest_doc = (
                await self.col_fund_hk_hist_em.find({"date": {"$exists": True, "$ne": None}}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_hk_hist_em.find({"date": {"$exists": True, "$ne": None}}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )
            
            # Âü∫Èáë‰ª£Á†ÅÂàÜÂ∏É
            fund_code_pipeline = [
                {"$group": {"_id": "$code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 20}
            ]
            fund_code_stats = await self.col_fund_hk_hist_em.aggregate(fund_code_pipeline).to_list(20)
            
            result = {
                "total_count": total_count,
                "fund_count": fund_count,
                "symbol_distribution": symbol_stats,
                "fund_code_stats": fund_code_stats,
                "earliest_date": earliest_doc[0]["date"] if earliest_doc else None,
                "latest_date": latest_doc[0]["date"] if latest_doc else None,
            }
            
            return result
            
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÈ¶ôÊ∏ØÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_count": 0,
                "symbol_distribution": [],
                "fund_code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }
    
    async def import_fund_hk_hist_em_from_file(self, content: bytes, filename: Optional[str] = None) -> Dict[str, Any]:
        """‰ªéÊñá‰ª∂ÂØºÂÖ•È¶ôÊ∏ØÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆ
        
        Args:
            content: Êñá‰ª∂ÂÜÖÂÆπÔºàÂ≠óËäÇÔºâ
            filename: Êñá‰ª∂Âêç
            
        Returns:
            ÂØºÂÖ•ÁªìÊûúÂ≠óÂÖ∏
        """
        if not content:
            raise ValueError("‰∏ä‰º†Êñá‰ª∂‰∏∫Á©∫")
        
        name = (filename or "").lower()
        buffer = io.BytesIO(content)
        
        df: Optional[pd.DataFrame] = None
        try:
            if name.endswith(".csv") or name.endswith(".txt"):
                df = pd.read_csv(buffer)
            elif name.endswith(".xls") or name.endswith(".xlsx"):
                df = pd.read_excel(buffer)
            else:
                try:
                    df = pd.read_csv(buffer)
                except Exception:
                    buffer.seek(0)
                    df = pd.read_excel(buffer)
        except Exception as e:
            logger.error(f"‚ùå [fund_hk_hist_em ÂØºÂÖ•] ËØªÂèñÊñá‰ª∂Â§±Ë¥•: {e}", exc_info=True)
            raise ValueError("Êó†Ê≥ïËß£Êûê‰∏ä‰º†Êñá‰ª∂ÔºåËØ∑Á°ÆËÆ§‰∏∫ÊúâÊïàÁöÑ CSV Êàñ Excel Êñá‰ª∂")
        
        if df is None or df.empty:
            logger.warning("‚ö†Ô∏è [fund_hk_hist_em ÂØºÂÖ•] Ëß£ÊûêÁªìÊûú‰∏∫Á©∫ DataFrame")
            return {"saved": 0, "rows": 0}
        
        rows = len(df)
        saved = await self.save_fund_hk_hist_em_data(df)
        logger.info(f"üíæ [fund_hk_hist_em ÂØºÂÖ•] ‰ªéÊñá‰ª∂ {filename} ÂØºÂÖ• {rows} Ë°åÔºå‰øùÂ≠ò {saved} Êù°ËÆ∞ÂΩï")
        
        return {"saved": saved, "rows": rows}
    
    async def sync_fund_hk_hist_em_from_remote(
        self,
        remote_host: str,
        batch_size: int = 1000,
        remote_collection: Optional[str] = None,
        remote_username: Optional[str] = None,
        remote_password: Optional[str] = None,
        remote_auth_source: Optional[str] = None,
    ) -> Dict[str, Any]:
        """‰ªéËøúÁ®ãMongoDBÂêåÊ≠•È¶ôÊ∏ØÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆ
        
        Args:
            remote_host: ËøúÁ®ã‰∏ªÊú∫Âú∞ÂùÄ
            batch_size: ÊâπÈáèÂ§ßÂ∞è
            remote_collection: ËøúÁ®ãÈõÜÂêàÂêçÁß∞
            remote_username: ËøúÁ®ãÁî®Êà∑Âêç
            remote_password: ËøúÁ®ãÂØÜÁ†Å
            remote_auth_source: ËÆ§ËØÅÊï∞ÊçÆÂ∫ì
            
        Returns:
            ÂêåÊ≠•ÁªìÊûúÂ≠óÂÖ∏
        """
        from motor.motor_asyncio import AsyncIOMotorClient
        
        if not remote_host:
            raise ValueError("ËøúÁ®ã‰∏ªÊú∫Âú∞ÂùÄ‰∏çËÉΩ‰∏∫Á©∫")
        
        # ÊûÑÂª∫ËøúÁ®ãËøûÊé•URI
        if remote_username and remote_password:
            auth_source = remote_auth_source or self.db.name
            remote_uri = f"mongodb://{remote_username}:{remote_password}@{remote_host}/?authSource={auth_source}"
        else:
            remote_uri = f"mongodb://{remote_host}"
        
        remote_client = None
        try:
            remote_client = AsyncIOMotorClient(remote_uri)
            remote_db = remote_client[self.db.name]
            remote_col_name = remote_collection or "fund_hk_hist_em"
            remote_col = remote_db[remote_col_name]
            
            # Ëé∑ÂèñËøúÁ®ãÊï∞ÊçÆÊÄªÊï∞
            remote_total = await remote_col.count_documents({})
            logger.info(f"üîó ËøúÁ®ãÈõÜÂêà {remote_col_name} ÂÖ±Êúâ {remote_total} Êù°Êï∞ÊçÆ")
            
            if remote_total == 0:
                return {"remote_total": 0, "synced": 0, "message": "ËøúÁ®ãÈõÜÂêà‰∏∫Á©∫"}
            
            # ÂàÜÊâπÂêåÊ≠•
            synced = 0
            skip = 0
            
            while skip < remote_total:
                cursor = remote_col.find({}).skip(skip).limit(batch_size)
                batch_docs = await cursor.to_list(batch_size)
                
                if not batch_docs:
                    break
                
                # ËΩ¨Êç¢‰∏∫DataFrameÂπ∂‰øùÂ≠ò
                df = pd.DataFrame(batch_docs)
                if '_id' in df.columns:
                    df = df.drop('_id', axis=1)
                
                batch_saved = await self.save_fund_hk_hist_em_data(df)
                synced += batch_saved
                skip += len(batch_docs)
                
                logger.info(f"üì• Â∑≤ÂêåÊ≠• {skip}/{remote_total} Êù°Êï∞ÊçÆ")
            
            logger.info(f"‚úÖ ÂêåÊ≠•ÂÆåÊàê: ËøúÁ®ã {remote_total} Êù°ÔºåÊú¨Âú∞‰øùÂ≠ò/Êõ¥Êñ∞ {synced} Êù°")
            
            return {
                "remote_total": remote_total,
                "synced": synced,
                "message": f"ÊàêÂäüÂêåÊ≠• {synced} Êù°È¶ôÊ∏ØÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆ"
            }
            
        except Exception as e:
            logger.error(f"‚ùå ‰ªéËøúÁ®ãÂêåÊ≠•È¶ôÊ∏ØÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
        finally:
            if remote_client:
                remote_client.close()
    
    async def clear_fund_hk_hist_em_data(self) -> int:
        """Ê∏ÖÁ©∫È¶ôÊ∏ØÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆ
        
        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_hk_hist_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°È¶ôÊ∏ØÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫È¶ôÊ∏ØÂü∫ÈáëÂéÜÂè≤Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    # ========== Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖ ==========
    async def save_fund_etf_fund_info_data(self, df: pd.DataFrame, fund_code: str = None, progress_callback=None) -> int:
        """‰øùÂ≠òÂú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ
        
        Args:
            df: ÂåÖÂê´ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÁöÑDataFrame
            fund_code: Âü∫Èáë‰ª£Á†Å
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞
            
        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ...")
            
            # ÂàÜÊâπÂ§ÑÁêÜÔºåÊØèÊâπ500Êù°
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"üìù Â§ÑÁêÜÁ¨¨ {batch_idx + 1}/{total_batches} ÊâπÔºåËÆ∞ÂΩïËåÉÂõ¥: {start_idx + 1}-{end_idx}")
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # Ê∏ÖÁêÜNaN/InfinityÂÄº
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    if fund_code:
                        doc['fund_code'] = fund_code
                    elif 'fund_code' not in doc:
                        doc['fund_code'] = doc.get('code', '')
                    
                    # Á°ÆÂÆöÊó•ÊúüÂ≠óÊÆµ
                    date_field = str(doc.get('ÂáÄÂÄºÊó•Êúü', ''))
                    doc['date'] = date_field
                    
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_etf_fund_info_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ÊûÑÂª∫ÂîØ‰∏ÄÊ†áËØÜÔºàfund_code + dateÔºâ
                    filter_query = {
                        'fund_code': doc['fund_code'],
                        'date': date_field
                    }
                    
                    ops.append(
                        UpdateOne(
                            filter_query,
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # ÊâßË°åÊâπÈáèÂÜôÂÖ•
                if ops:
                    result = await self.col_fund_etf_fund_info_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"‚úÖ Á¨¨ {batch_idx + 1}/{total_batches} ÊâπÂÜôÂÖ•ÂÆåÊàê: "
                        f"Êñ∞Â¢û={result.upserted_count}, Êõ¥Êñ∞={result.matched_count}, "
                        f"Êú¨Êâπ‰øùÂ≠ò={batch_saved}, Á¥ØËÆ°={total_saved}/{total_count}"
                    )
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        await progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ")
            return total_saved
            
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_etf_fund_info_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°‰ø°ÊÅØ
        
        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_count = await self.col_fund_etf_fund_info_em.count_documents({})
            
            # Âü∫Èáë‰ª£Á†ÅÂàÜÂ∏É
            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 20}
            ]
            fund_code_stats = await self.col_fund_etf_fund_info_em.aggregate(fund_code_pipeline).to_list(20)
            
            # Êó•ÊúüËåÉÂõ¥ÁªüËÆ°
            earliest_doc = (
                await self.col_fund_etf_fund_info_em.find({"date": {"$exists": True, "$ne": None}}, {"date": 1})
                .sort("date", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_etf_fund_info_em.find({"date": {"$exists": True, "$ne": None}}, {"date": 1})
                .sort("date", -1)
                .limit(1)
                .to_list(1)
            )
            
            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "earliest_date": earliest_doc[0]["date"] if earliest_doc else None,
                "latest_date": latest_doc[0]["date"] if latest_doc else None,
            }
            
            return result
            
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }
    
    async def clear_fund_etf_fund_info_data(self) -> int:
        """Ê∏ÖÁ©∫Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ
        
        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_etf_fund_info_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_etf_dividend_sina_data(self, df: pd.DataFrame, fund_code: str, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëÁ¥ØËÆ°ÂàÜÁ∫¢Êï∞ÊçÆÂà∞MongoDB
        
        Args:
            df: ÂåÖÂê´Âü∫ÈáëÁ¥ØËÆ°ÂàÜÁ∫¢Êï∞ÊçÆÁöÑDataFrame
            fund_code: Âü∫Èáë‰ª£Á†ÅÔºàÂ¶Ç sh510050Ôºâ
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞
            
        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning(f"Ê≤°ÊúâÂü∫ÈáëÁ¥ØËÆ°ÂàÜÁ∫¢Êï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò: {fund_code}")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {fund_code} ÁöÑ {total_count} Êù°Á¥ØËÆ°ÂàÜÁ∫¢Êï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime("%Y-%m-%d")
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime("%Y-%m-%d %H:%M:%S")
                        elif pd.isna(value):
                            doc[key] = None
                    
                    doc["fund_code"] = fund_code
                    doc["code"] = fund_code.replace("sh", "").replace("sz", "")
                    doc["source"] = "sina"
                    doc["updated_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    
                    date_str = doc.get("Êó•Êúü")
                    if date_str:
                        filter_query = {"fund_code": fund_code, "Êó•Êúü": date_str}
                        ops.append(UpdateOne(filter_query, {"$set": doc}, upsert=True))
                
                if ops:
                    result = await self.col_fund_etf_dividend_sina.bulk_write(ops, ordered=False)
                    batch_saved = result.upserted_count + result.modified_count
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        await progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"‚úÖ {fund_code} Á¥ØËÆ°ÂàÜÁ∫¢Êï∞ÊçÆ‰øùÂ≠òÂÆåÊàê: {total_saved}/{total_count}")
            return total_saved
            
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÁ¥ØËÆ°ÂàÜÁ∫¢Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_etf_dividend_sina_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëÁ¥ØËÆ°ÂàÜÁ∫¢ÁªüËÆ°‰ø°ÊÅØ
        
        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_count = await self.col_fund_etf_dividend_sina.count_documents({})
            
            fund_code_pipeline = [
                {"$group": {"_id": "$fund_code", "count": {"$sum": 1}}},
                {"$project": {"fund_code": "$_id", "count": 1, "_id": 0}},
                {"$sort": {"count": -1}},
                {"$limit": 20}
            ]
            fund_code_stats = await self.col_fund_etf_dividend_sina.aggregate(fund_code_pipeline).to_list(20)
            
            earliest_doc = (
                await self.col_fund_etf_dividend_sina.find({"Êó•Êúü": {"$exists": True, "$ne": None}}, {"Êó•Êúü": 1})
                .sort("Êó•Êúü", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_etf_dividend_sina.find({"Êó•Êúü": {"$exists": True, "$ne": None}}, {"Êó•Êúü": 1})
                .sort("Êó•Êúü", -1)
                .limit(1)
                .to_list(1)
            )
            
            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "earliest_date": earliest_doc[0]["Êó•Êúü"] if earliest_doc else None,
                "latest_date": latest_doc[0]["Êó•Êúü"] if latest_doc else None,
            }
            
            return result
            
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÁ¥ØËÆ°ÂàÜÁ∫¢ÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "earliest_date": None,
                "latest_date": None,
            }
    
    async def clear_fund_etf_dividend_sina_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëÁ¥ØËÆ°ÂàÜÁ∫¢Êï∞ÊçÆ
        
        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_etf_dividend_sina.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÁ¥ØËÆ°ÂàÜÁ∫¢Êï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÁ¥ØËÆ°ÂàÜÁ∫¢Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def import_fund_etf_dividend_sina_from_file(self, content: bytes, filename: Optional[str] = None) -> Dict[str, Any]:
        """‰ªéÊñá‰ª∂ÂØºÂÖ•Âü∫ÈáëÁ¥ØËÆ°ÂàÜÁ∫¢Êï∞ÊçÆ
        
        Args:
            content: Êñá‰ª∂ÂÜÖÂÆπ
            filename: Êñá‰ª∂Âêç
            
        Returns:
            ÂØºÂÖ•ÁªìÊûú
        """
        if not content:
            raise ValueError("‰∏ä‰º†Êñá‰ª∂‰∏∫Á©∫")
        
        name = (filename or "").lower()
        buffer = io.BytesIO(content)
        
        df: Optional[pd.DataFrame] = None
        try:
            if name.endswith(".csv") or name.endswith(".txt"):
                df = pd.read_csv(buffer)
            elif name.endswith(".xls") or name.endswith(".xlsx"):
                df = pd.read_excel(buffer)
            else:
                try:
                    df = pd.read_csv(buffer)
                except Exception:
                    buffer.seek(0)
                    df = pd.read_excel(buffer)
        except Exception as e:
            logger.error(f"ËØªÂèñÊñá‰ª∂Â§±Ë¥•: {e}", exc_info=True)
            raise ValueError("Êó†Ê≥ïËß£Êûê‰∏ä‰º†Êñá‰ª∂ÔºåËØ∑Á°ÆËÆ§‰∏∫ÊúâÊïàÁöÑ CSV Êàñ Excel Êñá‰ª∂")
        
        if df is None or df.empty:
            logger.warning("Ëß£ÊûêÁªìÊûú‰∏∫Á©∫ DataFrame")
            return {"saved": 0, "rows": 0}
        
        rows = len(df)
        fund_code = df.iloc[0].get("fund_code", "unknown") if "fund_code" in df.columns else "unknown"
        saved = await self.save_fund_etf_dividend_sina_data(df, fund_code)
        logger.info(f"‰ªéÊñá‰ª∂ {filename} ÂØºÂÖ• {rows} Ë°åÔºå‰øùÂ≠ò {saved} Êù°ËÆ∞ÂΩï")
        
        return {"saved": saved, "rows": rows}
    
    async def sync_fund_etf_dividend_sina_from_remote(self, remote_host: str, batch_size: int = 5000, 
                                                       remote_collection: Optional[str] = None,
                                                       remote_username: Optional[str] = None,
                                                       remote_password: Optional[str] = None,
                                                       remote_auth_source: Optional[str] = None) -> Dict[str, Any]:
        """‰ªéËøúÁ®ãMongoDBÂêåÊ≠•Âü∫ÈáëÁ¥ØËÆ°ÂàÜÁ∫¢Êï∞ÊçÆ
        
        Args:
            remote_host: ËøúÁ®ã‰∏ªÊú∫Âú∞ÂùÄ
            batch_size: ÊâπÊ¨°Â§ßÂ∞è
            remote_collection: ËøúÁ®ãÈõÜÂêàÂêçÁß∞
            remote_username: ËøúÁ®ãÁî®Êà∑Âêç
            remote_password: ËøúÁ®ãÂØÜÁ†Å
            remote_auth_source: ËÆ§ËØÅÊï∞ÊçÆÂ∫ì
            
        Returns:
            ÂêåÊ≠•ÁªìÊûú
        """
        from motor.motor_asyncio import AsyncIOMotorClient
        from bson import ObjectId
        
        if not remote_host:
            raise ValueError("ËøúÁ®ã‰∏ªÊú∫Âú∞ÂùÄ‰∏çËÉΩ‰∏∫Á©∫")
        
        try:
            batch = int(batch_size)
        except Exception:
            batch = 5000
        if batch <= 0:
            batch = 5000
        
        db_name = self.db.name
        auth_source = (remote_auth_source or db_name) if remote_username else None
        
        if remote_host.startswith("mongodb://") or remote_host.startswith("mongodb+srv://"):
            uri = remote_host
        else:
            host = remote_host
            port = 27017
            if ":" in remote_host:
                host_part, port_str = remote_host.split(":", 1)
                host = host_part or host
                try:
                    port = int(port_str)
                except Exception:
                    port = 27017
            
            if remote_username:
                if remote_password:
                    cred = f"{remote_username}:{remote_password}"
                else:
                    cred = remote_username
                
                if auth_source:
                    uri = f"mongodb://{cred}@{host}:{port}/{db_name}?authSource={auth_source}"
                else:
                    uri = f"mongodb://{cred}@{host}:{port}/{db_name}"
            else:
                uri = f"mongodb://{host}:{port}/{db_name}"
        
        logger.info(f"ÂºÄÂßã‰ªé {uri} ÂêåÊ≠•Âü∫ÈáëÁ¥ØËÆ°ÂàÜÁ∫¢Êï∞ÊçÆÔºåbatch_size={batch}")
        
        client: AsyncIOMotorClient = AsyncIOMotorClient(uri, serverSelectionTimeoutMS=5000)
        try:
            try:
                remote_db = client.get_default_database() or client[self.db.name]
            except Exception:
                remote_db = client[self.db.name]
            
            target_collection = remote_collection or "fund_etf_dividend_sina"
            remote_col = remote_db[target_collection]
            
            base_filter: Dict[str, Any] = {}
            
            try:
                remote_total = await remote_col.count_documents(base_filter)
            except Exception as e:
                logger.warning(f"ÁªüËÆ°ËøúÁ®ãÊñáÊ°£Êï∞ÈáèÂ§±Ë¥•: {e}")
                remote_total = 0
            
            synced = 0
            last_id: Optional[ObjectId] = None
            
            while True:
                if last_id is not None:
                    query: Dict[str, Any] = {"_id": {"$gt": last_id}}
                else:
                    query = base_filter
                
                cursor = remote_col.find(query).sort("_id", 1).limit(batch)
                docs = await cursor.to_list(length=batch)
                if not docs:
                    break
                
                last_id = docs[-1].get("_id")
                
                for d in docs:
                    d.pop("_id", None)
                
                ops = []
                for doc in docs:
                    fund_code = doc.get("fund_code", "unknown")
                    date_str = doc.get("Êó•Êúü")
                    if date_str:
                        filter_query = {"fund_code": fund_code, "Êó•Êúü": date_str}
                        ops.append(UpdateOne(filter_query, {"$set": doc}, upsert=True))
                
                if ops:
                    result = await self.col_fund_etf_dividend_sina.bulk_write(ops, ordered=False)
                    synced += result.upserted_count + result.modified_count
            
            logger.info(f"ÂÆåÊàêÂêåÊ≠•Ôºöremote_total={remote_total}, synced={synced}")
            
            return {"collection_name": "fund_etf_dividend_sina", "remote_total": remote_total, "synced": synced}
        finally:
            try:
                client.close()
            except Exception:
                pass
    
    async def save_fund_fh_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëÂàÜÁ∫¢Êï∞ÊçÆÂà∞MongoDB
        
        Args:
            df: ÂåÖÂê´Âü∫ÈáëÂàÜÁ∫¢Êï∞ÊçÆÁöÑDataFrame
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞
            
        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÂàÜÁ∫¢Êï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÂàÜÁ∫¢Êï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime("%Y-%m-%d")
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime("%Y-%m-%d %H:%M:%S")
                        elif pd.isna(value):
                            doc[key] = None
                    
                    doc["source"] = "eastmoney"
                    doc["updated_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    
                    # ÂîØ‰∏ÄÊ†áËØÜÔºöÂü∫Èáë‰ª£Á†Å + ÊùÉÁõäÁôªËÆ∞Êó• + Èô§ÊÅØÊó•Êúü
                    fund_code = doc.get("Âü∫Èáë‰ª£Á†Å")
                    equity_date = doc.get("ÊùÉÁõäÁôªËÆ∞Êó•")
                    ex_dividend_date = doc.get("Èô§ÊÅØÊó•Êúü")
                    
                    if fund_code and equity_date and ex_dividend_date:
                        filter_query = {
                            "Âü∫Èáë‰ª£Á†Å": fund_code,
                            "ÊùÉÁõäÁôªËÆ∞Êó•": equity_date,
                            "Èô§ÊÅØÊó•Êúü": ex_dividend_date
                        }
                        ops.append(UpdateOne(filter_query, {"$set": doc}, upsert=True))
                
                if ops:
                    result = await self.col_fund_fh_em.bulk_write(ops, ordered=False)
                    batch_saved = result.upserted_count + result.modified_count
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        await progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"‚úÖ Âü∫ÈáëÂàÜÁ∫¢Êï∞ÊçÆ‰øùÂ≠òÂÆåÊàê: {total_saved}/{total_count}")
            return total_saved
            
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÂàÜÁ∫¢Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_fh_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëÂàÜÁ∫¢ÁªüËÆ°‰ø°ÊÅØ
        
        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_count = await self.col_fund_fh_em.count_documents({})
            
            # Âü∫Èáë‰ª£Á†ÅÂàÜÂ∏ÉÔºàÂàÜÁ∫¢Ê¨°Êï∞ÊúÄÂ§öÁöÑÂü∫ÈáëÔºâ
            fund_code_pipeline = [
                {"$group": {"_id": "$Âü∫Èáë‰ª£Á†Å", "count": {"$sum": 1}, "Âü∫ÈáëÁÆÄÁß∞": {"$first": "$Âü∫ÈáëÁÆÄÁß∞"}}},
                {"$project": {"Âü∫Èáë‰ª£Á†Å": "$_id", "Âü∫ÈáëÁÆÄÁß∞": 1, "ÂàÜÁ∫¢Ê¨°Êï∞": "$count", "_id": 0}},
                {"$sort": {"ÂàÜÁ∫¢Ê¨°Êï∞": -1}},
                {"$limit": 20}
            ]
            fund_code_stats = await self.col_fund_fh_em.aggregate(fund_code_pipeline).to_list(20)
            
            # Êó•ÊúüËåÉÂõ¥ÁªüËÆ°
            earliest_doc = (
                await self.col_fund_fh_em.find({"ÊùÉÁõäÁôªËÆ∞Êó•": {"$exists": True, "$ne": None}}, {"ÊùÉÁõäÁôªËÆ∞Êó•": 1})
                .sort("ÊùÉÁõäÁôªËÆ∞Êó•", 1)
                .limit(1)
                .to_list(1)
            )
            latest_doc = (
                await self.col_fund_fh_em.find({"ÊùÉÁõäÁôªËÆ∞Êó•": {"$exists": True, "$ne": None}}, {"ÊùÉÁõäÁôªËÆ∞Êó•": 1})
                .sort("ÊùÉÁõäÁôªËÆ∞Êó•", -1)
                .limit(1)
                .to_list(1)
            )
            
            # ÂàÜÁ∫¢ÈáëÈ¢ùÁªüËÆ°
            total_dividend_pipeline = [
                {"$group": {"_id": None, "total_dividend": {"$sum": "$ÂàÜÁ∫¢"}}}
            ]
            total_dividend_result = await self.col_fund_fh_em.aggregate(total_dividend_pipeline).to_list(1)
            total_dividend = total_dividend_result[0]["total_dividend"] if total_dividend_result else 0
            
            result = {
                "total_count": total_count,
                "fund_code_stats": fund_code_stats,
                "earliest_date": earliest_doc[0]["ÊùÉÁõäÁôªËÆ∞Êó•"] if earliest_doc else None,
                "latest_date": latest_doc[0]["ÊùÉÁõäÁôªËÆ∞Êó•"] if latest_doc else None,
                "total_dividend": round(total_dividend, 4) if total_dividend else 0,
            }
            
            return result
            
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÂàÜÁ∫¢ÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            return {
                "total_count": 0,
                "fund_code_stats": [],
                "earliest_date": None,
                "latest_date": None,
                "total_dividend": 0,
            }
    
    async def clear_fund_fh_em_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëÂàÜÁ∫¢Êï∞ÊçÆ
        
        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_fh_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÂàÜÁ∫¢Êï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÂàÜÁ∫¢Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def import_fund_fh_em_from_file(self, content: bytes, filename: Optional[str] = None) -> Dict[str, Any]:
        """‰ªéÊñá‰ª∂ÂØºÂÖ•Âü∫ÈáëÂàÜÁ∫¢Êï∞ÊçÆ
        
        Args:
            content: Êñá‰ª∂ÂÜÖÂÆπ
            filename: Êñá‰ª∂Âêç
            
        Returns:
            ÂØºÂÖ•ÁªìÊûú
        """
        if not content:
            raise ValueError("‰∏ä‰º†Êñá‰ª∂‰∏∫Á©∫")
        
        name = (filename or "").lower()
        buffer = io.BytesIO(content)
        
        df: Optional[pd.DataFrame] = None
        try:
            if name.endswith(".csv") or name.endswith(".txt"):
                df = pd.read_csv(buffer)
            elif name.endswith(".xls") or name.endswith(".xlsx"):
                df = pd.read_excel(buffer)
            else:
                try:
                    df = pd.read_csv(buffer)
                except Exception:
                    buffer.seek(0)
                    df = pd.read_excel(buffer)
        except Exception as e:
            logger.error(f"ËØªÂèñÊñá‰ª∂Â§±Ë¥•: {e}", exc_info=True)
            raise ValueError("Êó†Ê≥ïËß£Êûê‰∏ä‰º†Êñá‰ª∂ÔºåËØ∑Á°ÆËÆ§‰∏∫ÊúâÊïàÁöÑ CSV Êàñ Excel Êñá‰ª∂")
        
        if df is None or df.empty:
            logger.warning("Ëß£ÊûêÁªìÊûú‰∏∫Á©∫ DataFrame")
            return {"saved": 0, "rows": 0}
        
        rows = len(df)
        saved = await self.save_fund_fh_em_data(df)
        logger.info(f"‰ªéÊñá‰ª∂ {filename} ÂØºÂÖ• {rows} Ë°åÔºå‰øùÂ≠ò {saved} Êù°ËÆ∞ÂΩï")
        
        return {"saved": saved, "rows": rows}
    
    async def sync_fund_fh_em_from_remote(self, remote_host: str, batch_size: int = 5000, 
                                          remote_collection: Optional[str] = None,
                                          remote_username: Optional[str] = None,
                                          remote_password: Optional[str] = None,
                                          remote_auth_source: Optional[str] = None) -> Dict[str, Any]:
        """‰ªéËøúÁ®ãMongoDBÂêåÊ≠•Âü∫ÈáëÂàÜÁ∫¢Êï∞ÊçÆ
        
        Args:
            remote_host: ËøúÁ®ã‰∏ªÊú∫Âú∞ÂùÄ
            batch_size: ÊâπÊ¨°Â§ßÂ∞è
            remote_collection: ËøúÁ®ãÈõÜÂêàÂêçÁß∞
            remote_username: ËøúÁ®ãÁî®Êà∑Âêç
            remote_password: ËøúÁ®ãÂØÜÁ†Å
            remote_auth_source: ËÆ§ËØÅÊï∞ÊçÆÂ∫ì
            
        Returns:
            ÂêåÊ≠•ÁªìÊûú
        """
        from motor.motor_asyncio import AsyncIOMotorClient
        from bson import ObjectId
        
        if not remote_host:
            raise ValueError("ËøúÁ®ã‰∏ªÊú∫Âú∞ÂùÄ‰∏çËÉΩ‰∏∫Á©∫")
        
        try:
            batch = int(batch_size)
        except Exception:
            batch = 5000
        if batch <= 0:
            batch = 5000
        
        db_name = self.db.name
        auth_source = (remote_auth_source or db_name) if remote_username else None
        
        if remote_host.startswith("mongodb://") or remote_host.startswith("mongodb+srv://"):
            uri = remote_host
        else:
            host = remote_host
            port = 27017
            if ":" in remote_host:
                host_part, port_str = remote_host.split(":", 1)
                host = host_part or host
                try:
                    port = int(port_str)
                except Exception:
                    port = 27017
            
            if remote_username:
                if remote_password:
                    cred = f"{remote_username}:{remote_password}"
                else:
                    cred = remote_username
                
                if auth_source:
                    uri = f"mongodb://{cred}@{host}:{port}/{db_name}?authSource={auth_source}"
                else:
                    uri = f"mongodb://{cred}@{host}:{port}/{db_name}"
            else:
                uri = f"mongodb://{host}:{port}/{db_name}"
        
        logger.info(f"ÂºÄÂßã‰ªé {uri} ÂêåÊ≠•Âü∫ÈáëÂàÜÁ∫¢Êï∞ÊçÆÔºåbatch_size={batch}")
        
        client: AsyncIOMotorClient = AsyncIOMotorClient(uri, serverSelectionTimeoutMS=5000)
        try:
            try:
                remote_db = client.get_default_database() or client[self.db.name]
            except Exception:
                remote_db = client[self.db.name]
            
            target_collection = remote_collection or "fund_fh_em"
            remote_col = remote_db[target_collection]
            
            base_filter: Dict[str, Any] = {}
            
            try:
                remote_total = await remote_col.count_documents(base_filter)
            except Exception as e:
                logger.warning(f"ÁªüËÆ°ËøúÁ®ãÊñáÊ°£Êï∞ÈáèÂ§±Ë¥•: {e}")
                remote_total = 0
            
            synced = 0
            last_id: Optional[ObjectId] = None
            
            while True:
                if last_id is not None:
                    query: Dict[str, Any] = {"_id": {"$gt": last_id}}
                else:
                    query = base_filter
                
                cursor = remote_col.find(query).sort("_id", 1).limit(batch)
                docs = await cursor.to_list(length=batch)
                if not docs:
                    break
                
                last_id = docs[-1].get("_id")
                
                for d in docs:
                    d.pop("_id", None)
                
                ops = []
                for doc in docs:
                    fund_code = doc.get("Âü∫Èáë‰ª£Á†Å")
                    equity_date = doc.get("ÊùÉÁõäÁôªËÆ∞Êó•")
                    ex_dividend_date = doc.get("Èô§ÊÅØÊó•Êúü")
                    
                    if fund_code and equity_date and ex_dividend_date:
                        filter_query = {
                            "Âü∫Èáë‰ª£Á†Å": fund_code,
                            "ÊùÉÁõäÁôªËÆ∞Êó•": equity_date,
                            "Èô§ÊÅØÊó•Êúü": ex_dividend_date
                        }
                        ops.append(UpdateOne(filter_query, {"$set": doc}, upsert=True))
                
                if ops:
                    result = await self.col_fund_fh_em.bulk_write(ops, ordered=False)
                    synced += result.upserted_count + result.modified_count
            
            logger.info(f"ÂÆåÊàêÂêåÊ≠•Ôºöremote_total={remote_total}, synced={synced}")
            
            return {"collection_name": "fund_fh_em", "remote_total": remote_total, "synced": synced}
        finally:
            try:
                client.close()
            except Exception:
                pass
    
    async def save_fund_cf_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ‰øùÂ≠òÂü∫ÈáëÊãÜÂàÜÊï∞ÊçÆÂà∞MongoDB
        
        Args:
            df: ÂåÖÂê´Âü∫ÈáëÊãÜÂàÜ‰ø°ÊÅØÁöÑDataFrame
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞
            
        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÊãÜÂàÜÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            # Ê∏ÖÁêÜÊó†ÊïàÁöÑÊµÆÁÇπÊï∞ÂÄºÔºàNaN, InfinityÁ≠âÔºâÔºåÈò≤Ê≠¢JSONÂ∫èÂàóÂåñÈîôËØØ
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÊãÜÂàÜÊï∞ÊçÆ...")
            
            # ÂàÜÊâπÂ§ÑÁêÜÔºåÊØèÊâπ500Êù°
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"üì¶ Â∞ÜÂàÜ {total_batches} ÊâπÊ¨°Â§ÑÁêÜÔºåÊØèÊâπ {batch_size} Êù°")
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"üìù Â§ÑÁêÜÁ¨¨ {batch_idx + 1}/{total_batches} ÊâπÔºåËÆ∞ÂΩïËåÉÂõ¥: {start_idx + 1}-{end_idx}")
                
                # ÊûÑÂª∫ÊâπÈáèÊìç‰Ωú
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # Ê∏ÖÁêÜNaN/InfinityÂÄº
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        # ËΩ¨Êç¢ datetime.date ÂØπË±°‰∏∫Â≠óÁ¨¶‰∏≤
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        # ËΩ¨Êç¢ datetime.datetime ÂØπË±°‰∏∫Â≠óÁ¨¶‰∏≤
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    split_date = str(doc.get('ÊãÜÂàÜÊäòÁÆóÊó•', ''))
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_cf_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫Èáë‰ª£Á†ÅÂíåÊãÜÂàÜÊäòÁÆóÊó•‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'ÊãÜÂàÜÊäòÁÆóÊó•': split_date},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # ÊâßË°åÊâπÈáèÂÜôÂÖ•
                if ops:
                    result = await self.col_fund_cf_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"‚úÖ Á¨¨ {batch_idx + 1}/{total_batches} ÊâπÂÜôÂÖ•ÂÆåÊàê: "
                        f"Êñ∞Â¢û={result.upserted_count}, Êõ¥Êñ∞={result.matched_count}, "
                        f"Êú¨Êâπ‰øùÂ≠ò={batch_saved}, Á¥ØËÆ°={total_saved}/{total_count}"
                    )
                    
                    # Ë∞ÉÁî®ËøõÂ∫¶ÂõûË∞ÉÔºàÂ¶ÇÊûúÊèê‰æõÔºâ
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëÊãÜÂàÜÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÊãÜÂàÜÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_cf_em_data(self) -> int:
        """
        Ê∏ÖÁ©∫Âü∫ÈáëÊãÜÂàÜÊï∞ÊçÆ
        
        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_cf_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÊãÜÂàÜÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÊãÜÂàÜÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_cf_em_stats(self) -> Dict[str, Any]:
        """
        Ëé∑ÂèñÂü∫ÈáëÊãÜÂàÜÁªüËÆ°
        
        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_count = await self.col_fund_cf_em.count_documents({})
            
            # ÊåâÊãÜÂàÜÁ±ªÂûãÁªüËÆ°
            pipeline = [
                {
                    '$group': {
                        '_id': '$ÊãÜÂàÜÁ±ªÂûã',
                        'count': {'$sum': 1}
                    }
                },
                {
                    '$sort': {'count': -1}
                }
            ]
            
            type_stats = []
            async for doc in self.col_fund_cf_em.aggregate(pipeline):
                type_stats.append({
                    'type': doc['_id'],
                    'count': doc['count']
                })
            
            # Ëé∑ÂèñÊúÄÊó©ÂíåÊúÄÊôöÁöÑÊãÜÂàÜÊó•Êúü
            earliest_date = None
            latest_date = None
            pipeline_date = [
                {
                    '$group': {
                        '_id': None,
                        'earliest': {'$min': '$ÊãÜÂàÜÊäòÁÆóÊó•'},
                        'latest': {'$max': '$ÊãÜÂàÜÊäòÁÆóÊó•'}
                    }
                }
            ]
            
            async for doc in self.col_fund_cf_em.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            return {
                'total_count': total_count,
                'type_stats': type_stats,
                'earliest_date': earliest_date,
                'latest_date': latest_date
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÊãÜÂàÜÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_fh_rank_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ‰øùÂ≠òÂü∫ÈáëÂàÜÁ∫¢ÊéíË°åÊï∞ÊçÆÂà∞MongoDB
        
        Args:
            df: ÂåÖÂê´Âü∫ÈáëÂàÜÁ∫¢ÊéíË°å‰ø°ÊÅØÁöÑDataFrame
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞
            
        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÂàÜÁ∫¢ÊéíË°åÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            # Ê∏ÖÁêÜÊó†ÊïàÁöÑÊµÆÁÇπÊï∞ÂÄºÔºàNaN, InfinityÁ≠âÔºâÔºåÈò≤Ê≠¢JSONÂ∫èÂàóÂåñÈîôËØØ
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÂàÜÁ∫¢ÊéíË°åÊï∞ÊçÆ...")
            
            # ÂàÜÊâπÂ§ÑÁêÜÔºåÊØèÊâπ500Êù°
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"üì¶ Â∞ÜÂàÜ {total_batches} ÊâπÊ¨°Â§ÑÁêÜÔºåÊØèÊâπ {batch_size} Êù°")
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"üìù Â§ÑÁêÜÁ¨¨ {batch_idx + 1}/{total_batches} ÊâπÔºåËÆ∞ÂΩïËåÉÂõ¥: {start_idx + 1}-{end_idx}")
                
                # ÊûÑÂª∫ÊâπÈáèÊìç‰Ωú
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # Ê∏ÖÁêÜNaN/InfinityÂÄº
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        # ËΩ¨Êç¢ datetime.date ÂØπË±°‰∏∫Â≠óÁ¨¶‰∏≤
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        # ËΩ¨Êç¢ datetime.datetime ÂØπË±°‰∏∫Â≠óÁ¨¶‰∏≤
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_fh_rank_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫Èáë‰ª£Á†Å‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    ops.append(
                        UpdateOne(
                            {'code': fund_code},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # ÊâßË°åÊâπÈáèÂÜôÂÖ•
                if ops:
                    result = await self.col_fund_fh_rank_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"‚úÖ Á¨¨ {batch_idx + 1}/{total_batches} ÊâπÂÜôÂÖ•ÂÆåÊàê: "
                        f"Êñ∞Â¢û={result.upserted_count}, Êõ¥Êñ∞={result.matched_count}, "
                        f"Êú¨Êâπ‰øùÂ≠ò={batch_saved}, Á¥ØËÆ°={total_saved}/{total_count}"
                    )
                    
                    # Ë∞ÉÁî®ËøõÂ∫¶ÂõûË∞ÉÔºàÂ¶ÇÊûúÊèê‰æõÔºâ
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëÂàÜÁ∫¢ÊéíË°åÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÂàÜÁ∫¢ÊéíË°åÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_fh_rank_em_data(self) -> int:
        """
        Ê∏ÖÁ©∫Âü∫ÈáëÂàÜÁ∫¢ÊéíË°åÊï∞ÊçÆ
        
        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_fh_rank_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÂàÜÁ∫¢ÊéíË°åÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÂàÜÁ∫¢ÊéíË°åÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_fh_rank_em_stats(self) -> Dict[str, Any]:
        """
        Ëé∑ÂèñÂü∫ÈáëÂàÜÁ∫¢ÊéíË°åÁªüËÆ°
        
        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_count = await self.col_fund_fh_rank_em.count_documents({})
            
            # Ëé∑ÂèñÊúÄÊó©ÂíåÊúÄÊôöÁöÑÊàêÁ´ãÊó•Êúü
            earliest_date = None
            latest_date = None
            pipeline_date = [
                {
                    '$group': {
                        '_id': None,
                        'earliest': {'$min': '$ÊàêÁ´ãÊó•Êúü'},
                        'latest': {'$max': '$ÊàêÁ´ãÊó•Êúü'}
                    }
                }
            ]
            
            async for doc in self.col_fund_fh_rank_em.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            # Ëé∑ÂèñÁ¥ØËÆ°ÂàÜÁ∫¢TOP10
            pipeline_top_dividend = [
                {
                    '$sort': {'Á¥ØËÆ°ÂàÜÁ∫¢': -1}
                },
                {
                    '$limit': 10
                },
                {
                    '$project': {
                        'code': '$Âü∫Èáë‰ª£Á†Å',
                        'name': '$Âü∫ÈáëÁÆÄÁß∞',
                        'total_dividend': '$Á¥ØËÆ°ÂàÜÁ∫¢',
                        'dividend_times': '$Á¥ØËÆ°Ê¨°Êï∞'
                    }
                }
            ]
            
            top_dividend = []
            async for doc in self.col_fund_fh_rank_em.aggregate(pipeline_top_dividend):
                top_dividend.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'total_dividend': doc.get('total_dividend'),
                    'dividend_times': doc.get('dividend_times')
                })
            
            # Ëé∑ÂèñÁ¥ØËÆ°Ê¨°Êï∞TOP10
            pipeline_top_times = [
                {
                    '$sort': {'Á¥ØËÆ°Ê¨°Êï∞': -1}
                },
                {
                    '$limit': 10
                },
                {
                    '$project': {
                        'code': '$Âü∫Èáë‰ª£Á†Å',
                        'name': '$Âü∫ÈáëÁÆÄÁß∞',
                        'total_dividend': '$Á¥ØËÆ°ÂàÜÁ∫¢',
                        'dividend_times': '$Á¥ØËÆ°Ê¨°Êï∞'
                    }
                }
            ]
            
            top_times = []
            async for doc in self.col_fund_fh_rank_em.aggregate(pipeline_top_times):
                top_times.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'total_dividend': doc.get('total_dividend'),
                    'dividend_times': doc.get('dividend_times')
                })
            
            return {
                'total_count': total_count,
                'earliest_date': earliest_date,
                'latest_date': latest_date,
                'top_dividend': top_dividend,
                'top_times': top_times
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÂàÜÁ∫¢ÊéíË°åÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_open_fund_rank_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """
        ‰øùÂ≠òÂºÄÊîæÂºèÂü∫ÈáëÊéíË°åÊï∞ÊçÆÂà∞MongoDB
        
        Args:
            df: ÂåÖÂê´ÂºÄÊîæÂºèÂü∫ÈáëÊéíË°å‰ø°ÊÅØÁöÑDataFrame
            progress_callback: ËøõÂ∫¶ÂõûË∞ÉÂáΩÊï∞
            
        Returns:
            ‰øùÂ≠òÁöÑËÆ∞ÂΩïÊï∞
        """
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂºÄÊîæÂºèÂü∫ÈáëÊéíË°åÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            # Ê∏ÖÁêÜÊó†ÊïàÁöÑÊµÆÁÇπÊï∞ÂÄºÔºàNaN, InfinityÁ≠âÔºâÔºåÈò≤Ê≠¢JSONÂ∫èÂàóÂåñÈîôËØØ
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°ÂºÄÊîæÂºèÂü∫ÈáëÊéíË°åÊï∞ÊçÆ...")
            
            # ÂàÜÊâπÂ§ÑÁêÜÔºåÊØèÊâπ500Êù°
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"üì¶ Â∞ÜÂàÜ {total_batches} ÊâπÊ¨°Â§ÑÁêÜÔºåÊØèÊâπ {batch_size} Êù°")
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                logger.info(f"üìù Â§ÑÁêÜÁ¨¨ {batch_idx + 1}/{total_batches} ÊâπÔºåËÆ∞ÂΩïËåÉÂõ¥: {start_idx + 1}-{end_idx}")
                
                # ÊûÑÂª∫ÊâπÈáèÊìç‰Ωú
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    # Ê∏ÖÁêÜNaN/InfinityÂÄº
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        # ËΩ¨Êç¢ datetime.date ÂØπË±°‰∏∫Â≠óÁ¨¶‰∏≤
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        # ËΩ¨Êç¢ datetime.datetime ÂØπË±°‰∏∫Â≠óÁ¨¶‰∏≤
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    date_str = str(doc.get('Êó•Êúü', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_open_fund_rank_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫Èáë‰ª£Á†ÅÂíåÊó•Êúü‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'date': date_str},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                # ÊâßË°åÊâπÈáèÂÜôÂÖ•
                if ops:
                    result = await self.col_fund_open_fund_rank_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    logger.info(
                        f"‚úÖ Á¨¨ {batch_idx + 1}/{total_batches} ÊâπÂÜôÂÖ•ÂÆåÊàê: "
                        f"Êñ∞Â¢û={result.upserted_count}, Êõ¥Êñ∞={result.matched_count}, "
                        f"Êú¨Êâπ‰øùÂ≠ò={batch_saved}, Á¥ØËÆ°={total_saved}/{total_count}"
                    )
                    
                    # Ë∞ÉÁî®ËøõÂ∫¶ÂõûË∞ÉÔºàÂ¶ÇÊûúÊèê‰æõÔºâ
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°ÂºÄÊîæÂºèÂü∫ÈáëÊéíË°åÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂºÄÊîæÂºèÂü∫ÈáëÊéíË°åÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_open_fund_rank_em_data(self) -> int:
        """
        Ê∏ÖÁ©∫ÂºÄÊîæÂºèÂü∫ÈáëÊéíË°åÊï∞ÊçÆ
        
        Returns:
            Âà†Èô§ÁöÑËÆ∞ÂΩïÊï∞
        """
        try:
            result = await self.col_fund_open_fund_rank_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÂºÄÊîæÂºèÂü∫ÈáëÊéíË°åÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÂºÄÊîæÂºèÂü∫ÈáëÊéíË°åÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_open_fund_rank_em_stats(self) -> Dict[str, Any]:
        """
        Ëé∑ÂèñÂºÄÊîæÂºèÂü∫ÈáëÊéíË°åÁªüËÆ°
        
        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_count = await self.col_fund_open_fund_rank_em.count_documents({})
            
            # Ëé∑ÂèñÊúÄÊó©ÂíåÊúÄÊôöÁöÑÊó•Êúü
            earliest_date = None
            latest_date = None
            pipeline_date = [
                {
                    '$group': {
                        '_id': None,
                        'earliest': {'$min': '$Êó•Êúü'},
                        'latest': {'$max': '$Êó•Êúü'}
                    }
                }
            ]
            
            async for doc in self.col_fund_open_fund_rank_em.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            # Ëé∑ÂèñËøë1Âπ¥Êî∂ÁõäÁéáTOP10
            pipeline_top_1year = [
                {
                    '$match': {'Ëøë1Âπ¥': {'$ne': None}}
                },
                {
                    '$sort': {'Ëøë1Âπ¥': -1}
                },
                {
                    '$limit': 10
                },
                {
                    '$project': {
                        'code': '$Âü∫Èáë‰ª£Á†Å',
                        'name': '$Âü∫ÈáëÁÆÄÁß∞',
                        'return_1year': '$Ëøë1Âπ¥',
                        'return_ytd': '$‰ªäÂπ¥Êù•'
                    }
                }
            ]
            
            top_performers = []
            async for doc in self.col_fund_open_fund_rank_em.aggregate(pipeline_top_1year):
                top_performers.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'return_1year': doc.get('return_1year'),
                    'return_ytd': doc.get('return_ytd')
                })
            
            # Ëé∑Âèñ‰ªäÂπ¥Êù•Êî∂ÁõäÁéáTOP10
            pipeline_top_ytd = [
                {
                    '$match': {'‰ªäÂπ¥Êù•': {'$ne': None}}
                },
                {
                    '$sort': {'‰ªäÂπ¥Êù•': -1}
                },
                {
                    '$limit': 10
                },
                {
                    '$project': {
                        'code': '$Âü∫Èáë‰ª£Á†Å',
                        'name': '$Âü∫ÈáëÁÆÄÁß∞',
                        'return_ytd': '$‰ªäÂπ¥Êù•',
                        'return_1year': '$Ëøë1Âπ¥'
                    }
                }
            ]
            
            top_ytd = []
            async for doc in self.col_fund_open_fund_rank_em.aggregate(pipeline_top_ytd):
                top_ytd.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'return_ytd': doc.get('return_ytd'),
                    'return_1year': doc.get('return_1year')
                })
            
            return {
                'total_count': total_count,
                'earliest_date': earliest_date,
                'latest_date': latest_date,
                'top_performers': top_performers,
                'top_ytd': top_ytd
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂºÄÊîæÂºèÂü∫ÈáëÊéíË°åÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_exchange_rank_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÊéíË°åÊï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÊéíË°åÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÊéíË°åÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    date_str = str(doc.get('Êó•Êúü', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_exchange_rank_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'date': date_str},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_exchange_rank_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÊéíË°åÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÊéíË°åÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_exchange_rank_em_data(self) -> int:
        """Ê∏ÖÁ©∫Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÊéíË°åÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_exchange_rank_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÊéíË°åÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÊéíË°åÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_exchange_rank_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÊéíË°åÁªüËÆ°"""
        try:
            total_count = await self.col_fund_exchange_rank_em.count_documents({})
            
            pipeline_date = [
                {'$group': {'_id': None, 'earliest': {'$min': '$Êó•Êúü'}, 'latest': {'$max': '$Êó•Êúü'}}}
            ]
            
            earliest_date = None
            latest_date = None
            async for doc in self.col_fund_exchange_rank_em.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            pipeline_type = [
                {'$group': {'_id': '$Á±ªÂûã', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            type_stats = []
            async for doc in self.col_fund_exchange_rank_em.aggregate(pipeline_type):
                type_stats.append({'type': doc['_id'], 'count': doc['count']})
            
            pipeline_top = [
                {'$match': {'Ëøë1Âπ¥': {'$ne': None}}},
                {'$sort': {'Ëøë1Âπ¥': -1}},
                {'$limit': 10},
                {'$project': {'code': '$Âü∫Èáë‰ª£Á†Å', 'name': '$Âü∫ÈáëÁÆÄÁß∞', 'type': '$Á±ªÂûã', 'return_1year': '$Ëøë1Âπ¥'}}
            ]
            
            top_performers = []
            async for doc in self.col_fund_exchange_rank_em.aggregate(pipeline_top):
                top_performers.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'type': doc.get('type'),
                    'return_1year': doc.get('return_1year')
                })
            
            return {
                'total_count': total_count,
                'earliest_date': earliest_date,
                'latest_date': latest_date,
                'type_stats': type_stats,
                'top_performers': top_performers
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂú∫ÂÜÖ‰∫§ÊòìÂü∫ÈáëÊéíË°åÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_money_rank_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òË¥ßÂ∏ÅÂûãÂü∫ÈáëÊéíË°åÊï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâË¥ßÂ∏ÅÂûãÂü∫ÈáëÊéíË°åÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÊéíË°åÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    date_str = str(doc.get('Êó•Êúü', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_money_rank_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'date': date_str},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_money_rank_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÊéíË°åÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òË¥ßÂ∏ÅÂûãÂü∫ÈáëÊéíË°åÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_money_rank_em_data(self) -> int:
        """Ê∏ÖÁ©∫Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÊéíË°åÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_money_rank_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÊéíË°åÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Ë¥ßÂ∏ÅÂûãÂü∫ÈáëÊéíË°åÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_money_rank_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñË¥ßÂ∏ÅÂûãÂü∫ÈáëÊéíË°åÁªüËÆ°"""
        try:
            total_count = await self.col_fund_money_rank_em.count_documents({})
            
            pipeline_date = [
                {'$group': {'_id': None, 'earliest': {'$min': '$Êó•Êúü'}, 'latest': {'$max': '$Êó•Êúü'}}}
            ]
            
            earliest_date = None
            latest_date = None
            async for doc in self.col_fund_money_rank_em.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            # Ëé∑ÂèñÂπ¥ÂåñÊî∂ÁõäÁéá7Êó•TOP10
            pipeline_top_7d = [
                {'$match': {'Âπ¥ÂåñÊî∂ÁõäÁéá7Êó•': {'$ne': None}}},
                {'$sort': {'Âπ¥ÂåñÊî∂ÁõäÁéá7Êó•': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$Âü∫Èáë‰ª£Á†Å',
                    'name': '$Âü∫ÈáëÁÆÄÁß∞',
                    'yield_7d': '$Âπ¥ÂåñÊî∂ÁõäÁéá7Êó•',
                    'yield_10k': '$‰∏á‰ªΩÊî∂Áõä'
                }}
            ]
            
            top_yield_7d = []
            async for doc in self.col_fund_money_rank_em.aggregate(pipeline_top_7d):
                top_yield_7d.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'yield_7d': doc.get('yield_7d'),
                    'yield_10k': doc.get('yield_10k')
                })
            
            # Ëé∑ÂèñËøë1Âπ¥Êî∂ÁõäTOP10
            pipeline_top_1y = [
                {'$match': {'Ëøë1Âπ¥': {'$ne': None}}},
                {'$sort': {'Ëøë1Âπ¥': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$Âü∫Èáë‰ª£Á†Å',
                    'name': '$Âü∫ÈáëÁÆÄÁß∞',
                    'return_1y': '$Ëøë1Âπ¥',
                    'yield_7d': '$Âπ¥ÂåñÊî∂ÁõäÁéá7Êó•'
                }}
            ]
            
            top_return_1y = []
            async for doc in self.col_fund_money_rank_em.aggregate(pipeline_top_1y):
                top_return_1y.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'return_1y': doc.get('return_1y'),
                    'yield_7d': doc.get('yield_7d')
                })
            
            return {
                'total_count': total_count,
                'earliest_date': earliest_date,
                'latest_date': latest_date,
                'top_yield_7d': top_yield_7d,
                'top_return_1y': top_return_1y
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñË¥ßÂ∏ÅÂûãÂü∫ÈáëÊéíË°åÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_lcx_rank_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÁêÜË¥¢Âü∫ÈáëÊéíË°åÊï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÁêÜË¥¢Âü∫ÈáëÊéíË°åÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°ÁêÜË¥¢Âü∫ÈáëÊéíË°åÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    date_str = str(doc.get('Êó•Êúü', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_lcx_rank_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'date': date_str},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_lcx_rank_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°ÁêÜË¥¢Âü∫ÈáëÊéíË°åÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÁêÜË¥¢Âü∫ÈáëÊéíË°åÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_lcx_rank_em_data(self) -> int:
        """Ê∏ÖÁ©∫ÁêÜË¥¢Âü∫ÈáëÊéíË°åÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_lcx_rank_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÁêÜË¥¢Âü∫ÈáëÊéíË°åÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÁêÜË¥¢Âü∫ÈáëÊéíË°åÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_lcx_rank_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÁêÜË¥¢Âü∫ÈáëÊéíË°åÁªüËÆ°"""
        try:
            total_count = await self.col_fund_lcx_rank_em.count_documents({})
            
            pipeline_date = [
                {'$group': {'_id': None, 'earliest': {'$min': '$Êó•Êúü'}, 'latest': {'$max': '$Êó•Êúü'}}}
            ]
            
            earliest_date = None
            latest_date = None
            async for doc in self.col_fund_lcx_rank_em.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            # Ëé∑ÂèñÂπ¥ÂåñÊî∂ÁõäÁéá7Êó•TOP10
            pipeline_top_7d = [
                {'$match': {'Âπ¥ÂåñÊî∂ÁõäÁéá7Êó•': {'$ne': None}}},
                {'$sort': {'Âπ¥ÂåñÊî∂ÁõäÁéá7Êó•': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$Âü∫Èáë‰ª£Á†Å',
                    'name': '$Âü∫ÈáëÁÆÄÁß∞',
                    'yield_7d': '$Âπ¥ÂåñÊî∂ÁõäÁéá7Êó•',
                    'yield_10k': '$‰∏á‰ªΩÊî∂Áõä',
                    'purchasable': '$ÂèØË¥≠‰π∞'
                }}
            ]
            
            top_yield_7d = []
            async for doc in self.col_fund_lcx_rank_em.aggregate(pipeline_top_7d):
                top_yield_7d.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'yield_7d': doc.get('yield_7d'),
                    'yield_10k': doc.get('yield_10k'),
                    'purchasable': doc.get('purchasable')
                })
            
            # Ëé∑ÂèñÊàêÁ´ãÊù•Êî∂ÁõäTOP10
            pipeline_top_since = [
                {'$match': {'ÊàêÁ´ãÊù•': {'$ne': None}}},
                {'$sort': {'ÊàêÁ´ãÊù•': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$Âü∫Èáë‰ª£Á†Å',
                    'name': '$Âü∫ÈáëÁÆÄÁß∞',
                    'return_since': '$ÊàêÁ´ãÊù•',
                    'yield_7d': '$Âπ¥ÂåñÊî∂ÁõäÁéá7Êó•',
                    'purchasable': '$ÂèØË¥≠‰π∞'
                }}
            ]
            
            top_return_since = []
            async for doc in self.col_fund_lcx_rank_em.aggregate(pipeline_top_since):
                top_return_since.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'return_since': doc.get('return_since'),
                    'yield_7d': doc.get('yield_7d'),
                    'purchasable': doc.get('purchasable')
                })
            
            return {
                'total_count': total_count,
                'earliest_date': earliest_date,
                'latest_date': latest_date,
                'top_yield_7d': top_yield_7d,
                'top_return_since': top_return_since
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÁêÜË¥¢Âü∫ÈáëÊéíË°åÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_hk_rank_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÈ¶ôÊ∏ØÂü∫ÈáëÊéíË°åÊï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÈ¶ôÊ∏ØÂü∫ÈáëÊéíË°åÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°È¶ôÊ∏ØÂü∫ÈáëÊéíË°åÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    date_str = str(doc.get('Êó•Êúü', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_hk_rank_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'date': date_str},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_hk_rank_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°È¶ôÊ∏ØÂü∫ÈáëÊéíË°åÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÈ¶ôÊ∏ØÂü∫ÈáëÊéíË°åÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_hk_rank_em_data(self) -> int:
        """Ê∏ÖÁ©∫È¶ôÊ∏ØÂü∫ÈáëÊéíË°åÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_hk_rank_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°È¶ôÊ∏ØÂü∫ÈáëÊéíË°åÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫È¶ôÊ∏ØÂü∫ÈáëÊéíË°åÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_hk_rank_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÈ¶ôÊ∏ØÂü∫ÈáëÊéíË°åÁªüËÆ°"""
        try:
            total_count = await self.col_fund_hk_rank_em.count_documents({})
            
            pipeline_date = [
                {'$group': {'_id': None, 'earliest': {'$min': '$Êó•Êúü'}, 'latest': {'$max': '$Êó•Êúü'}}}
            ]
            
            earliest_date = None
            latest_date = None
            async for doc in self.col_fund_hk_rank_em.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            # Ëé∑ÂèñÂ∏ÅÁßçÂàÜÂ∏ÉÁªüËÆ°
            pipeline_currency = [
                {'$group': {'_id': '$Â∏ÅÁßç', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            currency_stats = []
            async for doc in self.col_fund_hk_rank_em.aggregate(pipeline_currency):
                currency_stats.append({
                    'currency': doc.get('_id'),
                    'count': doc.get('count')
                })
            
            # Ëé∑ÂèñËøë1Âπ¥Êî∂ÁõäTOP10
            pipeline_top_1y = [
                {'$match': {'Ëøë1Âπ¥': {'$ne': None}}},
                {'$sort': {'Ëøë1Âπ¥': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$Âü∫Èáë‰ª£Á†Å',
                    'name': '$Âü∫ÈáëÁÆÄÁß∞',
                    'currency': '$Â∏ÅÁßç',
                    'return_1y': '$Ëøë1Âπ¥',
                    'nav': '$Âçï‰ΩçÂáÄÂÄº',
                    'purchasable': '$ÂèØË¥≠‰π∞'
                }}
            ]
            
            top_return_1y = []
            async for doc in self.col_fund_hk_rank_em.aggregate(pipeline_top_1y):
                top_return_1y.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'currency': doc.get('currency'),
                    'return_1y': doc.get('return_1y'),
                    'nav': doc.get('nav'),
                    'purchasable': doc.get('purchasable')
                })
            
            # Ëé∑ÂèñÊàêÁ´ãÊù•Êî∂ÁõäTOP10
            pipeline_top_since = [
                {'$match': {'ÊàêÁ´ãÊù•': {'$ne': None}}},
                {'$sort': {'ÊàêÁ´ãÊù•': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$Âü∫Èáë‰ª£Á†Å',
                    'name': '$Âü∫ÈáëÁÆÄÁß∞',
                    'currency': '$Â∏ÅÁßç',
                    'return_since': '$ÊàêÁ´ãÊù•',
                    'purchasable': '$ÂèØË¥≠‰π∞'
                }}
            ]
            
            top_return_since = []
            async for doc in self.col_fund_hk_rank_em.aggregate(pipeline_top_since):
                top_return_since.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'currency': doc.get('currency'),
                    'return_since': doc.get('return_since'),
                    'purchasable': doc.get('purchasable')
                })
            
            return {
                'total_count': total_count,
                'earliest_date': earliest_date,
                'latest_date': latest_date,
                'currency_stats': currency_stats,
                'top_return_1y': top_return_1y,
                'top_return_since': top_return_since
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÈ¶ôÊ∏ØÂü∫ÈáëÊéíË°åÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_individual_achievement_xq_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫Èáë‰∏öÁª©Êï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫Èáë‰∏öÁª©Êï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫Èáë‰∏öÁª©Êï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    perf_type = str(doc.get('‰∏öÁª©Á±ªÂûã', ''))
                    period = str(doc.get('Âë®Êúü', ''))
                    doc['code'] = fund_code
                    doc['performance_type'] = perf_type
                    doc['period'] = period
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_individual_achievement_xq'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'performance_type': perf_type, 'period': period},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_individual_achievement_xq.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫Èáë‰∏öÁª©Êï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫Èáë‰∏öÁª©Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_individual_achievement_xq_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫Èáë‰∏öÁª©Êï∞ÊçÆ"""
        try:
            result = await self.col_fund_individual_achievement_xq.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫Èáë‰∏öÁª©Êï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫Èáë‰∏öÁª©Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_individual_achievement_xq_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫Èáë‰∏öÁª©ÁªüËÆ°"""
        try:
            total_count = await self.col_fund_individual_achievement_xq.count_documents({})
            
            # Ëé∑ÂèñÂîØ‰∏ÄÂü∫ÈáëÊï∞Èáè
            pipeline_funds = [
                {'$group': {'_id': '$code'}},
                {'$count': 'unique_funds'}
            ]
            
            unique_funds = 0
            async for doc in self.col_fund_individual_achievement_xq.aggregate(pipeline_funds):
                unique_funds = doc.get('unique_funds', 0)
            
            # Ëé∑Âèñ‰∏öÁª©Á±ªÂûãÂàÜÂ∏É
            pipeline_types = [
                {'$group': {'_id': '$‰∏öÁª©Á±ªÂûã', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            performance_types = []
            async for doc in self.col_fund_individual_achievement_xq.aggregate(pipeline_types):
                performance_types.append({
                    'type': doc.get('_id'),
                    'count': doc.get('count')
                })
            
            # Ëé∑ÂèñÊàêÁ´ã‰ª•Êù•Êî∂ÁõäTOP10
            pipeline_top_return = [
                {'$match': {'Âë®Êúü': 'ÊàêÁ´ã‰ª•Êù•', 'Êú¨‰∫ßÂìÅÂå∫Èó¥Êî∂Áõä': {'$ne': None}}},
                {'$sort': {'Êú¨‰∫ßÂìÅÂå∫Èó¥Êî∂Áõä': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$Âü∫Èáë‰ª£Á†Å',
                    'return': '$Êú¨‰∫ßÂìÅÂå∫Èó¥Êî∂Áõä',
                    'max_drawdown': '$Êú¨‰∫ßÂìÅÊúÄÂ§ßÂõûÊíí',
                    'ranking': '$Âë®ÊúüÊî∂ÁõäÂêåÁ±ªÊéíÂêç'
                }}
            ]
            
            top_return_since = []
            async for doc in self.col_fund_individual_achievement_xq.aggregate(pipeline_top_return):
                top_return_since.append({
                    'code': doc.get('code'),
                    'return': doc.get('return'),
                    'max_drawdown': doc.get('max_drawdown'),
                    'ranking': doc.get('ranking')
                })
            
            # Ëé∑ÂèñÊúÄÂ∞èÂõûÊí§TOP10(ÊàêÁ´ã‰ª•Êù•)
            pipeline_min_drawdown = [
                {'$match': {'Âë®Êúü': 'ÊàêÁ´ã‰ª•Êù•', 'Êú¨‰∫ßÂìÅÊúÄÂ§ßÂõûÊíí': {'$ne': None}}},
                {'$sort': {'Êú¨‰∫ßÂìÅÊúÄÂ§ßÂõûÊíí': 1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$Âü∫Èáë‰ª£Á†Å',
                    'return': '$Êú¨‰∫ßÂìÅÂå∫Èó¥Êî∂Áõä',
                    'max_drawdown': '$Êú¨‰∫ßÂìÅÊúÄÂ§ßÂõûÊíí',
                    'ranking': '$Âë®ÊúüÊî∂ÁõäÂêåÁ±ªÊéíÂêç'
                }}
            ]
            
            min_drawdown_since = []
            async for doc in self.col_fund_individual_achievement_xq.aggregate(pipeline_min_drawdown):
                min_drawdown_since.append({
                    'code': doc.get('code'),
                    'return': doc.get('return'),
                    'max_drawdown': doc.get('max_drawdown'),
                    'ranking': doc.get('ranking')
                })
            
            return {
                'total_count': total_count,
                'unique_funds': unique_funds,
                'performance_types': performance_types,
                'top_return_since': top_return_since,
                'min_drawdown_since': min_drawdown_since
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫Èáë‰∏öÁª©ÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_value_estimation_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂáÄÂÄº‰º∞ÁÆóÊï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂáÄÂÄº‰º∞ÁÆóÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°ÂáÄÂÄº‰º∞ÁÆóÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    trade_date = str(doc.get('‰∫§ÊòìÊó•', ''))
                    doc['code'] = fund_code
                    doc['trade_date'] = trade_date
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_value_estimation_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'trade_date': trade_date},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_value_estimation_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°ÂáÄÂÄº‰º∞ÁÆóÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂáÄÂÄº‰º∞ÁÆóÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_value_estimation_em_data(self) -> int:
        """Ê∏ÖÁ©∫ÂáÄÂÄº‰º∞ÁÆóÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_value_estimation_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÂáÄÂÄº‰º∞ÁÆóÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÂáÄÂÄº‰º∞ÁÆóÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_value_estimation_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂáÄÂÄº‰º∞ÁÆóÁªüËÆ°"""
        try:
            total_count = await self.col_fund_value_estimation_em.count_documents({})
            
            # Ëé∑Âèñ‰∫§ÊòìÊó•ËåÉÂõ¥
            pipeline_date = [
                {'$group': {'_id': None, 'earliest': {'$min': '$‰∫§ÊòìÊó•'}, 'latest': {'$max': '$‰∫§ÊòìÊó•'}}}
            ]
            
            earliest_date = None
            latest_date = None
            async for doc in self.col_fund_value_estimation_em.aggregate(pipeline_date):
                earliest_date = doc.get('earliest')
                latest_date = doc.get('latest')
            
            # Ëé∑Âèñ‰º∞ÁÆóÂ¢ûÈïøÁéáTOP10
            pipeline_top_growth = [
                {'$match': {'‰∫§ÊòìÊó•-‰º∞ÁÆóÊï∞ÊçÆ-‰º∞ÁÆóÂ¢ûÈïøÁéá': {'$ne': None}}},
                {'$sort': {'‰∫§ÊòìÊó•-‰º∞ÁÆóÊï∞ÊçÆ-‰º∞ÁÆóÂ¢ûÈïøÁéá': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$Âü∫Èáë‰ª£Á†Å',
                    'name': '$Âü∫ÈáëÂêçÁß∞',
                    'estimated_value': '$‰∫§ÊòìÊó•-‰º∞ÁÆóÊï∞ÊçÆ-‰º∞ÁÆóÂÄº',
                    'estimated_growth': '$‰∫§ÊòìÊó•-‰º∞ÁÆóÊï∞ÊçÆ-‰º∞ÁÆóÂ¢ûÈïøÁéá',
                    'published_nav': '$‰∫§ÊòìÊó•-ÂÖ¨Â∏ÉÊï∞ÊçÆ-Âçï‰ΩçÂáÄÂÄº',
                    'deviation': '$‰º∞ÁÆóÂÅèÂ∑Æ'
                }}
            ]
            
            top_estimated_growth = []
            async for doc in self.col_fund_value_estimation_em.aggregate(pipeline_top_growth):
                top_estimated_growth.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'estimated_value': doc.get('estimated_value'),
                    'estimated_growth': doc.get('estimated_growth'),
                    'published_nav': doc.get('published_nav'),
                    'deviation': doc.get('deviation')
                })
            
            # Ëé∑Âèñ‰º∞ÁÆóÂÅèÂ∑ÆÊúÄÂ∞èTOP10ÔºàÁªùÂØπÂÄºÔºâ
            pipeline_min_deviation = [
                {'$match': {'‰º∞ÁÆóÂÅèÂ∑Æ': {'$ne': None}}},
                {'$addFields': {'abs_deviation': {'$abs': '$‰º∞ÁÆóÂÅèÂ∑Æ'}}},
                {'$sort': {'abs_deviation': 1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$Âü∫Èáë‰ª£Á†Å',
                    'name': '$Âü∫ÈáëÂêçÁß∞',
                    'estimated_value': '$‰∫§ÊòìÊó•-‰º∞ÁÆóÊï∞ÊçÆ-‰º∞ÁÆóÂÄº',
                    'estimated_growth': '$‰∫§ÊòìÊó•-‰º∞ÁÆóÊï∞ÊçÆ-‰º∞ÁÆóÂ¢ûÈïøÁéá',
                    'published_nav': '$‰∫§ÊòìÊó•-ÂÖ¨Â∏ÉÊï∞ÊçÆ-Âçï‰ΩçÂáÄÂÄº',
                    'deviation': '$‰º∞ÁÆóÂÅèÂ∑Æ'
                }}
            ]
            
            min_deviation_funds = []
            async for doc in self.col_fund_value_estimation_em.aggregate(pipeline_min_deviation):
                min_deviation_funds.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'estimated_value': doc.get('estimated_value'),
                    'estimated_growth': doc.get('estimated_growth'),
                    'published_nav': doc.get('published_nav'),
                    'deviation': doc.get('deviation')
                })
            
            return {
                'total_count': total_count,
                'earliest_date': earliest_date,
                'latest_date': latest_date,
                'top_estimated_growth': top_estimated_growth,
                'min_deviation_funds': min_deviation_funds
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂáÄÂÄº‰º∞ÁÆóÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_individual_analysis_xq_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëÊï∞ÊçÆÂàÜÊûêÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÊï∞ÊçÆÂàÜÊûêÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÊï∞ÊçÆÂàÜÊûê...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    period = str(doc.get('Âë®Êúü', ''))
                    doc['code'] = fund_code
                    doc['period'] = period
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_individual_analysis_xq'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'period': period},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_individual_analysis_xq.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëÊï∞ÊçÆÂàÜÊûê")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÊï∞ÊçÆÂàÜÊûêÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_individual_analysis_xq_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëÊï∞ÊçÆÂàÜÊûê"""
        try:
            result = await self.col_fund_individual_analysis_xq.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÊï∞ÊçÆÂàÜÊûê")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÊï∞ÊçÆÂàÜÊûêÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_individual_analysis_xq_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëÊï∞ÊçÆÂàÜÊûêÁªüËÆ°"""
        try:
            total_count = await self.col_fund_individual_analysis_xq.count_documents({})
            
            # Ëé∑ÂèñÂîØ‰∏ÄÂü∫ÈáëÊï∞
            unique_funds = await self.col_fund_individual_analysis_xq.distinct('code')
            
            # Ëé∑ÂèñÂë®ÊúüÂàÜÂ∏É
            pipeline_periods = [
                {'$group': {'_id': '$Âë®Êúü', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            period_distribution = []
            async for doc in self.col_fund_individual_analysis_xq.aggregate(pipeline_periods):
                period_distribution.append({
                    'period': doc['_id'],
                    'count': doc['count']
                })
            
            # Ëé∑ÂèñÂπ¥ÂåñÂ§èÊôÆÊØîÁéáTOP10
            pipeline_top_sharpe = [
                {'$match': {'Âπ¥ÂåñÂ§èÊôÆÊØîÁéá': {'$ne': None}}},
                {'$sort': {'Âπ¥ÂåñÂ§èÊôÆÊØîÁéá': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$code',
                    'period': '$Âë®Êúü',
                    'sharpe_ratio': '$Âπ¥ÂåñÂ§èÊôÆÊØîÁéá',
                    'volatility': '$Âπ¥ÂåñÊ≥¢Âä®Áéá',
                    'max_drawdown': '$ÊúÄÂ§ßÂõûÊí§',
                    'risk_return_ratio': '$ËæÉÂêåÁ±ªÈ£éÈô©Êî∂ÁõäÊØî'
                }}
            ]
            
            top_sharpe_ratio = []
            async for doc in self.col_fund_individual_analysis_xq.aggregate(pipeline_top_sharpe):
                top_sharpe_ratio.append({
                    'code': doc.get('code'),
                    'period': doc.get('period'),
                    'sharpe_ratio': doc.get('sharpe_ratio'),
                    'volatility': doc.get('volatility'),
                    'max_drawdown': doc.get('max_drawdown'),
                    'risk_return_ratio': doc.get('risk_return_ratio')
                })
            
            # Ëé∑ÂèñÊúÄÂ∞èÂõûÊí§TOP10ÔºàÊúÄÂ§ßÂõûÊí§ÁöÑÁªùÂØπÂÄºÊúÄÂ∞èÔºâ
            pipeline_min_drawdown = [
                {'$match': {'ÊúÄÂ§ßÂõûÊí§': {'$ne': None}}},
                {'$addFields': {'abs_drawdown': {'$abs': '$ÊúÄÂ§ßÂõûÊí§'}}},
                {'$sort': {'abs_drawdown': 1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$code',
                    'period': '$Âë®Êúü',
                    'max_drawdown': '$ÊúÄÂ§ßÂõûÊí§',
                    'sharpe_ratio': '$Âπ¥ÂåñÂ§èÊôÆÊØîÁéá',
                    'volatility': '$Âπ¥ÂåñÊ≥¢Âä®Áéá',
                    'anti_risk': '$ËæÉÂêåÁ±ªÊäóÈ£éÈô©Ê≥¢Âä®'
                }}
            ]
            
            min_drawdown_funds = []
            async for doc in self.col_fund_individual_analysis_xq.aggregate(pipeline_min_drawdown):
                min_drawdown_funds.append({
                    'code': doc.get('code'),
                    'period': doc.get('period'),
                    'max_drawdown': doc.get('max_drawdown'),
                    'sharpe_ratio': doc.get('sharpe_ratio'),
                    'volatility': doc.get('volatility'),
                    'anti_risk': doc.get('anti_risk')
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'period_distribution': period_distribution,
                'top_sharpe_ratio': top_sharpe_ratio,
                'min_drawdown_funds': min_drawdown_funds
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÊï∞ÊçÆÂàÜÊûêÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_individual_profit_probability_xq_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëÁõàÂà©Ê¶ÇÁéáÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÁõàÂà©Ê¶ÇÁéáÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÁõàÂà©Ê¶ÇÁéá...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    holding_period = str(doc.get('ÊåÅÊúâÊó∂Èïø', ''))
                    doc['code'] = fund_code
                    doc['holding_period'] = holding_period
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_individual_profit_probability_xq'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'holding_period': holding_period},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_individual_profit_probability_xq.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëÁõàÂà©Ê¶ÇÁéá")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÁõàÂà©Ê¶ÇÁéáÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_individual_profit_probability_xq_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëÁõàÂà©Ê¶ÇÁéá"""
        try:
            result = await self.col_fund_individual_profit_probability_xq.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÁõàÂà©Ê¶ÇÁéá")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÁõàÂà©Ê¶ÇÁéáÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_individual_profit_probability_xq_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëÁõàÂà©Ê¶ÇÁéáÁªüËÆ°"""
        try:
            total_count = await self.col_fund_individual_profit_probability_xq.count_documents({})
            
            # Ëé∑ÂèñÂîØ‰∏ÄÂü∫ÈáëÊï∞
            unique_funds = await self.col_fund_individual_profit_probability_xq.distinct('code')
            
            # Ëé∑ÂèñÊåÅÊúâÊó∂ÈïøÂàÜÂ∏É
            pipeline_periods = [
                {'$group': {'_id': '$ÊåÅÊúâÊó∂Èïø', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            holding_period_distribution = []
            async for doc in self.col_fund_individual_profit_probability_xq.aggregate(pipeline_periods):
                holding_period_distribution.append({
                    'holding_period': doc['_id'],
                    'count': doc['count']
                })
            
            # Ëé∑ÂèñÁõàÂà©Ê¶ÇÁéáTOP10ÔºàÈïøÊúüÊåÅÊúâÔºâ
            pipeline_top_probability = [
                {'$match': {'ÁõàÂà©Ê¶ÇÁéá': {'$ne': None}}},
                {'$sort': {'ÁõàÂà©Ê¶ÇÁéá': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$code',
                    'holding_period': '$ÊåÅÊúâÊó∂Èïø',
                    'profit_probability': '$ÁõàÂà©Ê¶ÇÁéá',
                    'average_return': '$Âπ≥ÂùáÊî∂Áõä'
                }}
            ]
            
            top_profit_probability = []
            async for doc in self.col_fund_individual_profit_probability_xq.aggregate(pipeline_top_probability):
                top_profit_probability.append({
                    'code': doc.get('code'),
                    'holding_period': doc.get('holding_period'),
                    'profit_probability': doc.get('profit_probability'),
                    'average_return': doc.get('average_return')
                })
            
            # Ëé∑ÂèñÂπ≥ÂùáÊî∂ÁõäTOP10
            pipeline_top_return = [
                {'$match': {'Âπ≥ÂùáÊî∂Áõä': {'$ne': None}}},
                {'$sort': {'Âπ≥ÂùáÊî∂Áõä': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$code',
                    'holding_period': '$ÊåÅÊúâÊó∂Èïø',
                    'profit_probability': '$ÁõàÂà©Ê¶ÇÁéá',
                    'average_return': '$Âπ≥ÂùáÊî∂Áõä'
                }}
            ]
            
            top_average_return = []
            async for doc in self.col_fund_individual_profit_probability_xq.aggregate(pipeline_top_return):
                top_average_return.append({
                    'code': doc.get('code'),
                    'holding_period': doc.get('holding_period'),
                    'profit_probability': doc.get('profit_probability'),
                    'average_return': doc.get('average_return')
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'holding_period_distribution': holding_period_distribution,
                'top_profit_probability': top_profit_probability,
                'top_average_return': top_average_return
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÁõàÂà©Ê¶ÇÁéáÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_individual_detail_hold_xq_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëÊåÅ‰ªìËµÑ‰∫ßÊØî‰æãÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÊåÅ‰ªìËµÑ‰∫ßÊØî‰æãÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÊåÅ‰ªìËµÑ‰∫ßÊØî‰æã...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    date_str = str(doc.get('Êó•Êúü', ''))
                    asset_type = str(doc.get('ËµÑ‰∫ßÁ±ªÂûã', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['asset_type'] = asset_type
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_individual_detail_hold_xq'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'date': date_str, 'asset_type': asset_type},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_individual_detail_hold_xq.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëÊåÅ‰ªìËµÑ‰∫ßÊØî‰æã")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÊåÅ‰ªìËµÑ‰∫ßÊØî‰æãÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_individual_detail_hold_xq_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëÊåÅ‰ªìËµÑ‰∫ßÊØî‰æã"""
        try:
            result = await self.col_fund_individual_detail_hold_xq.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÊåÅ‰ªìËµÑ‰∫ßÊØî‰æã")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÊåÅ‰ªìËµÑ‰∫ßÊØî‰æãÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_individual_detail_hold_xq_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëÊåÅ‰ªìËµÑ‰∫ßÊØî‰æãÁªüËÆ°"""
        try:
            total_count = await self.col_fund_individual_detail_hold_xq.count_documents({})
            
            # Ëé∑ÂèñÂîØ‰∏ÄÂü∫ÈáëÊï∞
            unique_funds = await self.col_fund_individual_detail_hold_xq.distinct('code')
            
            # Ëé∑ÂèñÂîØ‰∏ÄÊó•ÊúüÊï∞
            unique_dates = await self.col_fund_individual_detail_hold_xq.distinct('date')
            
            # Ëé∑ÂèñËµÑ‰∫ßÁ±ªÂûãÂàÜÂ∏É
            pipeline_asset_types = [
                {'$group': {'_id': '$ËµÑ‰∫ßÁ±ªÂûã', 'count': {'$sum': 1}, 'avg_position': {'$avg': '$‰ªì‰ΩçÂç†ÊØî'}}},
                {'$sort': {'count': -1}}
            ]
            
            asset_type_distribution = []
            async for doc in self.col_fund_individual_detail_hold_xq.aggregate(pipeline_asset_types):
                asset_type_distribution.append({
                    'asset_type': doc['_id'],
                    'count': doc['count'],
                    'avg_position': round(doc.get('avg_position', 0), 2) if doc.get('avg_position') else None
                })
            
            # Ëé∑ÂèñÊúÄÊñ∞Êó•Êúü
            pipeline_latest_date = [
                {'$sort': {'date': -1}},
                {'$limit': 1},
                {'$project': {'date': 1}}
            ]
            
            latest_date = None
            async for doc in self.col_fund_individual_detail_hold_xq.aggregate(pipeline_latest_date):
                latest_date = doc.get('date')
            
            # Ëé∑ÂèñËÇ°Á•®‰ªì‰ΩçÊúÄÈ´òÁöÑÂü∫ÈáëTOP10ÔºàÊúÄÊñ∞Êó•ÊúüÔºâ
            pipeline_top_stock = [
                {'$match': {'ËµÑ‰∫ßÁ±ªÂûã': 'ËÇ°Á•®'}},
                {'$sort': {'date': -1, '‰ªì‰ΩçÂç†ÊØî': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$code',
                    'date': '$date',
                    'asset_type': '$ËµÑ‰∫ßÁ±ªÂûã',
                    'position': '$‰ªì‰ΩçÂç†ÊØî'
                }}
            ]
            
            top_stock_position = []
            async for doc in self.col_fund_individual_detail_hold_xq.aggregate(pipeline_top_stock):
                top_stock_position.append({
                    'code': doc.get('code'),
                    'date': doc.get('date'),
                    'asset_type': doc.get('asset_type'),
                    'position': doc.get('position')
                })
            
            # Ëé∑ÂèñÂÄ∫Âà∏‰ªì‰ΩçÊúÄÈ´òÁöÑÂü∫ÈáëTOP10ÔºàÊúÄÊñ∞Êó•ÊúüÔºâ
            pipeline_top_bond = [
                {'$match': {'ËµÑ‰∫ßÁ±ªÂûã': 'ÂÄ∫Âà∏'}},
                {'$sort': {'date': -1, '‰ªì‰ΩçÂç†ÊØî': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$code',
                    'date': '$date',
                    'asset_type': '$ËµÑ‰∫ßÁ±ªÂûã',
                    'position': '$‰ªì‰ΩçÂç†ÊØî'
                }}
            ]
            
            top_bond_position = []
            async for doc in self.col_fund_individual_detail_hold_xq.aggregate(pipeline_top_bond):
                top_bond_position.append({
                    'code': doc.get('code'),
                    'date': doc.get('date'),
                    'asset_type': doc.get('asset_type'),
                    'position': doc.get('position')
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'unique_dates': len(unique_dates),
                'latest_date': latest_date,
                'asset_type_distribution': asset_type_distribution,
                'top_stock_position': top_stock_position,
                'top_bond_position': top_bond_position
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÊåÅ‰ªìËµÑ‰∫ßÊØî‰æãÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_overview_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëÂü∫Êú¨Ê¶ÇÂÜµÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÂü∫Êú¨Ê¶ÇÂÜµÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÂü∫Êú¨Ê¶ÇÂÜµ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_overview_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_overview_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëÂü∫Êú¨Ê¶ÇÂÜµ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÂü∫Êú¨Ê¶ÇÂÜµÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_overview_em_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëÂü∫Êú¨Ê¶ÇÂÜµ"""
        try:
            result = await self.col_fund_overview_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÂü∫Êú¨Ê¶ÇÂÜµ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÂü∫Êú¨Ê¶ÇÂÜµÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_overview_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëÂü∫Êú¨Ê¶ÇÂÜµÁªüËÆ°"""
        try:
            total_count = await self.col_fund_overview_em.count_documents({})
            
            # Ëé∑ÂèñÂü∫ÈáëÁ±ªÂûãÂàÜÂ∏É
            pipeline_fund_types = [
                {'$group': {'_id': '$Âü∫ÈáëÁ±ªÂûã', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            fund_type_distribution = []
            async for doc in self.col_fund_overview_em.aggregate(pipeline_fund_types):
                fund_type_distribution.append({
                    'fund_type': doc['_id'],
                    'count': doc['count']
                })
            
            # Ëé∑ÂèñÂü∫ÈáëÁÆ°ÁêÜ‰∫∫ÂàÜÂ∏ÉÔºàTOP10Ôºâ
            pipeline_managers = [
                {'$group': {'_id': '$Âü∫ÈáëÁÆ°ÁêÜ‰∫∫', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}},
                {'$limit': 10}
            ]
            
            top_managers = []
            async for doc in self.col_fund_overview_em.aggregate(pipeline_managers):
                top_managers.append({
                    'manager': doc['_id'],
                    'count': doc['count']
                })
            
            # Ëé∑ÂèñÂü∫ÈáëËßÑÊ®°TOP10
            pipeline_top_scale = [
                {'$match': {'Âü∫ÈáëËßÑÊ®°': {'$ne': None}}},
                {'$sort': {'Âü∫ÈáëËßÑÊ®°': -1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$code',
                    'name': '$Âü∫ÈáëÁÆÄÁß∞',
                    'scale': '$Âü∫ÈáëËßÑÊ®°',
                    'manager': '$Âü∫ÈáëÁÆ°ÁêÜ‰∫∫',
                    'fund_type': '$Âü∫ÈáëÁ±ªÂûã'
                }}
            ]
            
            top_scale_funds = []
            async for doc in self.col_fund_overview_em.aggregate(pipeline_top_scale):
                top_scale_funds.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'scale': doc.get('scale'),
                    'manager': doc.get('manager'),
                    'fund_type': doc.get('fund_type')
                })
            
            # Ëé∑ÂèñÊàêÁ´ãÊó•ÊúüÊúÄÊó©ÁöÑÂü∫ÈáëTOP10
            pipeline_oldest = [
                {'$match': {'ÊàêÁ´ãÊó•Êúü': {'$ne': None}}},
                {'$sort': {'ÊàêÁ´ãÊó•Êúü': 1}},
                {'$limit': 10},
                {'$project': {
                    'code': '$code',
                    'name': '$Âü∫ÈáëÁÆÄÁß∞',
                    'established_date': '$ÊàêÁ´ãÊó•Êúü',
                    'manager': '$Âü∫ÈáëÁÆ°ÁêÜ‰∫∫',
                    'fund_type': '$Âü∫ÈáëÁ±ªÂûã'
                }}
            ]
            
            oldest_funds = []
            async for doc in self.col_fund_overview_em.aggregate(pipeline_oldest):
                oldest_funds.append({
                    'code': doc.get('code'),
                    'name': doc.get('name'),
                    'established_date': doc.get('established_date'),
                    'manager': doc.get('manager'),
                    'fund_type': doc.get('fund_type')
                })
            
            return {
                'total_count': total_count,
                'fund_type_distribution': fund_type_distribution,
                'top_managers': top_managers,
                'top_scale_funds': top_scale_funds,
                'oldest_funds': oldest_funds
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÂü∫Êú¨Ê¶ÇÂÜµÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_fee_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫Èáë‰∫§ÊòìË¥πÁéáÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫Èáë‰∫§ÊòìË¥πÁéáÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫Èáë‰∫§ÊòìË¥πÁéá...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    fee_type = str(doc.get('Ë¥πÁî®Á±ªÂûã', ''))
                    condition = str(doc.get('Êù°‰ª∂', ''))
                    doc['code'] = fund_code
                    doc['fee_type'] = fee_type
                    doc['condition'] = condition
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_fee_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'fee_type': fee_type, 'condition': condition},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_fee_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫Èáë‰∫§ÊòìË¥πÁéá")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫Èáë‰∫§ÊòìË¥πÁéáÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_fee_em_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫Èáë‰∫§ÊòìË¥πÁéá"""
        try:
            result = await self.col_fund_fee_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫Èáë‰∫§ÊòìË¥πÁéá")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫Èáë‰∫§ÊòìË¥πÁéáÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_fee_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫Èáë‰∫§ÊòìË¥πÁéáÁªüËÆ°"""
        try:
            total_count = await self.col_fund_fee_em.count_documents({})
            
            # Ëé∑ÂèñÂîØ‰∏ÄÂü∫ÈáëÊï∞
            unique_funds = await self.col_fund_fee_em.distinct('code')
            
            # Ëé∑ÂèñË¥πÁî®Á±ªÂûãÂàÜÂ∏É
            pipeline_fee_types = [
                {'$group': {'_id': '$Ë¥πÁî®Á±ªÂûã', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            fee_type_distribution = []
            async for doc in self.col_fund_fee_em.aggregate(pipeline_fee_types):
                fee_type_distribution.append({
                    'fee_type': doc['_id'],
                    'count': doc['count']
                })
            
            # Ëé∑ÂèñÁî≥Ë¥≠Ë¥πÊúÄ‰ΩéÁöÑÂü∫ÈáëTOP10
            pipeline_lowest_purchase = [
                {'$match': {'Ë¥πÁî®Á±ªÂûã': 'Áî≥Ë¥≠Ë¥π', '‰ºòÊÉ†Ë¥πÁéá': {'$ne': None}}},
                {'$group': {'_id': '$code', 'avg_fee': {'$avg': '$‰ºòÊÉ†Ë¥πÁéá'}}},
                {'$sort': {'avg_fee': 1}},
                {'$limit': 10}
            ]
            
            lowest_purchase_fee_funds = []
            async for doc in self.col_fund_fee_em.aggregate(pipeline_lowest_purchase):
                lowest_purchase_fee_funds.append({
                    'code': doc['_id'],
                    'avg_fee': round(doc.get('avg_fee', 0), 3) if doc.get('avg_fee') else None
                })
            
            # Ëé∑ÂèñËµéÂõûË¥πÊúÄ‰ΩéÁöÑÂü∫ÈáëTOP10
            pipeline_lowest_redemption = [
                {'$match': {'Ë¥πÁî®Á±ªÂûã': 'ËµéÂõûË¥π', 'Ë¥πÁéá': {'$ne': None}}},
                {'$group': {'_id': '$code', 'avg_fee': {'$avg': '$Ë¥πÁéá'}}},
                {'$sort': {'avg_fee': 1}},
                {'$limit': 10}
            ]
            
            lowest_redemption_fee_funds = []
            async for doc in self.col_fund_fee_em.aggregate(pipeline_lowest_redemption):
                lowest_redemption_fee_funds.append({
                    'code': doc['_id'],
                    'avg_fee': round(doc.get('avg_fee', 0), 3) if doc.get('avg_fee') else None
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'fee_type_distribution': fee_type_distribution,
                'lowest_purchase_fee_funds': lowest_purchase_fee_funds,
                'lowest_redemption_fee_funds': lowest_redemption_fee_funds
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫Èáë‰∫§ÊòìË¥πÁéáÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_individual_detail_info_xq_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫Èáë‰∫§ÊòìËßÑÂàôÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫Èáë‰∫§ÊòìËßÑÂàôÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫Èáë‰∫§ÊòìËßÑÂàô...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    fee_type = str(doc.get('Ë¥πÁî®Á±ªÂûã', ''))
                    doc['code'] = fund_code
                    doc['fee_type'] = fee_type
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_individual_detail_info_xq'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'fee_type': fee_type},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_individual_detail_info_xq.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫Èáë‰∫§ÊòìËßÑÂàô")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫Èáë‰∫§ÊòìËßÑÂàôÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_individual_detail_info_xq_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫Èáë‰∫§ÊòìËßÑÂàô"""
        try:
            result = await self.col_fund_individual_detail_info_xq.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫Èáë‰∫§ÊòìËßÑÂàô")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫Èáë‰∫§ÊòìËßÑÂàôÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_individual_detail_info_xq_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫Èáë‰∫§ÊòìËßÑÂàôÁªüËÆ°"""
        try:
            total_count = await self.col_fund_individual_detail_info_xq.count_documents({})
            
            # Ëé∑ÂèñÂîØ‰∏ÄÂü∫ÈáëÊï∞
            unique_funds = await self.col_fund_individual_detail_info_xq.distinct('code')
            
            # Ëé∑ÂèñË¥πÁî®Á±ªÂûãÂàÜÂ∏É
            pipeline_fee_types = [
                {'$group': {'_id': '$Ë¥πÁî®Á±ªÂûã', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            fee_type_distribution = []
            async for doc in self.col_fund_individual_detail_info_xq.aggregate(pipeline_fee_types):
                fee_type_distribution.append({
                    'fee_type': doc['_id'],
                    'count': doc['count']
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'fee_type_distribution': fee_type_distribution
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫Èáë‰∫§ÊòìËßÑÂàôÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_portfolio_hold_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëÊåÅ‰ªìÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÊåÅ‰ªìÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÊåÅ‰ªì...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    stock_code = str(doc.get('ËÇ°Á•®‰ª£Á†Å', ''))
                    quarter = str(doc.get('Â≠£Â∫¶', ''))
                    doc['code'] = fund_code
                    doc['stock_code'] = stock_code
                    doc['quarter'] = quarter
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_portfolio_hold_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'stock_code': stock_code, 'quarter': quarter},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_portfolio_hold_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëÊåÅ‰ªì")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÊåÅ‰ªìÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_portfolio_hold_em_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëÊåÅ‰ªì"""
        try:
            result = await self.col_fund_portfolio_hold_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÊåÅ‰ªì")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÊåÅ‰ªìÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_portfolio_hold_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëÊåÅ‰ªìÁªüËÆ°"""
        try:
            total_count = await self.col_fund_portfolio_hold_em.count_documents({})
            
            # Ëé∑ÂèñÂîØ‰∏ÄÂü∫ÈáëÊï∞
            unique_funds = await self.col_fund_portfolio_hold_em.distinct('code')
            
            # Ëé∑ÂèñÂîØ‰∏ÄËÇ°Á•®Êï∞
            unique_stocks = await self.col_fund_portfolio_hold_em.distinct('stock_code')
            
            # Ëé∑ÂèñÂ≠£Â∫¶ÂàÜÂ∏É
            pipeline_quarters = [
                {'$group': {'_id': '$Â≠£Â∫¶', 'count': {'$sum': 1}}},
                {'$sort': {'_id': -1}}
            ]
            
            quarter_distribution = []
            async for doc in self.col_fund_portfolio_hold_em.aggregate(pipeline_quarters):
                quarter_distribution.append({
                    'quarter': doc['_id'],
                    'count': doc['count']
                })
            
            # Ëé∑ÂèñÊúÄÂèóÂü∫ÈáëÈùíÁùêÁöÑËÇ°Á•®TOP10
            pipeline_top_stocks = [
                {'$group': {'_id': '$ËÇ°Á•®‰ª£Á†Å', 'stock_name': {'$first': '$ËÇ°Á•®ÂêçÁß∞'}, 'fund_count': {'$sum': 1}}},
                {'$sort': {'fund_count': -1}},
                {'$limit': 10}
            ]
            
            top_stocks = []
            async for doc in self.col_fund_portfolio_hold_em.aggregate(pipeline_top_stocks):
                top_stocks.append({
                    'stock_code': doc['_id'],
                    'stock_name': doc.get('stock_name'),
                    'fund_count': doc['fund_count']
                })
            
            # Ëé∑ÂèñÊåÅ‰ªìÂç†ÊØîÊúÄÈ´òÁöÑËÆ∞ÂΩïTOP10
            pipeline_top_holdings = [
                {'$match': {'ÊåÅ‰ªìÂç†ÊØî': {'$ne': None}}},
                {'$sort': {'ÊåÅ‰ªìÂç†ÊØî': -1}},
                {'$limit': 10},
                {'$project': {'Âü∫Èáë‰ª£Á†Å': 1, 'ËÇ°Á•®‰ª£Á†Å': 1, 'ËÇ°Á•®ÂêçÁß∞': 1, 'Â≠£Â∫¶': 1, 'ÊåÅ‰ªìÂç†ÊØî': 1}}
            ]
            
            top_holdings = []
            async for doc in self.col_fund_portfolio_hold_em.aggregate(pipeline_top_holdings):
                top_holdings.append({
                    'fund_code': doc.get('Âü∫Èáë‰ª£Á†Å'),
                    'stock_code': doc.get('ËÇ°Á•®‰ª£Á†Å'),
                    'stock_name': doc.get('ËÇ°Á•®ÂêçÁß∞'),
                    'quarter': doc.get('Â≠£Â∫¶'),
                    'holding_ratio': doc.get('ÊåÅ‰ªìÂç†ÊØî')
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'unique_stocks': len(unique_stocks),
                'quarter_distribution': quarter_distribution,
                'top_stocks': top_stocks,
                'top_holdings': top_holdings
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÊåÅ‰ªìÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_portfolio_bond_hold_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂÄ∫Âà∏ÊåÅ‰ªìÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂÄ∫Âà∏ÊåÅ‰ªìÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°ÂÄ∫Âà∏ÊåÅ‰ªì...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    bond_code = str(doc.get('ÂÄ∫Âà∏‰ª£Á†Å', ''))
                    quarter = str(doc.get('Â≠£Â∫¶', ''))
                    doc['code'] = fund_code
                    doc['bond_code'] = bond_code
                    doc['quarter'] = quarter
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_portfolio_bond_hold_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'bond_code': bond_code, 'quarter': quarter},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_portfolio_bond_hold_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°ÂÄ∫Âà∏ÊåÅ‰ªì")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂÄ∫Âà∏ÊåÅ‰ªìÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_portfolio_bond_hold_em_data(self) -> int:
        """Ê∏ÖÁ©∫ÂÄ∫Âà∏ÊåÅ‰ªì"""
        try:
            result = await self.col_fund_portfolio_bond_hold_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÂÄ∫Âà∏ÊåÅ‰ªì")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÂÄ∫Âà∏ÊåÅ‰ªìÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_portfolio_bond_hold_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂÄ∫Âà∏ÊåÅ‰ªìÁªüËÆ°"""
        try:
            total_count = await self.col_fund_portfolio_bond_hold_em.count_documents({})
            
            unique_funds = await self.col_fund_portfolio_bond_hold_em.distinct('code')
            unique_bonds = await self.col_fund_portfolio_bond_hold_em.distinct('bond_code')
            
            pipeline_quarters = [
                {'$group': {'_id': '$Â≠£Â∫¶', 'count': {'$sum': 1}}},
                {'$sort': {'_id': -1}}
            ]
            
            quarter_distribution = []
            async for doc in self.col_fund_portfolio_bond_hold_em.aggregate(pipeline_quarters):
                quarter_distribution.append({
                    'quarter': doc['_id'],
                    'count': doc['count']
                })
            
            pipeline_top_bonds = [
                {'$group': {'_id': '$ÂÄ∫Âà∏‰ª£Á†Å', 'bond_name': {'$first': '$ÂÄ∫Âà∏ÂêçÁß∞'}, 'fund_count': {'$sum': 1}}},
                {'$sort': {'fund_count': -1}},
                {'$limit': 10}
            ]
            
            top_bonds = []
            async for doc in self.col_fund_portfolio_bond_hold_em.aggregate(pipeline_top_bonds):
                top_bonds.append({
                    'bond_code': doc['_id'],
                    'bond_name': doc.get('bond_name'),
                    'fund_count': doc['fund_count']
                })
            
            pipeline_top_holdings = [
                {'$match': {'ÊåÅ‰ªìÂç†ÊØî': {'$ne': None}}},
                {'$sort': {'ÊåÅ‰ªìÂç†ÊØî': -1}},
                {'$limit': 10},
                {'$project': {'Âü∫Èáë‰ª£Á†Å': 1, 'ÂÄ∫Âà∏‰ª£Á†Å': 1, 'ÂÄ∫Âà∏ÂêçÁß∞': 1, 'Â≠£Â∫¶': 1, 'ÊåÅ‰ªìÂç†ÊØî': 1}}
            ]
            
            top_holdings = []
            async for doc in self.col_fund_portfolio_bond_hold_em.aggregate(pipeline_top_holdings):
                top_holdings.append({
                    'fund_code': doc.get('Âü∫Èáë‰ª£Á†Å'),
                    'bond_code': doc.get('ÂÄ∫Âà∏‰ª£Á†Å'),
                    'bond_name': doc.get('ÂÄ∫Âà∏ÂêçÁß∞'),
                    'quarter': doc.get('Â≠£Â∫¶'),
                    'holding_ratio': doc.get('ÊåÅ‰ªìÂç†ÊØî')
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'unique_bonds': len(unique_bonds),
                'quarter_distribution': quarter_distribution,
                'top_bonds': top_bonds,
                'top_holdings': top_holdings
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂÄ∫Âà∏ÊåÅ‰ªìÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_portfolio_industry_allocation_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òË°å‰∏öÈÖçÁΩÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâË°å‰∏öÈÖçÁΩÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Ë°å‰∏öÈÖçÁΩÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    industry = str(doc.get('Ë°å‰∏öÁ±ªÂà´', ''))
                    end_date = str(doc.get('Êà™Ê≠¢Êó∂Èó¥', ''))
                    doc['code'] = fund_code
                    doc['industry'] = industry
                    doc['end_date'] = end_date
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_portfolio_industry_allocation_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'industry': industry, 'end_date': end_date},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_portfolio_industry_allocation_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Ë°å‰∏öÈÖçÁΩÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òË°å‰∏öÈÖçÁΩÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_portfolio_industry_allocation_em_data(self) -> int:
        """Ê∏ÖÁ©∫Ë°å‰∏öÈÖçÁΩÆ"""
        try:
            result = await self.col_fund_portfolio_industry_allocation_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Ë°å‰∏öÈÖçÁΩÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Ë°å‰∏öÈÖçÁΩÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_portfolio_industry_allocation_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñË°å‰∏öÈÖçÁΩÆÁªüËÆ°"""
        try:
            total_count = await self.col_fund_portfolio_industry_allocation_em.count_documents({})
            
            unique_funds = await self.col_fund_portfolio_industry_allocation_em.distinct('code')
            unique_industries = await self.col_fund_portfolio_industry_allocation_em.distinct('industry')
            
            pipeline_industries = [
                {'$group': {'_id': '$Ë°å‰∏öÁ±ªÂà´', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}},
                {'$limit': 20}
            ]
            
            industry_distribution = []
            async for doc in self.col_fund_portfolio_industry_allocation_em.aggregate(pipeline_industries):
                industry_distribution.append({
                    'industry': doc['_id'],
                    'count': doc['count']
                })
            
            pipeline_top_allocation = [
                {'$match': {'Âç†ÂáÄÂÄºÊØî‰æã': {'$ne': None}}},
                {'$sort': {'Âç†ÂáÄÂÄºÊØî‰æã': -1}},
                {'$limit': 10},
                {'$project': {'Âü∫Èáë‰ª£Á†Å': 1, 'Ë°å‰∏öÁ±ªÂà´': 1, 'Êà™Ê≠¢Êó∂Èó¥': 1, 'Âç†ÂáÄÂÄºÊØî‰æã': 1}}
            ]
            
            top_allocations = []
            async for doc in self.col_fund_portfolio_industry_allocation_em.aggregate(pipeline_top_allocation):
                top_allocations.append({
                    'fund_code': doc.get('Âü∫Èáë‰ª£Á†Å'),
                    'industry': doc.get('Ë°å‰∏öÁ±ªÂà´'),
                    'end_date': doc.get('Êà™Ê≠¢Êó∂Èó¥'),
                    'ratio': doc.get('Âç†ÂáÄÂÄºÊØî‰æã')
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'unique_industries': len(unique_industries),
                'industry_distribution': industry_distribution,
                'top_allocations': top_allocations
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñË°å‰∏öÈÖçÁΩÆÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def save_fund_portfolio_change_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÈáçÂ§ßÂèòÂä®Âà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÈáçÂ§ßÂèòÂä®ÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°ÈáçÂ§ßÂèòÂä®...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    stock_code = str(doc.get('ËÇ°Á•®‰ª£Á†Å', ''))
                    indicator_type = str(doc.get('ÊåáÊ†áÁ±ªÂûã', ''))
                    quarter = str(doc.get('Â≠£Â∫¶', ''))
                    doc['code'] = fund_code
                    doc['stock_code'] = stock_code
                    doc['indicator_type'] = indicator_type
                    doc['quarter'] = quarter
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_portfolio_change_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    ops.append(
                        UpdateOne(
                            {'code': fund_code, 'stock_code': stock_code, 'indicator_type': indicator_type, 'quarter': quarter},
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_portfolio_change_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°ÈáçÂ§ßÂèòÂä®")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÈáçÂ§ßÂèòÂä®Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_portfolio_change_em_data(self) -> int:
        """Ê∏ÖÁ©∫ÈáçÂ§ßÂèòÂä®"""
        try:
            result = await self.col_fund_portfolio_change_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÈáçÂ§ßÂèòÂä®")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÈáçÂ§ßÂèòÂä®Â§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_portfolio_change_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÈáçÂ§ßÂèòÂä®ÁªüËÆ°"""
        try:
            total_count = await self.col_fund_portfolio_change_em.count_documents({})
            
            unique_funds = await self.col_fund_portfolio_change_em.distinct('code')
            unique_stocks = await self.col_fund_portfolio_change_em.distinct('stock_code')
            
            pipeline_indicator_types = [
                {'$group': {'_id': '$ÊåáÊ†áÁ±ªÂûã', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            indicator_type_distribution = []
            async for doc in self.col_fund_portfolio_change_em.aggregate(pipeline_indicator_types):
                indicator_type_distribution.append({
                    'indicator_type': doc['_id'],
                    'count': doc['count']
                })
            
            pipeline_quarters = [
                {'$group': {'_id': '$Â≠£Â∫¶', 'count': {'$sum': 1}}},
                {'$sort': {'_id': -1}}
            ]
            
            quarter_distribution = []
            async for doc in self.col_fund_portfolio_change_em.aggregate(pipeline_quarters):
                quarter_distribution.append({
                    'quarter': doc['_id'],
                    'count': doc['count']
                })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'unique_stocks': len(unique_stocks),
                'indicator_type_distribution': indicator_type_distribution,
                'quarter_distribution': quarter_distribution
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÈáçÂ§ßÂèòÂä®ÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_rating_all_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëËØÑÁ∫ßÊÄªÊ±áÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëËØÑÁ∫ßÊÄªÊ±áÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëËØÑÁ∫ßÊÄªÊ±áÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    fund_code = str(doc.get('‰ª£Á†Å', ''))
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_rating_all'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫Èáë‰ª£Á†Å‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if fund_code:
                        ops.append(
                            UpdateOne(
                                {'code': fund_code},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_rating_all_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëËØÑÁ∫ßÊÄªÊ±áÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëËØÑÁ∫ßÊÄªÊ±áÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_rating_all_em_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëËØÑÁ∫ßÊÄªÊ±áÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_rating_all_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëËØÑÁ∫ßÊÄªÊ±áÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëËØÑÁ∫ßÊÄªÊ±áÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_rating_all_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëËØÑÁ∫ßÊÄªÊ±áÁªüËÆ°"""
        try:
            total_count = await self.col_fund_rating_all_em.count_documents({})
            
            unique_funds = await self.col_fund_rating_all_em.distinct('code')
            
            # ËØÑÁ∫ßÂàÜÂ∏ÉÔºàÊåâÊãõÂïÜËØÅÂà∏ËØÑÁ∫ßÔºâ
            pipeline_rating = [
                {'$group': {'_id': '$ÊãõÂïÜËØÅÂà∏', 'count': {'$sum': 1}}},
                {'$sort': {'_id': -1}}
            ]
            
            rating_distribution = []
            async for doc in self.col_fund_rating_all_em.aggregate(pipeline_rating):
                if doc['_id'] is not None:
                    rating_distribution.append({
                        'rating': doc['_id'],
                        'count': doc['count']
                    })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'rating_distribution': rating_distribution
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëËØÑÁ∫ßÊÄªÊ±áÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_rating_sh_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠ò‰∏äÊµ∑ËØÅÂà∏ËØÑÁ∫ßÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°Êúâ‰∏äÊµ∑ËØÅÂà∏ËØÑÁ∫ßÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°‰∏äÊµ∑ËØÅÂà∏ËØÑÁ∫ßÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    fund_code = str(doc.get('‰ª£Á†Å', ''))
                    date_str = str(doc.get('Êó•Êúü', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_rating_sh'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫Èáë‰ª£Á†ÅÂíåÊó•Êúü‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if fund_code and date_str:
                        ops.append(
                            UpdateOne(
                                {'code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_rating_sh_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°‰∏äÊµ∑ËØÅÂà∏ËØÑÁ∫ßÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠ò‰∏äÊµ∑ËØÅÂà∏ËØÑÁ∫ßÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_rating_sh_em_data(self) -> int:
        """Ê∏ÖÁ©∫‰∏äÊµ∑ËØÅÂà∏ËØÑÁ∫ßÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_rating_sh_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°‰∏äÊµ∑ËØÅÂà∏ËØÑÁ∫ßÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫‰∏äÊµ∑ËØÅÂà∏ËØÑÁ∫ßÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_rating_sh_em_stats(self) -> Dict[str, Any]:
        """Ëé∑Âèñ‰∏äÊµ∑ËØÅÂà∏ËØÑÁ∫ßÁªüËÆ°"""
        try:
            total_count = await self.col_fund_rating_sh_em.count_documents({})
            
            unique_funds = await self.col_fund_rating_sh_em.distinct('code')
            
            # 3Âπ¥ÊúüËØÑÁ∫ßÂàÜÂ∏É
            pipeline_rating = [
                {'$group': {'_id': '$3Âπ¥ÊúüËØÑÁ∫ß-3Âπ¥ËØÑÁ∫ß', 'count': {'$sum': 1}}},
                {'$sort': {'_id': -1}}
            ]
            
            rating_distribution = []
            async for doc in self.col_fund_rating_sh_em.aggregate(pipeline_rating):
                if doc['_id'] is not None:
                    rating_distribution.append({
                        'rating': doc['_id'],
                        'count': doc['count']
                    })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'rating_distribution': rating_distribution
            }
        except Exception as e:
            logger.error(f"Ëé∑Âèñ‰∏äÊµ∑ËØÅÂà∏ËØÑÁ∫ßÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_rating_zs_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÊãõÂïÜËØÅÂà∏ËØÑÁ∫ßÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÊãõÂïÜËØÅÂà∏ËØÑÁ∫ßÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°ÊãõÂïÜËØÅÂà∏ËØÑÁ∫ßÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    fund_code = str(doc.get('‰ª£Á†Å', ''))
                    date_str = str(doc.get('Êó•Êúü', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_rating_zs'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫Èáë‰ª£Á†ÅÂíåÊó•Êúü‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if fund_code and date_str:
                        ops.append(
                            UpdateOne(
                                {'code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_rating_zs_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°ÊãõÂïÜËØÅÂà∏ËØÑÁ∫ßÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÊãõÂïÜËØÅÂà∏ËØÑÁ∫ßÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_rating_zs_em_data(self) -> int:
        """Ê∏ÖÁ©∫ÊãõÂïÜËØÅÂà∏ËØÑÁ∫ßÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_rating_zs_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÊãõÂïÜËØÅÂà∏ËØÑÁ∫ßÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÊãõÂïÜËØÅÂà∏ËØÑÁ∫ßÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_rating_zs_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÊãõÂïÜËØÅÂà∏ËØÑÁ∫ßÁªüËÆ°"""
        try:
            total_count = await self.col_fund_rating_zs_em.count_documents({})
            
            unique_funds = await self.col_fund_rating_zs_em.distinct('code')
            
            # 3Âπ¥ÊúüËØÑÁ∫ßÂàÜÂ∏É
            pipeline_rating = [
                {'$group': {'_id': '$3Âπ¥ÊúüËØÑÁ∫ß-3Âπ¥ËØÑÁ∫ß', 'count': {'$sum': 1}}},
                {'$sort': {'_id': -1}}
            ]
            
            rating_distribution = []
            async for doc in self.col_fund_rating_zs_em.aggregate(pipeline_rating):
                if doc['_id'] is not None:
                    rating_distribution.append({
                        'rating': doc['_id'],
                        'count': doc['count']
                    })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'rating_distribution': rating_distribution
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÊãõÂïÜËØÅÂà∏ËØÑÁ∫ßÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_rating_ja_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÊµéÂÆâÈáë‰ø°ËØÑÁ∫ßÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÊµéÂÆâÈáë‰ø°ËØÑÁ∫ßÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°ÊµéÂÆâÈáë‰ø°ËØÑÁ∫ßÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    fund_code = str(doc.get('‰ª£Á†Å', ''))
                    date_str = str(doc.get('Êó•Êúü', ''))
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_rating_ja'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫Èáë‰ª£Á†ÅÂíåÊó•Êúü‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if fund_code and date_str:
                        ops.append(
                            UpdateOne(
                                {'code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_rating_ja_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°ÊµéÂÆâÈáë‰ø°ËØÑÁ∫ßÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÊµéÂÆâÈáë‰ø°ËØÑÁ∫ßÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_rating_ja_em_data(self) -> int:
        """Ê∏ÖÁ©∫ÊµéÂÆâÈáë‰ø°ËØÑÁ∫ßÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_rating_ja_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÊµéÂÆâÈáë‰ø°ËØÑÁ∫ßÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÊµéÂÆâÈáë‰ø°ËØÑÁ∫ßÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_rating_ja_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÊµéÂÆâÈáë‰ø°ËØÑÁ∫ßÁªüËÆ°"""
        try:
            total_count = await self.col_fund_rating_ja_em.count_documents({})
            
            unique_funds = await self.col_fund_rating_ja_em.distinct('code')
            
            # 3Âπ¥ÊúüËØÑÁ∫ßÂàÜÂ∏É
            pipeline_rating = [
                {'$group': {'_id': '$3Âπ¥ÊúüËØÑÁ∫ß-3Âπ¥ËØÑÁ∫ß', 'count': {'$sum': 1}}},
                {'$sort': {'_id': -1}}
            ]
            
            rating_distribution = []
            async for doc in self.col_fund_rating_ja_em.aggregate(pipeline_rating):
                if doc['_id'] is not None:
                    rating_distribution.append({
                        'rating': doc['_id'],
                        'count': doc['count']
                    })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'rating_distribution': rating_distribution
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÊµéÂÆâÈáë‰ø°ËØÑÁ∫ßÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_manager_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëÁªèÁêÜÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÁªèÁêÜÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÁªèÁêÜÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    name = str(doc.get('ÂßìÂêç', ''))
                    fund_codes = str(doc.get('Áé∞‰ªªÂü∫Èáë‰ª£Á†Å', ''))
                    doc['name'] = name
                    doc['fund_codes'] = fund_codes
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_manager_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®ÂßìÂêçÂíåÁé∞‰ªªÂü∫Èáë‰ª£Á†Å‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜÔºàÂ¶ÇÊûúÂü∫Èáë‰ª£Á†Å‰∏∫Á©∫ÔºåÂèØËÉΩÈúÄË¶ÅÂÖ∂‰ªñÊ†áËØÜÔºåÂ¶ÇÂ∫èÂè∑Ôºâ
                    unique_key = {'name': name, 'fund_codes': fund_codes}
                    
                    # Â¶ÇÊûúÊúâÂ∫èÂè∑Ôºå‰ºòÂÖà‰ΩøÁî®Â∫èÂè∑‰Ωú‰∏∫ËæÖÂä©ÔºàÂÅáËÆæÂ∫èÂè∑ÊòØÂîØ‰∏ÄIDÔºâ
                    # ‰ΩÜAPIËøîÂõûÁöÑÂ∫èÂè∑‰∏ç‰∏ÄÂÆöÊòØÂõ∫ÂÆöÁöÑIDÔºåÂèØËÉΩÊòØÊú¨Ê¨°Êü•ËØ¢ÁöÑÂ∫èÂè∑„ÄÇ
                    # ÊâÄ‰ª•ËøòÊòØÁî® name + fund_codes ÊØîËæÉÁ®≥Â¶•„ÄÇ
                    
                    ops.append(
                        UpdateOne(
                            unique_key,
                            {'$set': doc},
                            upsert=True
                        )
                    )
                
                if ops:
                    result = await self.col_fund_manager_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëÁªèÁêÜÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÁªèÁêÜÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_manager_em_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëÁªèÁêÜÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_manager_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÁªèÁêÜÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÁªèÁêÜÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_manager_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëÁªèÁêÜÁªüËÆ°"""
        try:
            total_count = await self.col_fund_manager_em.count_documents({})
            
            unique_companies = await self.col_fund_manager_em.distinct('ÊâÄÂ±ûÂÖ¨Âè∏')
            
            # Âü∫ÈáëÁªèÁêÜ‰∫∫Êï∞ÊúÄÂ§öÁöÑÂÖ¨Âè∏TOP10
            pipeline_company = [
                {'$group': {'_id': '$ÊâÄÂ±ûÂÖ¨Âè∏', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}},
                {'$limit': 10}
            ]
            
            company_distribution = []
            async for doc in self.col_fund_manager_em.aggregate(pipeline_company):
                if doc['_id']:
                    company_distribution.append({
                        'company': doc['_id'],
                        'count': doc['count']
                    })
            
            return {
                'total_count': total_count,
                'unique_companies': len(unique_companies),
                'company_distribution': company_distribution
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÁªèÁêÜÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_new_found_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÊñ∞ÂèëÂü∫ÈáëÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÊñ∞ÂèëÂü∫ÈáëÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Êñ∞ÂèëÂü∫ÈáëÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    doc['code'] = fund_code
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_new_found_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫Èáë‰ª£Á†Å‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if fund_code:
                        ops.append(
                            UpdateOne(
                                {'code': fund_code},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_new_found_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Êñ∞ÂèëÂü∫ÈáëÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÊñ∞ÂèëÂü∫ÈáëÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_new_found_em_data(self) -> int:
        """Ê∏ÖÁ©∫Êñ∞ÂèëÂü∫ÈáëÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_new_found_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Êñ∞ÂèëÂü∫ÈáëÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Êñ∞ÂèëÂü∫ÈáëÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_new_found_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÊñ∞ÂèëÂü∫ÈáëÁªüËÆ°"""
        try:
            total_count = await self.col_fund_new_found_em.count_documents({})
            
            unique_funds = await self.col_fund_new_found_em.distinct('code')
            
            # Âü∫ÈáëÁ±ªÂûãÂàÜÂ∏ÉÔºàÂÅáËÆæÊúâ'Âü∫ÈáëÁ±ªÂûã'Â≠óÊÆµÔºâ
            pipeline_type = [
                {'$group': {'_id': '$Âü∫ÈáëÁ±ªÂûã', 'count': {'$sum': 1}}},
                {'$sort': {'count': -1}}
            ]
            
            type_distribution = []
            async for doc in self.col_fund_new_found_em.aggregate(pipeline_type):
                if doc['_id']:
                    type_distribution.append({
                        'type': doc['_id'],
                        'count': doc['count']
                    })
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds),
                'type_distribution': type_distribution
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÊñ∞ÂèëÂü∫ÈáëÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_scale_open_sina_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂºÄÊîæÂºèÂü∫ÈáëËßÑÊ®°Âà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂºÄÊîæÂºèÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°ÂºÄÊîæÂºèÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    # ÂÅáËÆæÊúâÊó•ÊúüÂ≠óÊÆµÔºåÂ¶ÇÊûúÊ≤°ÊúâÔºå‰ΩøÁî®ÂΩìÂâçÊó•Êúü
                    date_str = str(doc.get('Êõ¥Êñ∞Êó•Êúü', '')) or str(doc.get('Êà™Ê≠¢Êó•Êúü', '')) or datetime.now().strftime('%Y-%m-%d')
                    
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_scale_open_sina'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫Èáë‰ª£Á†ÅÂíåÊó•Êúü‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if fund_code:
                        ops.append(
                            UpdateOne(
                                {'code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_scale_open_sina.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°ÂºÄÊîæÂºèÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂºÄÊîæÂºèÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_scale_open_sina_data(self) -> int:
        """Ê∏ÖÁ©∫ÂºÄÊîæÂºèÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆ"""
        try:
            result = await self.col_fund_scale_open_sina.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÂºÄÊîæÂºèÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÂºÄÊîæÂºèÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_scale_open_sina_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂºÄÊîæÂºèÂü∫ÈáëËßÑÊ®°ÁªüËÆ°"""
        try:
            total_count = await self.col_fund_scale_open_sina.count_documents({})
            
            unique_funds = await self.col_fund_scale_open_sina.distinct('code')
            
            # ËßÑÊ®°TOP10ÔºàÈúÄË¶ÅÊ†πÊçÆÊúÄÊñ∞Êó•ÊúüËøáÊª§ÔºåËøôÈáåÁÆÄÂçïÊåâËßÑÊ®°ÊéíÂ∫èÔºåÂÅáËÆæËßÑÊ®°Â≠óÊÆµ‰∏∫'ÊÄªËµÑ‰∫ß'ÊàñÁ±ª‰ººÔºâ
            # ÂÅáËÆæÂ≠óÊÆµÂêç‰∏∫ 'ËµÑ‰∫ßÂáÄÂÄº' Êàñ 'Âü∫ÈáëËßÑÊ®°'
            # ËøôÊòØ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÁªüËÆ°Ôºå‰∏çÈúÄË¶ÅÂ§™Â§çÊùÇ
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds)
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂºÄÊîæÂºèÂü∫ÈáëËßÑÊ®°ÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_scale_close_sina_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂ∞ÅÈó≠ÂºèÂü∫ÈáëËßÑÊ®°Âà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂ∞ÅÈó≠ÂºèÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Â∞ÅÈó≠ÂºèÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    date_str = str(doc.get('Êõ¥Êñ∞Êó•Êúü', '')) or str(doc.get('Êà™Ê≠¢Êó•Êúü', '')) or datetime.now().strftime('%Y-%m-%d')
                    
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_scale_close_sina'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫Èáë‰ª£Á†ÅÂíåÊó•Êúü‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if fund_code:
                        ops.append(
                            UpdateOne(
                                {'code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_scale_close_sina.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Â∞ÅÈó≠ÂºèÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂ∞ÅÈó≠ÂºèÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_scale_close_sina_data(self) -> int:
        """Ê∏ÖÁ©∫Â∞ÅÈó≠ÂºèÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆ"""
        try:
            result = await self.col_fund_scale_close_sina.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Â∞ÅÈó≠ÂºèÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Â∞ÅÈó≠ÂºèÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_scale_close_sina_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂ∞ÅÈó≠ÂºèÂü∫ÈáëËßÑÊ®°ÁªüËÆ°"""
        try:
            total_count = await self.col_fund_scale_close_sina.count_documents({})
            
            unique_funds = await self.col_fund_scale_close_sina.distinct('code')
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds)
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂ∞ÅÈó≠ÂºèÂü∫ÈáëËßÑÊ®°ÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_scale_structured_sina_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂàÜÁ∫ßÂ≠êÂü∫ÈáëËßÑÊ®°Âà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂàÜÁ∫ßÂ≠êÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°ÂàÜÁ∫ßÂ≠êÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', ''))
                    date_str = str(doc.get('Êõ¥Êñ∞Êó•Êúü', '')) or str(doc.get('Êà™Ê≠¢Êó•Êúü', '')) or datetime.now().strftime('%Y-%m-%d')
                    
                    doc['code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_scale_structured_sina'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫Èáë‰ª£Á†ÅÂíåÊó•Êúü‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if fund_code:
                        ops.append(
                            UpdateOne(
                                {'code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_scale_structured_sina.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°ÂàÜÁ∫ßÂ≠êÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂàÜÁ∫ßÂ≠êÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_scale_structured_sina_data(self) -> int:
        """Ê∏ÖÁ©∫ÂàÜÁ∫ßÂ≠êÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆ"""
        try:
            result = await self.col_fund_scale_structured_sina.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÂàÜÁ∫ßÂ≠êÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÂàÜÁ∫ßÂ≠êÂü∫ÈáëËßÑÊ®°Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_scale_structured_sina_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂàÜÁ∫ßÂ≠êÂü∫ÈáëËßÑÊ®°ÁªüËÆ°"""
        try:
            total_count = await self.col_fund_scale_structured_sina.count_documents({})
            
            unique_funds = await self.col_fund_scale_structured_sina.distinct('code')
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds)
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂàÜÁ∫ßÂ≠êÂü∫ÈáëËßÑÊ®°ÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_aum_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëËßÑÊ®°ËØ¶ÊÉÖÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëËßÑÊ®°ËØ¶ÊÉÖÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëËßÑÊ®°ËØ¶ÊÉÖÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    fund_company = str(doc.get('Âü∫ÈáëÂÖ¨Âè∏', ''))
                    # ÂÅáËÆæÊúâÊõ¥Êñ∞Êó•ÊúüÂ≠óÊÆµ
                    date_str = str(doc.get('Êõ¥Êñ∞Êó•Êúü', '')) or str(doc.get('Êà™Ê≠¢Êó•Êúü', '')) or datetime.now().strftime('%Y-%m-%d')
                    
                    doc['company'] = fund_company
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_aum_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫ÈáëÂÖ¨Âè∏ÂíåÊó•Êúü‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if fund_company:
                        ops.append(
                            UpdateOne(
                                {'company': fund_company, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_aum_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëËßÑÊ®°ËØ¶ÊÉÖÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëËßÑÊ®°ËØ¶ÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_aum_em_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëËßÑÊ®°ËØ¶ÊÉÖÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_aum_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëËßÑÊ®°ËØ¶ÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëËßÑÊ®°ËØ¶ÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_aum_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëËßÑÊ®°ËØ¶ÊÉÖÁªüËÆ°"""
        try:
            total_count = await self.col_fund_aum_em.count_documents({})
            
            unique_companies = await self.col_fund_aum_em.distinct('company')
            
            return {
                'total_count': total_count,
                'unique_companies': len(unique_companies)
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëËßÑÊ®°ËØ¶ÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_aum_trend_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëËßÑÊ®°Ëµ∞ÂäøÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëËßÑÊ®°Ëµ∞ÂäøÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëËßÑÊ®°Ëµ∞ÂäøÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    date_str = str(doc.get('date', '')) or str(doc.get('Êà™Ê≠¢Êó•Êúü', '')) or datetime.now().strftime('%Y-%m-%d')
                    
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_aum_trend_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Êó•Êúü‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if date_str:
                        ops.append(
                            UpdateOne(
                                {'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_aum_trend_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëËßÑÊ®°Ëµ∞ÂäøÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëËßÑÊ®°Ëµ∞ÂäøÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_aum_trend_em_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëËßÑÊ®°Ëµ∞ÂäøÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_aum_trend_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëËßÑÊ®°Ëµ∞ÂäøÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëËßÑÊ®°Ëµ∞ÂäøÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_aum_trend_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëËßÑÊ®°Ëµ∞ÂäøÁªüËÆ°"""
        try:
            total_count = await self.col_fund_aum_trend_em.count_documents({})
            
            # ÂÅáËÆæÊàë‰ª¨Âè™ÂÖ≥ÂøÉÊÄªËÆ∞ÂΩïÊï∞
            
            return {
                'total_count': total_count
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëËßÑÊ®°Ëµ∞ÂäøÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_aum_hist_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëÂÖ¨Âè∏ÂéÜÂπ¥ÁÆ°ÁêÜËßÑÊ®°Âà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÂÖ¨Âè∏ÂéÜÂπ¥ÁÆ°ÁêÜËßÑÊ®°Êï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÂÖ¨Âè∏ÂéÜÂπ¥ÁÆ°ÁêÜËßÑÊ®°Êï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    fund_company = str(doc.get('Âü∫ÈáëÂÖ¨Âè∏', ''))
                    date_str = str(doc.get('Êõ¥Êñ∞Êó•Êúü', '')) or str(doc.get('Êà™Ê≠¢Êó•Êúü', '')) or datetime.now().strftime('%Y-%m-%d')
                    
                    doc['company'] = fund_company
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_aum_hist_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®Âü∫ÈáëÂÖ¨Âè∏ÂíåÊó•Êúü‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if fund_company and date_str:
                        ops.append(
                            UpdateOne(
                                {'company': fund_company, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_aum_hist_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëÂÖ¨Âè∏ÂéÜÂπ¥ÁÆ°ÁêÜËßÑÊ®°Êï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÂÖ¨Âè∏ÂéÜÂπ¥ÁÆ°ÁêÜËßÑÊ®°Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_aum_hist_em_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëÂÖ¨Âè∏ÂéÜÂπ¥ÁÆ°ÁêÜËßÑÊ®°Êï∞ÊçÆ"""
        try:
            result = await self.col_fund_aum_hist_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÂÖ¨Âè∏ÂéÜÂπ¥ÁÆ°ÁêÜËßÑÊ®°Êï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÂÖ¨Âè∏ÂéÜÂπ¥ÁÆ°ÁêÜËßÑÊ®°Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_aum_hist_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëÂÖ¨Âè∏ÂéÜÂπ¥ÁÆ°ÁêÜËßÑÊ®°ÁªüËÆ°"""
        try:
            total_count = await self.col_fund_aum_hist_em.count_documents({})
            
            unique_companies = await self.col_fund_aum_hist_em.distinct('company')
            
            return {
                'total_count': total_count,
                'unique_companies': len(unique_companies)
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÂÖ¨Âè∏ÂéÜÂπ¥ÁÆ°ÁêÜËßÑÊ®°ÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_reits_realtime_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òREITsÂÆûÊó∂Ë°åÊÉÖÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâREITsÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°REITsÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    code = str(doc.get('‰ª£Á†Å', ''))
                    date_str = datetime.now().strftime('%Y-%m-%d')
                    
                    doc['code'] = code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'reits_realtime_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®‰ª£Á†ÅÂíåÊó•Êúü‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if code:
                        ops.append(
                            UpdateOne(
                                {'code': code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_reits_realtime_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°REITsÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òREITsÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_reits_realtime_em_data(self) -> int:
        """Ê∏ÖÁ©∫REITsÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ"""
        try:
            result = await self.col_reits_realtime_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°REITsÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫REITsÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_reits_realtime_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñREITsÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°"""
        try:
            total_count = await self.col_reits_realtime_em.count_documents({})
            
            unique_codes = await self.col_reits_realtime_em.distinct('code')
            
            return {
                'total_count': total_count,
                'unique_codes': len(unique_codes)
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñREITsÂÆûÊó∂Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_reits_hist_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òREITsÂéÜÂè≤Ë°åÊÉÖÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâREITsÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°REITsÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    code = str(doc.get('code', '')) or str(doc.get('‰ª£Á†Å', ''))
                    date_str = str(doc.get('date', '')) or str(doc.get('Êó•Êúü', ''))
                    
                    doc['code'] = code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'reits_hist_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî®‰ª£Á†ÅÂíåÊó•Êúü‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if code and date_str:
                        ops.append(
                            UpdateOne(
                                {'code': code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_reits_hist_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°REITsÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òREITsÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_reits_hist_em_data(self) -> int:
        """Ê∏ÖÁ©∫REITsÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ"""
        try:
            result = await self.col_reits_hist_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°REITsÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫REITsÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_reits_hist_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñREITsÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°"""
        try:
            total_count = await self.col_reits_hist_em.count_documents({})
            
            unique_codes = await self.col_reits_hist_em.distinct('code')
            
            return {
                'total_count': total_count,
                'unique_codes': len(unique_codes)
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñREITsÂéÜÂè≤Ë°åÊÉÖÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_report_stock_cninfo_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëÈáç‰ªìËÇ°-Â∑®ÊΩÆÊï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÈáç‰ªìËÇ°Êï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÈáç‰ªìËÇ°Êï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    # ÂÅáËÆæÂ≠óÊÆµÂåÖÂê´ÔºöÂü∫Èáë‰ª£Á†Å„ÄÅËÇ°Á•®‰ª£Á†Å„ÄÅÊà™Ê≠¢Êó•Êúü/Êä•ÂëäÊúü
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', '')) or str(doc.get('fund_code', ''))
                    stock_code = str(doc.get('ËÇ°Á•®‰ª£Á†Å', '')) or str(doc.get('stock_code', ''))
                    date_str = str(doc.get('Êä•ÂëäÊúü', '')) or str(doc.get('date', '')) or str(doc.get('Êà™Ê≠¢Êó•Êúü', ''))
                    
                    doc['fund_code'] = fund_code
                    doc['stock_code'] = stock_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_report_stock_cninfo'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî® Âü∫Èáë‰ª£Á†Å + ËÇ°Á•®‰ª£Á†Å + Êä•ÂëäÊúü ‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if fund_code and stock_code and date_str:
                        ops.append(
                            UpdateOne(
                                {'fund_code': fund_code, 'stock_code': stock_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_report_stock_cninfo.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëÈáç‰ªìËÇ°Êï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÈáç‰ªìËÇ°Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_report_stock_cninfo_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëÈáç‰ªìËÇ°Êï∞ÊçÆ"""
        try:
            result = await self.col_fund_report_stock_cninfo.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÈáç‰ªìËÇ°Êï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÈáç‰ªìËÇ°Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_report_stock_cninfo_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëÈáç‰ªìËÇ°ÁªüËÆ°"""
        try:
            total_count = await self.col_fund_report_stock_cninfo.count_documents({})
            
            return {
                'total_count': total_count
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÈáç‰ªìËÇ°ÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_report_industry_allocation_cninfo_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëË°å‰∏öÈÖçÁΩÆ-Â∑®ÊΩÆÊï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëË°å‰∏öÈÖçÁΩÆÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëË°å‰∏öÈÖçÁΩÆÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    # ÂÅáËÆæÂ≠óÊÆµÂåÖÂê´ÔºöÂü∫Èáë‰ª£Á†Å„ÄÅË°å‰∏öÂêçÁß∞„ÄÅË°å‰∏öÁºñÁ†Å„ÄÅÊà™Ê≠¢Êó•Êúü/Êä•ÂëäÊúü
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', '')) or str(doc.get('fund_code', ''))
                    industry_name = str(doc.get('Ë°å‰∏öÂêçÁß∞', '')) or str(doc.get('industry_name', ''))
                    date_str = str(doc.get('Êä•ÂëäÊúü', '')) or str(doc.get('date', '')) or str(doc.get('Êà™Ê≠¢Êó•Êúü', ''))
                    
                    doc['fund_code'] = fund_code
                    doc['industry_name'] = industry_name
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_report_industry_allocation_cninfo'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî® Âü∫Èáë‰ª£Á†Å + Ë°å‰∏öÂêçÁß∞ + Êä•ÂëäÊúü ‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if fund_code and industry_name and date_str:
                        ops.append(
                            UpdateOne(
                                {'fund_code': fund_code, 'industry_name': industry_name, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_report_industry_allocation_cninfo.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëË°å‰∏öÈÖçÁΩÆÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëË°å‰∏öÈÖçÁΩÆÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_report_industry_allocation_cninfo_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëË°å‰∏öÈÖçÁΩÆÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_report_industry_allocation_cninfo.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëË°å‰∏öÈÖçÁΩÆÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëË°å‰∏öÈÖçÁΩÆÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_report_industry_allocation_cninfo_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëË°å‰∏öÈÖçÁΩÆÁªüËÆ°"""
        try:
            total_count = await self.col_fund_report_industry_allocation_cninfo.count_documents({})
            
            return {
                'total_count': total_count
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëË°å‰∏öÈÖçÁΩÆÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_report_asset_allocation_cninfo_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëËµÑ‰∫ßÈÖçÁΩÆ-Â∑®ÊΩÆÊï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëËµÑ‰∫ßÈÖçÁΩÆÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëËµÑ‰∫ßÈÖçÁΩÆÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    # ÂÅáËÆæÂ≠óÊÆµÂåÖÂê´ÔºöÂü∫Èáë‰ª£Á†Å„ÄÅÊà™Ê≠¢Êó•Êúü/Êä•ÂëäÊúü
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', '')) or str(doc.get('fund_code', ''))
                    date_str = str(doc.get('Êä•ÂëäÊúü', '')) or str(doc.get('date', '')) or str(doc.get('Êà™Ê≠¢Êó•Êúü', ''))
                    
                    doc['fund_code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_report_asset_allocation_cninfo'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî® Âü∫Èáë‰ª£Á†Å + Êä•ÂëäÊúü ‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if fund_code and date_str:
                        ops.append(
                            UpdateOne(
                                {'fund_code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_report_asset_allocation_cninfo.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëËµÑ‰∫ßÈÖçÁΩÆÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëËµÑ‰∫ßÈÖçÁΩÆÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_report_asset_allocation_cninfo_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëËµÑ‰∫ßÈÖçÁΩÆÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_report_asset_allocation_cninfo.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëËµÑ‰∫ßÈÖçÁΩÆÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëËµÑ‰∫ßÈÖçÁΩÆÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_report_asset_allocation_cninfo_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëËµÑ‰∫ßÈÖçÁΩÆÁªüËÆ°"""
        try:
            total_count = await self.col_fund_report_asset_allocation_cninfo.count_documents({})
            
            return {
                'total_count': total_count
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëËµÑ‰∫ßÈÖçÁΩÆÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_scale_change_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òËßÑÊ®°ÂèòÂä®-‰∏úË¥¢Êï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâËßÑÊ®°ÂèòÂä®Êï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°ËßÑÊ®°ÂèòÂä®Êï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    # ÂÅáËÆæÂ≠óÊÆµÂåÖÂê´ÔºöÊà™Ê≠¢Êó•Êúü„ÄÅÂáÄËµÑ‰∫ß„ÄÅÊúüÈó¥Áî≥Ë¥≠„ÄÅÊúüÈó¥ËµéÂõûÁ≠âÔºåÈúÄË¶Åcode
                    # Áî±‰∫éÊé•Âè£ÊòØÊåâcodeÊü•ÁöÑÔºådfÈáåÂèØËÉΩÊ≤°ÊúâcodeÔºåÈúÄË¶ÅÁ°Æ‰øù‰º†ÂÖ•ÂâçÂä†‰∏ä
                    fund_code = str(doc.get('code', '')) or str(doc.get('fund_code', ''))
                    date_str = str(doc.get('Êà™Ê≠¢Êó•Êúü', '')) or str(doc.get('date', ''))
                    
                    doc['fund_code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_scale_change_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî® Âü∫Èáë‰ª£Á†Å + Êà™Ê≠¢Êó•Êúü ‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if fund_code and date_str:
                        ops.append(
                            UpdateOne(
                                {'fund_code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_scale_change_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°ËßÑÊ®°ÂèòÂä®Êï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òËßÑÊ®°ÂèòÂä®Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_scale_change_em_data(self) -> int:
        """Ê∏ÖÁ©∫ËßÑÊ®°ÂèòÂä®Êï∞ÊçÆ"""
        try:
            result = await self.col_fund_scale_change_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ËßÑÊ®°ÂèòÂä®Êï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ËßÑÊ®°ÂèòÂä®Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_scale_change_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñËßÑÊ®°ÂèòÂä®ÁªüËÆ°"""
        try:
            total_count = await self.col_fund_scale_change_em.count_documents({})
            unique_funds = await self.col_fund_scale_change_em.distinct('fund_code')
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds)
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñËßÑÊ®°ÂèòÂä®ÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_hold_structure_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÊåÅÊúâ‰∫∫ÁªìÊûÑ-‰∏úË¥¢Êï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÊåÅÊúâ‰∫∫ÁªìÊûÑÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°ÊåÅÊúâ‰∫∫ÁªìÊûÑÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    # ÂÅáËÆæÂ≠óÊÆµÂåÖÂê´ÔºöÊà™Ê≠¢Êó•Êúü„ÄÅÊú∫ÊûÑÊåÅÊúâÊØî‰æã„ÄÅ‰∏™‰∫∫ÊåÅÊúâÊØî‰æã„ÄÅÂÜÖÈÉ®ÊåÅÊúâÊØî‰æã„ÄÅÊÄª‰ªΩÈ¢ùÁ≠â
                    # ÈúÄË¶Åcode
                    fund_code = str(doc.get('code', '')) or str(doc.get('fund_code', ''))
                    date_str = str(doc.get('Êà™Ê≠¢Êó•Êúü', '')) or str(doc.get('date', ''))
                    
                    doc['fund_code'] = fund_code
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_hold_structure_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî® Âü∫Èáë‰ª£Á†Å + Êà™Ê≠¢Êó•Êúü ‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if fund_code and date_str:
                        ops.append(
                            UpdateOne(
                                {'fund_code': fund_code, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_hold_structure_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°ÊåÅÊúâ‰∫∫ÁªìÊûÑÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÊåÅÊúâ‰∫∫ÁªìÊûÑÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_hold_structure_em_data(self) -> int:
        """Ê∏ÖÁ©∫ÊåÅÊúâ‰∫∫ÁªìÊûÑÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_hold_structure_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÊåÅÊúâ‰∫∫ÁªìÊûÑÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÊåÅÊúâ‰∫∫ÁªìÊûÑÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_hold_structure_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÊåÅÊúâ‰∫∫ÁªìÊûÑÁªüËÆ°"""
        try:
            total_count = await self.col_fund_hold_structure_em.count_documents({})
            unique_funds = await self.col_fund_hold_structure_em.distinct('fund_code')
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds)
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÊåÅÊúâ‰∫∫ÁªìÊûÑÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_stock_position_lg_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òËÇ°Á•®ÂûãÂü∫Èáë‰ªì‰Ωç-‰πêÂíï‰πêËÇ°Êï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâËÇ°Á•®ÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°ËÇ°Á•®ÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    # ÂÅáËÆæÂ≠óÊÆµÂåÖÂê´Ôºödate, ‰ªì‰Ωç
                    date_str = str(doc.get('date', '')) or str(doc.get('Êó•Êúü', ''))
                    
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_stock_position_lg'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî® date ‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if date_str:
                        ops.append(
                            UpdateOne(
                                {'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_stock_position_lg.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°ËÇ°Á•®ÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òËÇ°Á•®ÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_stock_position_lg_data(self) -> int:
        """Ê∏ÖÁ©∫ËÇ°Á•®ÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_stock_position_lg.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ËÇ°Á•®ÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ËÇ°Á•®ÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_stock_position_lg_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñËÇ°Á•®ÂûãÂü∫Èáë‰ªì‰ΩçÁªüËÆ°"""
        try:
            total_count = await self.col_fund_stock_position_lg.count_documents({})
            
            return {
                'total_count': total_count
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñËÇ°Á•®ÂûãÂü∫Èáë‰ªì‰ΩçÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_balance_position_lg_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂπ≥Ë°°Ê∑∑ÂêàÂûãÂü∫Èáë‰ªì‰Ωç-‰πêÂíï‰πêËÇ°Êï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂπ≥Ë°°Ê∑∑ÂêàÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âπ≥Ë°°Ê∑∑ÂêàÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    # ÂÅáËÆæÂ≠óÊÆµÂåÖÂê´Ôºödate, ‰ªì‰Ωç
                    date_str = str(doc.get('date', '')) or str(doc.get('Êó•Êúü', ''))
                    
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_balance_position_lg'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî® date ‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if date_str:
                        ops.append(
                            UpdateOne(
                                {'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_balance_position_lg.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âπ≥Ë°°Ê∑∑ÂêàÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂπ≥Ë°°Ê∑∑ÂêàÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_balance_position_lg_data(self) -> int:
        """Ê∏ÖÁ©∫Âπ≥Ë°°Ê∑∑ÂêàÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_balance_position_lg.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âπ≥Ë°°Ê∑∑ÂêàÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âπ≥Ë°°Ê∑∑ÂêàÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_balance_position_lg_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂπ≥Ë°°Ê∑∑ÂêàÂûãÂü∫Èáë‰ªì‰ΩçÁªüËÆ°"""
        try:
            total_count = await self.col_fund_balance_position_lg.count_documents({})
            
            return {
                'total_count': total_count
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂπ≥Ë°°Ê∑∑ÂêàÂûãÂü∫Èáë‰ªì‰ΩçÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_linghuo_position_lg_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÁÅµÊ¥ªÈÖçÁΩÆÂûãÂü∫Èáë‰ªì‰Ωç-‰πêÂíï‰πêËÇ°Êï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÁÅµÊ¥ªÈÖçÁΩÆÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°ÁÅµÊ¥ªÈÖçÁΩÆÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    # ÂÅáËÆæÂ≠óÊÆµÂåÖÂê´Ôºödate, ‰ªì‰Ωç
                    date_str = str(doc.get('date', '')) or str(doc.get('Êó•Êúü', ''))
                    
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_linghuo_position_lg'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ‰ΩøÁî® date ‰Ωú‰∏∫ÂîØ‰∏ÄÊ†áËØÜ
                    if date_str:
                        ops.append(
                            UpdateOne(
                                {'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_linghuo_position_lg.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°ÁÅµÊ¥ªÈÖçÁΩÆÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÁÅµÊ¥ªÈÖçÁΩÆÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_linghuo_position_lg_data(self) -> int:
        """Ê∏ÖÁ©∫ÁÅµÊ¥ªÈÖçÁΩÆÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_linghuo_position_lg.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°ÁÅµÊ¥ªÈÖçÁΩÆÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫ÁÅµÊ¥ªÈÖçÁΩÆÂûãÂü∫Èáë‰ªì‰ΩçÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_linghuo_position_lg_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÁÅµÊ¥ªÈÖçÁΩÆÂûãÂü∫Èáë‰ªì‰ΩçÁªüËÆ°"""
        try:
            total_count = await self.col_fund_linghuo_position_lg.count_documents({})
            
            return {
                'total_count': total_count
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÁÅµÊ¥ªÈÖçÁΩÆÂûãÂü∫Èáë‰ªì‰ΩçÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_announcement_dividend_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëÂÖ¨ÂëäÂàÜÁ∫¢ÈÖçÈÄÅ-‰∏úË¥¢Êï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÂÖ¨ÂëäÂàÜÁ∫¢ÈÖçÈÄÅÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÂÖ¨ÂëäÂàÜÁ∫¢ÈÖçÈÄÅÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    # ÂÅáËÆæÂ≠óÊÆµÂåÖÂê´ÔºöÂÖ¨ÂëäÊó•Êúü„ÄÅÂÖ¨ÂëäÊ†áÈ¢ò„ÄÅÂÖ¨ÂëäÂÜÖÂÆπÁ≠â
                    # ÂîØ‰∏ÄÊ†áËØÜÂèØËÉΩÊòØÔºöÂü∫Èáë‰ª£Á†Å + ÂÖ¨ÂëäÊ†áÈ¢ò + ÂÖ¨ÂëäÊó•Êúü
                    
                    fund_code = str(doc.get('code', '')) or str(doc.get('symbol', ''))
                    title = str(doc.get('ÂÖ¨ÂëäÊ†áÈ¢ò', '')) or str(doc.get('title', ''))
                    date_str = str(doc.get('ÂÖ¨ÂëäÊó•Êúü', '')) or str(doc.get('date', ''))
                    
                    doc['fund_code'] = fund_code
                    doc['title'] = title
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_announcement_dividend_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ÂîØ‰∏ÄÊ†áËØÜÔºöfund_code + title + date
                    if fund_code and title and date_str:
                        ops.append(
                            UpdateOne(
                                {'fund_code': fund_code, 'title': title, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_announcement_dividend_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëÂÖ¨ÂëäÂàÜÁ∫¢ÈÖçÈÄÅÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÂÖ¨ÂëäÂàÜÁ∫¢ÈÖçÈÄÅÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_announcement_dividend_em_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëÂÖ¨ÂëäÂàÜÁ∫¢ÈÖçÈÄÅÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_announcement_dividend_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÂÖ¨ÂëäÂàÜÁ∫¢ÈÖçÈÄÅÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÂÖ¨ÂëäÂàÜÁ∫¢ÈÖçÈÄÅÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_announcement_dividend_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëÂÖ¨ÂëäÂàÜÁ∫¢ÈÖçÈÄÅÁªüËÆ°"""
        try:
            total_count = await self.col_fund_announcement_dividend_em.count_documents({})
            unique_funds = await self.col_fund_announcement_dividend_em.distinct('fund_code')
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds)
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÂÖ¨ÂëäÂàÜÁ∫¢ÈÖçÈÄÅÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_announcement_report_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëÂÖ¨ÂëäÂÆöÊúüÊä•Âëä-‰∏úË¥¢Êï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÂÖ¨ÂëäÂÆöÊúüÊä•ÂëäÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÂÖ¨ÂëäÂÆöÊúüÊä•ÂëäÊï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    # ÂÅáËÆæÂ≠óÊÆµÂåÖÂê´ÔºöÂÖ¨ÂëäÊó•Êúü„ÄÅÂÖ¨ÂëäÊ†áÈ¢ò„ÄÅÂÖ¨ÂëäÂÜÖÂÆπÁ≠â
                    
                    fund_code = str(doc.get('code', '')) or str(doc.get('symbol', ''))
                    title = str(doc.get('ÂÖ¨ÂëäÊ†áÈ¢ò', '')) or str(doc.get('title', ''))
                    date_str = str(doc.get('ÂÖ¨ÂëäÊó•Êúü', '')) or str(doc.get('date', ''))
                    
                    doc['fund_code'] = fund_code
                    doc['title'] = title
                    doc['date'] = date_str
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_announcement_report_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ÂîØ‰∏ÄÊ†áËØÜÔºöfund_code + title + date
                    if fund_code and title and date_str:
                        ops.append(
                            UpdateOne(
                                {'fund_code': fund_code, 'title': title, 'date': date_str},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_announcement_report_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëÂÖ¨ÂëäÂÆöÊúüÊä•ÂëäÊï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÂÖ¨ÂëäÂÆöÊúüÊä•ÂëäÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_announcement_report_em_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëÂÖ¨ÂëäÂÆöÊúüÊä•ÂëäÊï∞ÊçÆ"""
        try:
            result = await self.col_fund_announcement_report_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÂÖ¨ÂëäÂÆöÊúüÊä•ÂëäÊï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÂÖ¨ÂëäÂÆöÊúüÊä•ÂëäÊï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_announcement_report_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëÂÖ¨ÂëäÂÆöÊúüÊä•ÂëäÁªüËÆ°"""
        try:
            total_count = await self.col_fund_announcement_report_em.count_documents({})
            unique_funds = await self.col_fund_announcement_report_em.distinct('fund_code')
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds)
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÂÖ¨ÂëäÂÆöÊúüÊä•ÂëäÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise

    async def save_fund_announcement_personnel_em_data(self, df: pd.DataFrame, progress_callback=None) -> int:
        """‰øùÂ≠òÂü∫ÈáëÂÖ¨Âëä‰∫∫‰∫ãË∞ÉÊï¥-‰∏úË¥¢Êï∞ÊçÆÂà∞MongoDB"""
        if df is None or df.empty:
            logger.warning("Ê≤°ÊúâÂü∫ÈáëÂÖ¨Âëä‰∫∫‰∫ãË∞ÉÊï¥Êï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            import numpy as np
            df = df.replace([np.inf, -np.inf], None)
            df = df.where(pd.notna(df), None)
            
            total_count = len(df)
            logger.info(f"üìä ÂºÄÂßãÂ§ÑÁêÜ {total_count} Êù°Âü∫ÈáëÂÖ¨Âëä‰∫∫‰∫ãË∞ÉÊï¥Êï∞ÊçÆ...")
            
            batch_size = 500
            total_saved = 0
            total_batches = (total_count + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_count)
                batch_df = df.iloc[start_idx:end_idx]
                
                ops = []
                for idx, row in batch_df.iterrows():
                    doc = row.to_dict()
                    
                    import math
                    import datetime as dt
                    for key, value in list(doc.items()):
                        if isinstance(value, (int, float)) and not isinstance(value, bool):
                            try:
                                if math.isnan(value) or math.isinf(value):
                                    doc[key] = None
                            except (TypeError, ValueError):
                                pass
                        elif isinstance(value, dt.date) and not isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                        elif isinstance(value, dt.datetime):
                            doc[key] = value.strftime('%Y-%m-%d')
                    
                    # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
                    # Â≠óÊÆµÔºöÂü∫Èáë‰ª£Á†Å„ÄÅÂÖ¨ÂëäÊ†áÈ¢ò„ÄÅÂü∫ÈáëÂêçÁß∞„ÄÅÂÖ¨ÂëäÊó•Êúü„ÄÅÊä•ÂëäID
                    
                    fund_code = str(doc.get('Âü∫Èáë‰ª£Á†Å', '')) or str(doc.get('code', ''))
                    title = str(doc.get('ÂÖ¨ÂëäÊ†áÈ¢ò', '')) or str(doc.get('title', ''))
                    fund_name = str(doc.get('Âü∫ÈáëÂêçÁß∞', '')) or str(doc.get('name', ''))
                    date_str = str(doc.get('ÂÖ¨ÂëäÊó•Êúü', '')) or str(doc.get('date', ''))
                    report_id = str(doc.get('Êä•ÂëäID', '')) or str(doc.get('report_id', ''))
                    
                    doc['fund_code'] = fund_code
                    doc['title'] = title
                    doc['fund_name'] = fund_name
                    doc['date'] = date_str
                    doc['report_id'] = report_id
                    doc['source'] = 'akshare'
                    doc['endpoint'] = 'fund_announcement_personnel_em'
                    doc['updated_at'] = datetime.now().isoformat()
                    
                    # ÂîØ‰∏ÄÊ†áËØÜÔºöfund_code + report_idÔºàÊä•ÂëäIDÊòØÂîØ‰∏ÄÁöÑÔºâ
                    if fund_code and report_id:
                        ops.append(
                            UpdateOne(
                                {'fund_code': fund_code, 'report_id': report_id},
                                {'$set': doc},
                                upsert=True
                            )
                        )
                
                if ops:
                    result = await self.col_fund_announcement_personnel_em.bulk_write(ops, ordered=False)
                    batch_saved = (result.upserted_count or 0) + (result.matched_count or 0)
                    total_saved += batch_saved
                    
                    if progress_callback:
                        progress = int((end_idx / total_count) * 100)
                        progress_callback(
                            current=end_idx,
                            total=total_count,
                            percentage=progress,
                            message=f"Â∑≤‰øùÂ≠ò {end_idx}/{total_count} Êù°Êï∞ÊçÆ ({progress}%)"
                        )
            
            logger.info(f"üéâ ÂÖ®ÈÉ®Êï∞ÊçÆÂÜôÂÖ•ÂÆåÊàê: ÊÄªËÆ°‰øùÂ≠ò {total_saved}/{total_count} Êù°Âü∫ÈáëÂÖ¨Âëä‰∫∫‰∫ãË∞ÉÊï¥Êï∞ÊçÆ")
            return total_saved
                
        except Exception as e:
            logger.error(f"‰øùÂ≠òÂü∫ÈáëÂÖ¨Âëä‰∫∫‰∫ãË∞ÉÊï¥Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def clear_fund_announcement_personnel_em_data(self) -> int:
        """Ê∏ÖÁ©∫Âü∫ÈáëÂÖ¨Âëä‰∫∫‰∫ãË∞ÉÊï¥Êï∞ÊçÆ"""
        try:
            result = await self.col_fund_announcement_personnel_em.delete_many({})
            deleted_count = result.deleted_count
            logger.info(f"ÊàêÂäüÊ∏ÖÁ©∫ {deleted_count} Êù°Âü∫ÈáëÂÖ¨Âëä‰∫∫‰∫ãË∞ÉÊï¥Êï∞ÊçÆ")
            return deleted_count
        except Exception as e:
            logger.error(f"Ê∏ÖÁ©∫Âü∫ÈáëÂÖ¨Âëä‰∫∫‰∫ãË∞ÉÊï¥Êï∞ÊçÆÂ§±Ë¥•: {e}", exc_info=True)
            raise
    
    async def get_fund_announcement_personnel_em_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂü∫ÈáëÂÖ¨Âëä‰∫∫‰∫ãË∞ÉÊï¥ÁªüËÆ°"""
        try:
            total_count = await self.col_fund_announcement_personnel_em.count_documents({})
            unique_funds = await self.col_fund_announcement_personnel_em.distinct('fund_code')
            
            return {
                'total_count': total_count,
                'unique_funds': len(unique_funds)
            }
        except Exception as e:
            logger.error(f"Ëé∑ÂèñÂü∫ÈáëÂÖ¨Âëä‰∫∫‰∫ãË∞ÉÊï¥ÁªüËÆ°Â§±Ë¥•: {e}", exc_info=True)
            raise
